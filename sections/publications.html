       <!-- Publications
      ----------------------------------------------- -->
      <div class="row add-bottom">
      <div class="twelve columns">
    <h1>Publications</h1>
      <aside> <!-- class="pull-quote" -->
      <blockquote>
      <p>This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author's copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder.</p>
    </blockquote>
    </aside>
    <p>See also the Google Scholar profiles of <a href="http://scholar.google.com/citations?hl=en&user=uUzoFH8AAAAJ">prof. Chris Develder</a> or <a href="http://scholar.google.com/citations?user=x_0MCH4AAAAJ&hl=en">prof. Thomas Demeester</a>.
    If you have trouble obtaining a copy of one of the papers below, please get in touch via <a href="mailto:chris.develder@ugent.be?subject=Publication from the T2K website">chris.develder@ugent.be.</a></p>
   
    <h2>Published papers</h2>

    <nav class="pubfiltering">
    <br>Type:
    <span class="pub-classes type current">All</span>
    <span class="pub-classes type article">Journal Article</span>
    <span class="pub-classes type inproceedings">Conference Paper</span>
    <!-- span class="pub-classes type inbook">Book Chapter</span -->
    <br>Year: <span class="pub-classes year current all">All</span>
  <span class="pub-classes year">2022</span>
  <span class="pub-classes year">2021</span>
  <span class="pub-classes year">2020</span> 
  <span class="pub-classes year">2019</span> 
  <span class="pub-classes year">2018</span>
  <span class="pub-classes year">2017</span>
  <span class="pub-classes year">2016</span>
    <span class="pub-classes year">2015</span>
    <span class="pub-classes year">2014</span>
    <span class="pub-classes year">2013</span>
    <span class="pub-classes year">2012</span>
    <span class="pub-classes year">2011</span>

        <br>Search: <input id="pubfilter"><span class="pub-classes search" id="pubsearch">Search</span> | Showing <span id="pubcount">0</span> results
    </nav>
      </div> <!-- twelve columns -->
      </div> <!-- end row -->
<div id="zaporojets2022neurips" class="row add-bottom pubentry pub2022 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>K. Zaporojets, L.-A. Kaffee, T. Demeester, C. Develder and I. Augenstein</pubauth>,
<pubtitle><a href="papers/2022/zaporojets2022neurips.pdf" target="_blank">"TempEL: Linking dynamically evolving and newly emerging entities"</a></pubtitle>, in <pubconf>Proc. 36th Conf. Neural Inf. Process. Sys. (NeurIPS 2022)</pubconf>, New Orleans, LA, USA, 28 Nov. - 9 Dec. <pubyear>2022</pubyear>.
    <ul class="file-links">
               <li><a href="papers/2022/zaporojets2022neurips.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           
           <li><a href="https://openreview.net/forum?id=vrnqr3PG4yB" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#zaporojets2022neurips-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#zaporojets2022neurips-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>         
         
    </ul>
     </div>
     <div id="zaporojets2022neurips-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>TempEL: Linking dynamically evolving and newly emerging entities</h2>
        <h4>K. Zaporojets, L.-A. Kaffee, T. Demeester, C. Develder and I. Augenstein</h4>
        <br>in <pubconf>Proc. 36th Conf. Neural Inf. Process. Sys. (NeurIPS 2022)</pubconf>, New Orleans, LA, USA, 28 Nov. - 9 Dec. <pubyear>2022</pubyear>.
        <br><br>
        <p>In our continuously evolving world, entities change over time and new, previously non-existing or unknown, entities appear. We study how this evolutionary scenario impacts the performance on a well established entity linking (EL) task. For that study, we introduce TempEL, an entity linking dataset that consists of time-stratified English Wikipedia snapshots from 2013 to 2022, from which we collect both anchor mentions of entities, and these target entities’ descriptions. By capturing such temporal aspects, our newly introduced TempEL resource contrasts with currently existing entity linking datasets, which are composed of fixed mentions linked to a single static version of a target Knowledge Base (e.g., Wikipedia 2010 for CoNLL-AIDA). Indeed, for each of our collected temporal snapshots, TempEL contains links to entities that are continual, i.e., occur in all of the years, as well as completely new entities that appear for the first time at some point. Thus, we enable to quantify the performance of current state-of-the-art EL models for: (i) entities that are subject to changes over time in their Knowledge Base descriptions as well as their mentions’ contexts, and (ii) newly created entities that were previously non-existing (e.g., at the time the EL model was trained). Our experimental results show that in terms of temporal performance degradation, (i) continual entities suffer a decrease of up to 3.1% EL accuracy, while (ii) for new entities this accuracy drop is up to 17.9%. This highlights the challenge of the introduced TempEL dataset and opens new research prospects in the area of time-evolving entity disambiguation.</p>
    </div>
    <div id="zaporojets2022neurips-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>TempEL: Linking dynamically evolving and newly emerging entities</h2>
        <h4>K. Zaporojets, L.-A. Kaffee, T. Demeester, C. Develder and I. Augenstein</h4>
        <br>in <pubconf>Proc. 36th Conf. Neural Inf. Process. Sys. (NeurIPS 2022)</pubconf>, New Orleans, LA, USA, 28 Nov. - 9 Dec. <pubyear>2022</pubyear>.
        <br><br>
        <code>
@inproceedings{Zaporojets2022NeurIPS,<br>
  author = {Zaporojets, Klim and Kaffee, Lucie-Aim&eacute;e and Demeester, Thomas and Develder, Chris and Augenstein, Isabelle},<br>
  title = {TempEL: Linking dynamically evolving and newly emerging entities},<br>
  booktitle = {Proc. 36th Conf. Neural Inf. Process. Sys. (NeurIPS 2022)},<br>
  month = {28 Nov. -- 9 Dec.},<br>
  year = {2022},<br>
  address = {New Orleans, LA, USA},<br>
  url = {https://openreview.net/forum?id=vrnqr3PG4yB}<br>
}
        </code>
      </div>
</div> <!-- end row -->

<div id="decorte2022recsyshr" class="row add-bottom pubentry pub2022 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>J.-J. Decorte, J. Van Hautte, J. Deleu, C. Develder and T. Demeester</pubauth>,
<pubtitle><a href="papers/2022/decorte2022recsyshr.pdf" target="_blank">"Design of negative sampling strategies for distantly supervised skill extraction"</a></pubtitle>, in <pubconf>Proc. 2nd Workshop Recomm. Sys. Hum. Resour. at RecSys 2022 (RecSys in HR 2022)</pubconf>, Seattle, WA, USA, 22 Sep. <pubyear>2022</pubyear>.
    <ul class="file-links">
               <li><a href="papers/2022/decorte2022recsyshr.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           
           
               <li><a class="popup-with-zoom-anim" href="#decorte2022recsyshr-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#decorte2022recsyshr-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/2209.05987" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         <li><a href="https://github.com/jensjorisdecorte/skill-extraction-benchmark" target="_blank"><i class="fa fa-github"></i> Code</a></li>
         
    </ul>
     </div>
     <div id="decorte2022recsyshr-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Design of negative sampling strategies for distantly supervised skill extraction</h2>
        <h4>J.-J. Decorte, J. Van Hautte, J. Deleu, C. Develder and T. Demeester</h4>
        <br>in <pubconf>Proc. 2nd Workshop Recomm. Sys. Hum. Resour. at RecSys 2022 (RecSys in HR 2022)</pubconf>, Seattle, WA, USA, 22 Sep. <pubyear>2022</pubyear>.
        <br><br>
        <p>Skills play a central role in the job market and many human resources (HR) processes. In the wake of other digital experiences, today's online job market has candidates expecting to see the right opportunities based on their skill set. Similarly, enterprises increasingly need to use data to guarantee that the skills within their workforce remain future-proof. However, structured information about skills is often missing, and processes building on self- or manager-assessment have shown to struggle with issues around adoption, completeness, and freshness of the resulting data. Extracting skills is a highly challenging task, given the many thousands of possible skill labels mentioned either explicitly or merely described implicitly and the lack of finely annotated training corpora. Previous work on skill extraction overly simplifies the task to an explicit entity detection task or builds on manually annotated training data that would be infeasible if applied to a complete vocabulary of skills. We propose an end-to-end system for skill extraction, based on distant supervision through literal matching. We propose and evaluate several negative sampling strategies, tuned on a small validation dataset, to improve the generalization of skill extraction towards implicitly mentioned skills, despite the lack of such implicit skills in the distantly supervised data. We observe that using the ESCO taxonomy to select negative examples from related skills yields the biggest improvements, and combining three different strategies in one model further increases the performance, up to 8 percentage points in RP@5. We introduce a manually annotated evaluation benchmark for skill extraction based on the ESCO taxonomy, on which we validate our models. We release the benchmark dataset for research purposes to stimulate further research on the task.</p>
    </div>
    <div id="decorte2022recsyshr-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Design of negative sampling strategies for distantly supervised skill extraction</h2>
        <h4>J.-J. Decorte, J. Van Hautte, J. Deleu, C. Develder and T. Demeester</h4>
        <br>in <pubconf>Proc. 2nd Workshop Recomm. Sys. Hum. Resour. at RecSys 2022 (RecSys in HR 2022)</pubconf>, Seattle, WA, USA, 22 Sep. <pubyear>2022</pubyear>.
        <br><br>
        <code>
@inproceedings{Decorte2022RecSysHR,<br>
  author = {Decorte, Jens-Joris and Van Hautte, Jeroen and Deleu, Johannes and Develder, Chris and Demeester, T.},<br>
  title = {Design of negative sampling strategies for distantly supervised skill extraction},<br>
  booktitle = {Proc. 2nd Workshop Recomm. Sys. Hum. Resour. at RecSys 2022 (RecSys in HR 2022)},<br>
  month = {22 Sep.},<br>
  year = {2022},<br>
  address = {Seattle, WA, USA}<br>
}
        </code>
      </div>
</div> <!-- end row -->

<div id="jiang2022cookdial" class="row add-bottom pubentry pub2022 pubir pubarticle">
     <div class="one column">
     <h3 class="article"><span>pubarticle</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>Y. Jiang, K. Zaporojets, J. Deleu, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2022/jiang2022cookdial.pdf" target="_blank">"CookDial: A dataset for task-oriented dialogs grounded in procedural documents"</a></pubtitle>, <pubjournal>Appl. Intelligence</pubjournal>, Jun. <pubyear>2022</pubyear>.
    <ul class="file-links">
               <li><a href="papers/2022/jiang2022cookdial.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           <li><a href="https://doi.org/10.1007/s10489-022-03692-0" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
           
               <li><a class="popup-with-zoom-anim" href="#jiang2022cookdial-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#jiang2022cookdial-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/2206.08723" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         <li><a href="https://github.com/YiweiJiang2015/CookDial" target="_blank"><i class="fa fa-github"></i> Code</a></li>
         
    </ul>
     </div>
     <div id="jiang2022cookdial-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>CookDial: A dataset for task-oriented dialogs grounded in procedural documents</h2>
        <h4>Y. Jiang, K. Zaporojets, J. Deleu, T. Demeester and C. Develder</h4>
        <br><pubjournal>Appl. Intelligence</pubjournal>, Jun. <pubyear>2022</pubyear>.
        <br><br>
        <p>This work presents a new dialog dataset, CookDial, that facilitates research on task-oriented dialog systems with procedural knowledge understanding. The corpus contains 260 human-to-human task-oriented dialogs in which an agent, given a recipe document, guides the user to cook a dish. Dialogs in CookDial exhibit two unique features: (i) procedural alignment between the dialog flow and supporting document; (ii) complex agent decision-making that involves segmenting long sentences, paraphrasing hard instructions and resolving coreference in the dialog context. In addition, we identify three challenging (sub)tasks in the assumed task-oriented dialog system: (1) User Question Understanding, (2) Agent Action Frame Prediction, and (3) Agent Response Generation. For each of these tasks, we develop a neural baseline model, which we evaluate on the CookDial dataset. We publicly release the CookDial dataset, comprising rich annotations of both dialogs and recipe documents, to stimulate further research on domain-specific document-grounded dialog systems.</p>
    </div>
    <div id="jiang2022cookdial-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>CookDial: A dataset for task-oriented dialogs grounded in procedural documents</h2>
        <h4>Y. Jiang, K. Zaporojets, J. Deleu, T. Demeester and C. Develder</h4>
        <br><pubjournal>Appl. Intelligence</pubjournal>, Jun. <pubyear>2022</pubyear>.
        <br><br>
        <code>
@article{jiang2022cookdial,<br>
  author = {Jiang, Yiwei and Zaporojets, Klim and Deleu, Johannes and Demeester, Thomas and Develder, Chris},<br>
  title = {CookDial: A dataset for task-oriented dialogs grounded in procedural documents},<br>
  journal = {Appl. Intelligence},<br>
  month = {Jun.},<br>
  year = {2022},<br>
  doi = {10.1007/s10489-022-03692-0}<br>
}
        </code>
      </div>
</div> <!-- end row -->

<div id="jiang2022acl" class="row add-bottom pubentry pub2022 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>Y. Jiang, A. Hadifar, J. Deleu, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2022/jiang2022acl.pdf" target="_blank">"UGent-T2K at the 2nd DialDoc shared task: A retrieval-focused dialog system grounded in multiple documents"</a></pubtitle>, in <pubconf>Proc. DialDoc Workshop at ACL 2022</pubconf>, Dublin, Ireland, May 26 <pubyear>2022</pubyear>, pp. 1-8.
    <ul class="file-links">
               <li><a href="papers/2022/jiang2022acl.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           <li><a href="https://doi.org/10.18653/v1/2022.dialdoc-1.12" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
           
               <li><a class="popup-with-zoom-anim" href="#jiang2022acl-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#jiang2022acl-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
         
    </ul>
     </div>
     <div id="jiang2022acl-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>UGent-T2K at the 2nd DialDoc shared task: A retrieval-focused dialog system grounded in multiple documents</h2>
        <h4>Y. Jiang, A. Hadifar, J. Deleu, T. Demeester and C. Develder</h4>
        <br>in <pubconf>Proc. DialDoc Workshop at ACL 2022</pubconf>, Dublin, Ireland, May 26 <pubyear>2022</pubyear>, pp. 1-8.
        <br><br>
        <p>This work presents the contribution from the text-to-Knowledge team of Ghent University (UGent-T2K)1 to the MultiDoc2Dial shared task on modeling dialogs grounded in multiple documents. We propose a pipeline system, comprising (1) document retrieval, (2) passage retrieval, and (3) response generation. We engineered these individual components mainly by, for (1)-(2), combining multiple ranking models and adding a final LambdaMART reranker, and, for (3), by adopting a Fusion-in-Decoder (FiD) model. We thus significantly boost the baseline system’s performance (over +10 points for both F1 and SacreBLEU). Further, error analysis reveals two major failure cases, to be addressed in future work: (i) in case of topic shift within the dialog, retrieval often fails to select the correct grounding document(s), and (ii) generation sometimes fails to use the correctly retrieved grounding passage.</p>
    </div>
    <div id="jiang2022acl-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>UGent-T2K at the 2nd DialDoc shared task: A retrieval-focused dialog system grounded in multiple documents</h2>
        <h4>Y. Jiang, A. Hadifar, J. Deleu, T. Demeester and C. Develder</h4>
        <br>in <pubconf>Proc. DialDoc Workshop at ACL 2022</pubconf>, Dublin, Ireland, May 26 <pubyear>2022</pubyear>, pp. 1-8.
        <br><br>
        <code>
@inproceedings{jiang2022acl,<br>
  author = {Jiang, Yiwei and Hadifar, Amir and Deleu, Johannes and Demeester, Thomas and Develder, Chris},<br>
  title = {UGent-T2K at the 2nd DialDoc shared task: A retrieval-focused dialog system grounded in multiple documents},<br>
  booktitle = {Proc. DialDoc Workshop at ACL 2022},<br>
  month = {May 26},<br>
  year = {2022},<br>
  pages = {1--8},<br>
  address = {Dublin, Ireland},<br>
  doi = {10.18653/v1/2022.dialdoc-1.12}<br>
}
        </code>
      </div>
</div> <!-- end row -->

<div id="zaporojets2022acl" class="row add-bottom pubentry pub2022 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>K. Zaporojets, J. Deleu, Y. Jiang, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2022/zaporojets2022acl.pdf" target="_blank">"Towards consistent document-level entity linking: Joint Models for entity linking and coreference resolution"</a></pubtitle>, in <pubconf>Proc. 60th Annual Meet. Assoc. Comput. Linguist. (ACL 2022)</pubconf>, Dublin, Ireland, 22-27 May <pubyear>2022</pubyear>, pp. 1-7.
    <ul class="file-links">
               <li><a href="papers/2022/zaporojets2022acl.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           <li><a href="https://doi.org/10.18653/v1/2022.acl-short.88" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
           
               <li><a class="popup-with-zoom-anim" href="#zaporojets2022acl-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#zaporojets2022acl-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/2108.13530" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         <li><a href="https://github.com/klimzaporojets/consistent-EL" target="_blank"><i class="fa fa-github"></i> Code</a></li>
         
    </ul>
     </div>
     <div id="zaporojets2022acl-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Towards consistent document-level entity linking: Joint Models for entity linking and coreference resolution</h2>
        <h4>K. Zaporojets, J. Deleu, Y. Jiang, T. Demeester and C. Develder</h4>
        <br>in <pubconf>Proc. 60th Annual Meet. Assoc. Comput. Linguist. (ACL 2022)</pubconf>, Dublin, Ireland, 22-27 May <pubyear>2022</pubyear>, pp. 1-7.
        <br><br>
        <p>We consider the task of document-level entity linking (EL), where it is important to make onsistent decisions for entity mentions over the full document jointly. We aim to leverage explicit “connections” among mentions within the document itself: we propose to join EL and coreference resolution (coref) in a single structured prediction task over directed trees and use a globally normalized model to solve it. This contrasts with related works where two separate models are trained for each of the tasks and additional logic is required to merge the outputs. Experimental results on two datasets show a boost of up to +5% F1-score on both coref and EL tasks, compared to their standalone counterparts. For a subset of hard cases, with individual mentions lacking the correct EL in their candidate entity list, we obtain a +50% increase in accuracy.</p>
    </div>
    <div id="zaporojets2022acl-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Towards consistent document-level entity linking: Joint Models for entity linking and coreference resolution</h2>
        <h4>K. Zaporojets, J. Deleu, Y. Jiang, T. Demeester and C. Develder</h4>
        <br>in <pubconf>Proc. 60th Annual Meet. Assoc. Comput. Linguist. (ACL 2022)</pubconf>, Dublin, Ireland, 22-27 May <pubyear>2022</pubyear>, pp. 1-7.
        <br><br>
        <code>
@inproceedings{zaporojets2022acl,<br>
  author = {Zaporojets, Klim and Deleu, Johannes and Jiang, Yiwei and Demeester, Thomas and Develder, Chris},<br>
  title = {Towards consistent document-level entity linking: Joint Models for entity linking and coreference resolution},<br>
  booktitle = {Proc. 60th Annual Meet. Assoc. Comput. Linguist. (ACL 2022)},<br>
  month = {22--27 May},<br>
  year = {2022},<br>
  pages = {1--7},<br>
  address = {Dublin, Ireland},<br>
  doi = {10.18653/v1/2022.acl-short.88}<br>
}
        </code>
      </div>
</div> <!-- end row -->

<div id="bitew2021crac" class="row add-bottom pubentry pub2021 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>S.K. Bitew, J. Deleu, C. Develder and T. Demeester</pubauth>,
<pubtitle><a href="papers/2021/bitew2021crac.pdf" target="_blank">"Lazy low-resource coreference resolution: A study on leveraging black-box translation tools"</a></pubtitle>, in <pubconf>Proc. 4th Workshop Comput. Models of Reference, Anaphora and Coreference (CRAC 2021) at EMNLP 2021</pubconf>, Punta Cana, Domenican Republic, 11 Nov. <pubyear>2021</pubyear>, pp. 1-6.
    <ul class="file-links">
               <li><a href="papers/2021/bitew2021crac.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           
           <li><a href="https://aclanthology.org/2021.crac-1.6/" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#bitew2021crac-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#bitew2021crac-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
         
    </ul>
     </div>
     <div id="bitew2021crac-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Lazy low-resource coreference resolution: A study on leveraging black-box translation tools</h2>
        <h4>S.K. Bitew, J. Deleu, C. Develder and T. Demeester</h4>
        <br>in <pubconf>Proc. 4th Workshop Comput. Models of Reference, Anaphora and Coreference (CRAC 2021) at EMNLP 2021</pubconf>, Punta Cana, Domenican Republic, 11 Nov. <pubyear>2021</pubyear>, pp. 1-6.
        <br><br>
        <p>Large annotated corpora for coreference resolution are available for few languages. For machine translation, however, strong black-box systems exist for many languages. We empirically explore the appealing idea of leveraging such translation tools for bootstrapping coreference resolution in languages with limited resources. Two scenarios are analyzed, in which a large coreference corpus in a high-resource language is used for coreference predictions in a smaller language, i.e., by machine translating either the training corpus, or the test data. In our empirical evaluation of coreference resolution using the two scenarios on several medium-resource languages, we find no improvement over monolingual baseline models. Our analysis of the various sources of error inherent to the studied scenarios, reveals that in fact the quality of contemporary machine translation tools is the main limiting factor.</p>
    </div>
    <div id="bitew2021crac-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Lazy low-resource coreference resolution: A study on leveraging black-box translation tools</h2>
        <h4>S.K. Bitew, J. Deleu, C. Develder and T. Demeester</h4>
        <br>in <pubconf>Proc. 4th Workshop Comput. Models of Reference, Anaphora and Coreference (CRAC 2021) at EMNLP 2021</pubconf>, Punta Cana, Domenican Republic, 11 Nov. <pubyear>2021</pubyear>, pp. 1-6.
        <br><br>
        <code>
@inproceedings{bitew2021crac,<br>
  author = {Bitew, Semere Kiros and Deleu, Johannes and Develder, Chris and Demeester, Thomas},<br>
  title = {Lazy low-resource coreference resolution: A study on leveraging black-box translation tools},<br>
  booktitle = {Proc. 4th Workshop Comput. Models of Reference, Anaphora and Coreference (CRAC 2021) at EMNLP 2021},<br>
  month = {11 Nov.},<br>
  year = {2021},<br>
  pages = {1--6},<br>
  address = {Punta Cana, Domenican Republic},<br>
  url = {https://aclanthology.org/2021.crac-1.6/}<br>
}
        </code>
      </div>
</div> <!-- end row -->

<div id="deraedt2021emnlp" class="row add-bottom pubentry pub2021 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>M. De Raedt, F. Godin, P. Buteneers, C. Develder and T. Demeester</pubauth>,
<pubtitle><a href="papers/2021/deraedt2021emnlp.pdf" target="_blank">"A simple geometric method for cross-lingual linguistic transformations with pre-trained autoencoders"</a></pubtitle>, in <pubconf>Proc. Conf. Empirical Methods in Natural Lang. Processing (EMNLP 2021)</pubconf>, Punta Cana, Domenican Republic, 7-11 Nov. <pubyear>2021</pubyear>.
    <ul class="file-links">
               <li><a href="papers/2021/deraedt2021emnlp.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           
           <li><a href="https://aclanthology.org/2021.emnlp-main.792/" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#deraedt2021emnlp-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#deraedt2021emnlp-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/2104.03630" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         
         
    </ul>
     </div>
     <div id="deraedt2021emnlp-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>A simple geometric method for cross-lingual linguistic transformations with pre-trained autoencoders</h2>
        <h4>M. De Raedt, F. Godin, P. Buteneers, C. Develder and T. Demeester</h4>
        <br>in <pubconf>Proc. Conf. Empirical Methods in Natural Lang. Processing (EMNLP 2021)</pubconf>, Punta Cana, Domenican Republic, 7-11 Nov. <pubyear>2021</pubyear>.
        <br><br>
        <p>Powerful sentence encoders trained for multiple languages are on the rise. These systems are capable of embedding a wide range of linuistic properties into vector representations. While explicit probing tasks can be used to verify the presence of specific linguistic properties, it is unclear whether the vector represen- tations can be manipulated to indirectly steer such properties. For efficient learning, we i vestigate the use of a geometric mapping in embedding space to transform linguistic prop- erties, without any tuning of the pre-trained sentence encoder or decoder. We validate our approach on three linguistic properties using a pre-trained multilingual autoencoder and ana- lyze the results in both monolingual and cross- lingual settings.</p>
    </div>
    <div id="deraedt2021emnlp-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>A simple geometric method for cross-lingual linguistic transformations with pre-trained autoencoders</h2>
        <h4>M. De Raedt, F. Godin, P. Buteneers, C. Develder and T. Demeester</h4>
        <br>in <pubconf>Proc. Conf. Empirical Methods in Natural Lang. Processing (EMNLP 2021)</pubconf>, Punta Cana, Domenican Republic, 7-11 Nov. <pubyear>2021</pubyear>.
        <br><br>
        <code>
@inproceedings{deraedt2021emnlp,<br>
  author = {De Raedt, Maarten and Godin, Fr&eacute;deric and Buteneers, Pieter and Develder, Chris and Demeester, Thomas},<br>
  title = {A simple geometric method for cross-lingual linguistic transformations with pre-trained autoencoders},<br>
  booktitle = {Proc. Conf. Empirical Methods in Natural Lang. Processing (EMNLP 2021)},<br>
  month = {7--11 Nov.},<br>
  year = {2021},<br>
  address = {Punta Cana, Domenican Republic},<br>
  url = {https://aclanthology.org/2021.emnlp-main.792/}<br>
}
        </code>
      </div>
</div> <!-- end row -->


<div id="hadifar2021prl" class="row add-bottom pubentry pub2021 pubir pubarticle">
     <div class="one column">
     <h3 class="article"><span>pubarticle</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>A. Hadifar, J. Deleu, C. Develder and T. Demeester</pubauth>,
<pubtitle><a href="papers/2021/hadifar2021prl.pdf" target="_blank">"Exploration of block-wise dynamic sparseness"</a></pubtitle>, <pubjournal>Pattern Recognit. Lett.</pubjournal>, Vol. 151, Nov. <pubyear>2021</pubyear>, pp. 187-192.
    <ul class="file-links">
               <li><a href="papers/2021/hadifar2021prl.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           <li><a href="https://doi.org/10.1016/j.patrec.2021.08.013" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
           
               <li><a class="popup-with-zoom-anim" href="#hadifar2021prl-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#hadifar2021prl-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/2001.04686" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         <li><a href="https://github.com/hadifar/dynamic-sparseness" target="_blank"><i class="fa fa-github"></i> Code</a></li>
         
    </ul>
     </div>
     <div id="hadifar2021prl-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Exploration of block-wise dynamic sparseness</h2>
        <h4>A. Hadifar, J. Deleu, C. Develder and T. Demeester</h4>
        <br><pubjournal>Pattern Recognit. Lett.</pubjournal>, Vol. 151, Nov. <pubyear>2021</pubyear>, pp. 187-192.
        <br><br>
        <p>Neural networks have achieved state of the art performance across a wide variety of machine learning tasks, often with large and computation-heavy models. Inducing sparseness as a way to reduce the memory and computation footprint of these models has seen significant research attention in recent years. In this paper, we present a new method for dynamic sparseness, whereby part of the computations are omitted dynamically, based on the input. For efficiency, we combined the idea of dynamic sparseness with block-wise matrix-vector multiplications. In contrast to static sparseness, which permanently zeroes out selected positions in weight matrices, our method preserves the full network capabilities by potentially accessing any trained weights. Yet, matrix vector multiplications are accelerated by omitting a pre-defined fraction of weight blocks from the matrix, based on the input. Experimental results on the task of language modeling, using recurrent and quasi-recurrent models, show that the proposed method can outperform static sparseness baselines. In addition, our method can reach similar language modeling perplexities as the dense baseline, at half the computational cost at inference time.</p>
    </div>
    <div id="hadifar2021prl-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Exploration of block-wise dynamic sparseness</h2>
        <h4>A. Hadifar, J. Deleu, C. Develder and T. Demeester</h4>
        <br><pubjournal>Pattern Recognit. Lett.</pubjournal>, Vol. 151, Nov. <pubyear>2021</pubyear>, pp. 187-192.
        <br><br>
        <code>
@article{hadifar2021prl,<br>
  author = {Hadifar, Amir and Deleu, Johannes and Develder, Chris and Demeester, Thomas},<br>
  title = {Exploration of block-wise dynamic sparseness},<br>
  journal = {Pattern Recognit. Lett.},<br>
  month = {Nov.},<br>
  year = {2021},<br>
  volume = {151},<br>
  pages = {187--192},<br>
  doi = {10.1016/j.patrec.2021.08.013}<br>
}
        </code>
      </div>
</div> <!-- end row -->

<div id="decorte2021feast" class="row add-bottom pubentry pub2021 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>J.-J. Decorte, J. Van Hautte, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2021/decorte2021feast.pdf" target="_blank">"JobBERT: Understanding job titles through skills"</a></pubtitle>, in <pubconf>Proc. Int. Workshop Fair, Effective and Sustainable Talent at ECML-PKDD (FEAST 2021)</pubconf>, Bilbao, Spain, 13-17 Sep. <pubyear>2021</pubyear>.
    <ul class="file-links">
               <li><a href="papers/2021/decorte2021feast.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           
           
               <li><a class="popup-with-zoom-anim" href="#decorte2021feast-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#decorte2021feast-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/2109.09605" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         
         <li><a href="papers/2021/decorte2021feast-poster.pdf" target="_blank"><i class="fa fa-file-powerpoint-o"></i> Poster</a></li>
    </ul>
     </div>
     <div id="decorte2021feast-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>JobBERT: Understanding job titles through skills</h2>
        <h4>J.-J. Decorte, J. Van Hautte, T. Demeester and C. Develder</h4>
        <br>in <pubconf>Proc. Int. Workshop Fair, Effective and Sustainable Talent at ECML-PKDD (FEAST 2021)</pubconf>, Bilbao, Spain, 13-17 Sep. <pubyear>2021</pubyear>.
        <br><br>
        <p>Job titles form a cornerstone of today’s human resources (HR) processes. Within online recruitment, they allow candidates to understand the contents of a vacancy at a glance, while internal HR departments use them to organize and structure many of their processes. As job titles are a compact, convenient, and readily available data source, modeling them with high accuracy can greatly benefit many HR tech applications. In this paper, we propose a neural representation model for job titles, by augmenting a pre-trained language model with co-occurrence information from skill labels extracted from vacancies. Our JobBERT method leads to considerable improvements compared to using generic sentence encoders, for the task of job title normalization, for which we release a new evaluation benchmark.</p>
    </div>
    <div id="decorte2021feast-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>JobBERT: Understanding job titles through skills</h2>
        <h4>J.-J. Decorte, J. Van Hautte, T. Demeester and C. Develder</h4>
        <br>in <pubconf>Proc. Int. Workshop Fair, Effective and Sustainable Talent at ECML-PKDD (FEAST 2021)</pubconf>, Bilbao, Spain, 13-17 Sep. <pubyear>2021</pubyear>.
        <br><br>
        <code>
@inproceedings{decorte2021feast,<br>
  author = {Decorte, Jens-Joris and Van Hautte, Jeroen and Demeester, Thomas and Develder, Chris},<br>
  title = {JobBERT: Understanding job titles through skills},<br>
  booktitle = {Proc. Int. Workshop Fair, Effective and Sustainable Talent at ECML-PKDD (FEAST 2021)},<br>
  month = {13--17 Sep.},<br>
  year = {2021},<br>
  address = {Bilbao, Spain}<br>
}
        </code>
      </div>
</div> <!-- end row -->

<div id="verlinden2021" class="row add-bottom pubentry pub2021 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>S. Verlinden, K. Zaporojets, J. Deleu, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2021/verlinden2021.pdf" target="_blank">"Injecting knowledge base information into end-to-end joint entity and relation extraction and coreference resolution"</a></pubtitle>, in <pubconf>Findings of the ACL: ACL-IJCNLP 2021</pubconf>, Bangkok, Thailand, 1-6 Aug. <pubyear>2021</pubyear>.
  <ul class="file-links">
               <li><a href="papers/2021/verlinden2021.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.18653/v1/2021.findings-acl.171" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#verlinden2021-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#verlinden2021-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/2107.02286" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         <li><a href="https://github.com/klimzaporojets/e2e-kb-ie" target="_blank"><i class="fa fa-github"></i> Code</a></li>
  </ul>
     </div>
     <div id="verlinden2021-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Injecting knowledge base information into end-to-end joint entity and relation extraction and coreference resolution</h2>
      <h4>S. Verlinden, K. Zaporojets, J. Deleu, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Findings of the ACL: ACL-IJCNLP 2021</pubconf>, Bangkok, Thailand, 1-6 Aug. <pubyear>2021</pubyear>.
      <br><br>
      <p>We consider a joint information extraction (IE) model, solving named entity recognition, coreference resolution and relation extraction jointly over the whole document. In particu- lar, we study how to inject information from a knowledge base (KB) in such IE model, based on unsupervised entity linking. The used KB entity representations are learned from either (i) hyperlinked text documents (Wikipedia), or (ii) a knowledge graph (Wikidata), and ap- pear complementary in raising IE performance. Representations of corresponding entity link- ing (EL) candidates are added to text span rep- resentations of the input document, and we ex- periment with (i) taking a weighted average of the EL candidate representations based on their prior (in Wikipedia), and (ii) using an attention scheme over the EL candidate list. Results demonstrate an increase of up to 5% F1-score for the evaluated IE tasks on two datasets. Despite a strong performance of the prior-based model, our quantitative and quali- tative analysis reveals the advantage of using the attention-based approach.</p>
    </div>
    <div id="verlinden2021-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Injecting knowledge base information into end-to-end joint entity and relation extraction and coreference resolution</h2>
      <h4>S. Verlinden, K. Zaporojets, J. Deleu, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Findings of the ACL: ACL-IJCNLP 2021</pubconf>, Bangkok, Thailand, 1-6 Aug. <pubyear>2021</pubyear>.
      <br><br>
      <code>
@inproceedings{verlinden2021,<br>
  author = {Verlinden, Severine and Zaporojets, Klim and Deleu, Johannes and Demeester, Thomas and Develder, Chris},<br>
  title = {Injecting knowledge base information into end-to-end joint entity and relation extraction and coreference resolution},<br>
  booktitle = {Findings of the ACL: ACL-IJCNLP 2021},<br>
  month = {1--6 Aug.},<br>
  year = {2021},<br>
  address = {Bangkok, Thailand},<br>
  doi = {10.18653/v1/2021.findings-acl.171}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="zaporojets2021eswa" class="row add-bottom pubentry pub2021 pubir pubarticle">
     <div class="one column">
     <h3 class="article"><span>pubarticle</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>K. Zaporojets, G. Bekoulis, J. Deleu, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2021/zaporojets2021eswa.pdf" target="_blank">"Solving arithmetic word problems by scoring equations with recursive neural networks"</a></pubtitle>, <pubjournal>Expert Syst. Appl.</pubjournal>, Vol. 174, 15 Jul. <pubyear>2021</pubyear>.
  <ul class="file-links">
               <li><a href="papers/2021/zaporojets2021eswa.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.1016/j.eswa.2021.114704" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#zaporojets2021eswa-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#zaporojets2021eswa-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/2009.05639" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         <li><a href="https://github.com/klimzaporojets/arithmetic-word-problems/" target="_blank"><i class="fa fa-github"></i> Code</a></li>
  </ul>
     </div>
     <div id="zaporojets2021eswa-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Solving arithmetic word problems by scoring equations with recursive neural networks</h2>
      <h4>K. Zaporojets, G. Bekoulis, J. Deleu, T. Demeester and C. Develder</h4>
      <br><pubjournal>Expert Syst. Appl.</pubjournal>, Vol. 174, 15 Jul. <pubyear>2021</pubyear>.
      <br><br>
      <p>Solving arithmetic word problems is a cornerstone task in assessing language understanding and reasoning capabilities in NLP systems. Recent works use automatic extraction and ranking of candidate solution equations providing the answer to arithmetic word problems. In this work, we explore novel approaches to score such candidate solution equations using tree-structured recursive neural network (Tree-RNN) configurations. The advantage of this Tree-RNN approach over using more established sequential representations, is that it can naturally capture the structure of the equations. Our proposed method consists of transforming the mathematical expression of the equation into an expression tree. Further, we encode this tree into a Tree-RNN by using different Tree-LSTM architectures. Experimental results show that our proposed method (i) improves overall performance with more than 3% accuracy points compared to previous state-of-the-art, and with over 15% points on a subset of problems that require more complex reasoning, and (ii) outperforms sequential LSTMs by 4% accuracy points on such more complex problems.</p>
    </div>
    <div id="zaporojets2021eswa-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Solving arithmetic word problems by scoring equations with recursive neural networks</h2>
      <h4>K. Zaporojets, G. Bekoulis, J. Deleu, T. Demeester and C. Develder</h4>
      <br><pubjournal>Expert Syst. Appl.</pubjournal>, Vol. 174, 15 Jul. <pubyear>2021</pubyear>.
      <br><br>
      <code>
@article{zaporojets2021eswa,<br>
  author = {Zaporojets, Klim and Bekoulis, Giannis and Deleu, Johannes and Demeester, Thomas and Develder, Chris},<br>
  title = {Solving arithmetic word problems by scoring equations with recursive neural networks},<br>
  journal = {Expert Syst. Appl.},<br>
  month = {15 Jul.},<br>
  year = {2021},<br>
  volume = {174},<br>
  doi = {10.1016/j.eswa.2021.114704}<br>
}
      </code>
    </div>
</div> <!-- end row -->


<div id="zaporojets2021dwie" class="row add-bottom pubentry pub2021 pubir pubarticle">
     <div class="one column">
     <h3 class="article"><span>pubarticle</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>K. Zaporojets, J. Deleu, C. Develder and T. Demeester</pubauth>,
<pubtitle><a href="papers/2021/zaporojets2021dwie.pdf" target="_blank">"DWIE: An entity-centric dataset for multi-task document-level information extraction"</a></pubtitle>, <pubjournal>Inf. Process. Manag.</pubjournal>, Vol. 158, No. 4, Jul. <pubyear>2021</pubyear>.
	<ul class="file-links">
               <li><a href="papers/2021/zaporojets2021dwie.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
	       <li><a href="https://doi.org/10.1016/j.ipm.2021.102563" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
	       
               <li><a class="popup-with-zoom-anim" href="#zaporojets2021dwie-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
	       <li><a class="popup-with-zoom-anim" href="#zaporojets2021dwie-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/2009.12626" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         <li><a href="https://github.com/klimzaporojets/DWIE" target="_blank"><i class="fa fa-github"></i> Code</a></li>
	</ul>
     </div>
     <div id="zaporojets2021dwie-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
	    <h2>DWIE: An entity-centric dataset for multi-task document-level information extraction</h2>
	    <h4>K. Zaporojets, J. Deleu, C. Develder and T. Demeester</h4>
	    <br><pubjournal>Inf. Process. Manag.</pubjournal>, Vol. 158, No. 4, Jul. <pubyear>2021</pubyear>.
	    <br><br>
	    <p>This paper presents DWIE, the ‘Deutsche Welle corpus for Information Extraction’, a newly created multi-task dataset that combines four main Information Extraction (IE) annotation subtasks: (i) Named Entity Recognition (NER), (ii) Coreference Resolution, (iii) Relation Extraction (RE), and (iv) Entity Linking. DWIE is conceived as an entity-centric dataset that describes interactions and properties of conceptual entities on the level of the complete document. This contrasts with currently dominant mention-driven approaches that start from the detection and classification of named entity mentions in individual sentences. Further, DWIE presented two main challenges when building and evaluating IE models for it. First, the use of traditional mention-level evaluation metrics for NER and RE tasks on entity-centric DWIE dataset can result in measurements dominated by predictions on more frequently mentioned entities. We tackle this issue by proposing a new entity-driven metric that takes into account the number of mentions that compose each of the predicted and ground truth entities. Second, the document-level multi-task annotations require the models to transfer information between entity mentions located in different parts of the document, as well as between different tasks, in a joint learning setting. To realize this, we propose to use graph-based neural message passing techniques between document-level mention spans. Our experiments show an improvement of up to 5.5  F1 percentage points when incorporating neural graph propagation into our joint model. This demonstrates DWIE’s potential to stimulate further research in graph neural networks for representation learning in multi-task IE. We make DWIE publicly available at https://github.com/klimzaporojets/DWIE.</p>
    </div>
    <div id="zaporojets2021dwie-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
	    <h2>DWIE: An entity-centric dataset for multi-task document-level information extraction</h2>
	    <h4>K. Zaporojets, J. Deleu, C. Develder and T. Demeester</h4>
	    <br><pubjournal>Inf. Process. Manag.</pubjournal>, Vol. 158, No. 4, Jul. <pubyear>2021</pubyear>.
	    <br><br>
	    <code>
@article{zaporojets2021dwie,<br>
  author = {Zaporojets, Klim and Deleu, Johannes and Develder, Chris and Demeester, Thomas},<br>
  title = {DWIE: An entity-centric dataset for multi-task document-level information extraction},<br>
  journal = {Inf. Process. Manag.},<br>
  month = {Jul.},<br>
  year = {2021},<br>
  volume = {158},<br>
  number = {4},<br>
  doi = {10.1016/j.ipm.2021.102563}<br>
}
	    </code>
	  </div>
</div> <!-- end row -->


<div id="hadifar2021naacl" class="row add-bottom pubentry pub2021 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>A. Hadifar, S. Labat, V. Hoste, C. Develder and T. Demeester</pubauth>,
<pubtitle><a href="papers/2021/hadifar2021naacl.pdf" target="_blank">"A million tweets are worth a few points: Tuning transformers for customer support tasks"</a></pubtitle>, in <pubconf>Proc. Ann. Conf. North American Chapter Assoc. Comp. Linguist. (NAACL 2021)</pubconf>, Online, 6-11 Jun. <pubyear>2021</pubyear>.
  <ul class="file-links">
               <li><a href="papers/2021/hadifar2021naacl.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         
         <li><a href="https://www.aclweb.org/anthology/2021.naacl-main.21/" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#hadifar2021naacl-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#hadifar2021naacl-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/2104.07944" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         <li><a href="http://github.com/hadifar/customerservicetasks" target="_blank"><i class="fa fa-github"></i> Code</a></li>
  </ul>
     </div>
     <div id="hadifar2021naacl-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>A million tweets are worth a few points: Tuning transformers for customer support tasks</h2>
      <h4>A. Hadifar, S. Labat, V. Hoste, C. Develder and T. Demeester</h4>
      <br>in <pubconf>Proc. Ann. Conf. North American Chapter Assoc. Comp. Linguist. (NAACL 2021)</pubconf>, Online, 6-11 Jun. <pubyear>2021</pubyear>.
      <br><br>
      <p>In online domain-specific customer service applications, many companies struggle to deploy advanced NLP models successfully, due to the limited availability of and noise in their datasets. While prior research demonstrated the potential of migrating large open-domain pretrained models for domain-specific tasks, the appropriate (pre)training strategies have not yet been rigorously evaluated in such social media customer service settings, especially under multilingual conditions. We address this gap by (i) collecting a multilingual social media corpus containing customer service conversations (865k tweets), (ii) comparing various pipelines of pretraining and fine- tuning approaches, (iii) applying them on 5 different end tasks. We show that pretraining a generic multilingual transformer model on our in-domain dataset, before finetuning on specific end tasks, consistently boosts performance, especially in non-English settings.</p>
    </div>
    <div id="hadifar2021naacl-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>A million tweets are worth a few points: Tuning transformers for customer support tasks</h2>
      <h4>A. Hadifar, S. Labat, V. Hoste, C. Develder and T. Demeester</h4>
      <br>in <pubconf>Proc. Ann. Conf. North American Chapter Assoc. Comp. Linguist. (NAACL 2021)</pubconf>, Online, 6-11 Jun. <pubyear>2021</pubyear>.
      <br><br>
      <code>
@inproceedings{hadifar2021naacl,<br>
  author = {Hadifar, Amir and Labat, Sofie and Hoste, Veronique and Develder, Chris and Demeester, Thomas},<br>
  title = {A million tweets are worth a few points: Tuning transformers for customer support tasks},<br>
  booktitle = {Proc. Ann. Conf. North American Chapter Assoc. Comp. Linguist. (NAACL 2021)},<br>
  month = {6--11 Jun.},<br>
  year = {2021},<br>
  address = {Online},<br>
  url = {https://www.aclweb.org/anthology/2021.naacl-main.21/}<br>
}
      </code>
    </div>
</div> <!-- end row -->

       
<div id="jiang2020aacl" class="row add-bottom pubentry pub2020 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>Y. Jiang, K. Zaporojets, J. Deleu, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2020/jiang2020aacl.pdf" target="_blank">"Recipe instruction semantics corpus (RISeC): Resolving semantic structure and zero anaphora in recipes"</a></pubtitle>, in <pubconf>Proc. 1st Conf. Asia-Pacific Chapter of the Assoc. Comput. Linguist. and 10th Int. Joint Conf. Natural Lang. Processing (AACL-IJCNLP 2020)</pubconf>, Online, 4-7 Dec. <pubyear>2020</pubyear>, pp. 821-826.
  <ul class="file-links">
               <li><a href="papers/2020/jiang2020aacl.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         
         <li><a href="https://www.aclweb.org/anthology/2020.aacl-main.82" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#jiang2020aacl-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#jiang2020aacl-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="jiang2020aacl-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Recipe instruction semantics corpus (RISeC): Resolving semantic structure and zero anaphora in recipes</h2>
      <h4>Y. Jiang, K. Zaporojets, J. Deleu, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Proc. 1st Conf. Asia-Pacific Chapter of the Assoc. Comput. Linguist. and 10th Int. Joint Conf. Natural Lang. Processing (AACL-IJCNLP 2020)</pubconf>, Online, 4-7 Dec. <pubyear>2020</pubyear>, pp. 821-826.
      <br><br>
      <p>We propose a newly annotated dataset for information extraction on recipes. Unlike previous approaches to machine comprehension of procedural texts, we avoid a priori pre-defining domain-specific predicates to recognize (e.g., the primitive instructions in MILK) and focus on basic understanding of the expressed semantics rather than directly reduce them to a simplified state representation (e.g., ProPara). We thus frame the semantic comprehension of procedural text such as recipes, as fairly generic NLP subtasks, covering (i) entity recognition (ingredients, tools and actions), (ii) relation extraction (what ingredients and tools are involved in the actions), and (iii) zero anaphora resolution (link actions to implicit arguments, e.g., results from previous recipe steps). Further, our Recipe Instruction Semantic Corpus (RISeC) dataset includes textual descriptions for the zero anaphora, to facilitate language generation thereof. Besides the dataset itself, we contribute a pipeline neural architecture that addresses entity and relation extraction as well as identification of zero anaphora. These basic building blocks can facilitate more advanced downstream applications (e.g., question answering, conversational agents).</p>
    </div>
    <div id="jiang2020aacl-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Recipe instruction semantics corpus (RISeC): Resolving semantic structure and zero anaphora in recipes</h2>
      <h4>Y. Jiang, K. Zaporojets, J. Deleu, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Proc. 1st Conf. Asia-Pacific Chapter of the Assoc. Comput. Linguist. and 10th Int. Joint Conf. Natural Lang. Processing (AACL-IJCNLP 2020)</pubconf>, Online, 4-7 Dec. <pubyear>2020</pubyear>, pp. 821-826.
      <br><br>
      <code>
@inproceedings{jiang2020aacl,<br>
  author = {Jiang, Yiwei and Zaporojets, Klim and Deleu, Johannes and Demeester, Thomas and Develder, Chris},<br>
  title = {Recipe instruction semantics corpus (RISeC): Resolving semantic structure and zero anaphora in recipes},<br>
  booktitle = {Proc. 1st Conf. Asia-Pacific Chapter of the Assoc. Comput. Linguist. and 10th Int. Joint Conf. Natural Lang. Processing (AACL-IJCNLP 2020)},<br>
  month = {4--7 Dec.},<br>
  year = {2020},<br>
  pages = {821--826},<br>
  address = {Online},<br>
  url = {https://www.aclweb.org/anthology/2020.aacl-main.82}<br>
}
      </code>
    </div>
</div> <!-- end row -->


<div id="bekoulis2019benelearn" class="row add-bottom pubentry pub2019 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>G. Bekoulis, J. Deleu, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2019/bekoulis2019benelearn.pdf" target="_blank">"Adversarial perturbations for joint entity and relation extraction"</a></pubtitle>, in <pubconf>Proc. 28th Belgian Dutch Conf. Machine Learn. (BeneLearn 2019)</pubconf>, Brussels, Belgium, 6-8 Nov. <pubyear>2019</pubyear>.
  <ul class="file-links">
               <li><a href="papers/2019/bekoulis2019benelearn.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         
         <li><a href="http://ceur-ws.org/Vol-2491/abstract5.pdf" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#bekoulis2019benelearn-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#bekoulis2019benelearn-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="bekoulis2019benelearn-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Adversarial perturbations for joint entity and relation extraction</h2>
      <h4>G. Bekoulis, J. Deleu, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Proc. 28th Belgian Dutch Conf. Machine Learn. (BeneLearn 2019)</pubconf>, Brussels, Belgium, 6-8 Nov. <pubyear>2019</pubyear>.
      <br><br>
      <p>The goal of the entity recognition and relation extraction task is to discover relational structures of entity mentions from unstructured texts. It is a central problem in information extraction since it is critical for tasks such as knowledge base population and question answering. In this work, we focus on extending the training procedure of our newly proposed general purpose joint model [4] for entity recognition and relation extraction with adversarial training (AT) [2]. Our model performs the two tasks of entity recognition and relation extraction simultaneously. It achieves state-of-the-art performance in a number of different contexts (i.e., news, biomedical, real estate) and languages (i.e., English, Dutch) without relying on any manually engineered features nor additional NLP tools. In summary, our proposed model: (i) does not rely on external NLP tools nor hand-crafted features, (ii) entities and relations within the same text fragment (typically a sentence) are extracted simultaneously, where (iii) an entity can be involved in multiple relations at once. To evaluate the proposed AT method, we perform the same set of experiments while we apply AT on top of our joint model. Compared to the baseline model, applying AT during training leads to a consistent additional increase in joint extraction effectiveness.</p>
    </div>
    <div id="bekoulis2019benelearn-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Adversarial perturbations for joint entity and relation extraction</h2>
      <h4>G. Bekoulis, J. Deleu, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Proc. 28th Belgian Dutch Conf. Machine Learn. (BeneLearn 2019)</pubconf>, Brussels, Belgium, 6-8 Nov. <pubyear>2019</pubyear>.
      <br><br>
      <code>
@inproceedings{bekoulis2019benelearn,<br>
  author = {Bekoulis, Giannis and Deleu, Johannes and Demeester, Thomas and Develder, Chris},<br>
  title = {Adversarial perturbations for joint entity and relation extraction},<br>
  booktitle = {Proc. 28th Belgian Dutch Conf. Machine Learn. (BeneLearn 2019)},<br>
  month = {6--8 Nov.},<br>
  year = {2019},<br>
  address = {Brussels, Belgium},<br>
  url = {http://ceur-ws.org/Vol-2491/abstract5.pdf}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="deraedt2019" class="row add-bottom pubentry pub2019 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>L. De Raedt, R. Manhaeve, S. Duman&ccaron;i&cacute;, T. Demeester and A. Kimmig</pubauth>,
<pubtitle><a href="papers/2019/deraedt2019.pdf" target="_blank">"Neuro-Symbolic = Neural + Logical + Probabilistic"</a></pubtitle>, in <pubconf>Proc. 14th Int. Workshop Neural-Symbolic Learn. and Reasoning (NeSy 2019 @ IJCAI 2019)</pubconf>, Macao, China, 12 Aug. <pubyear>2019</pubyear>.
  <ul class="file-links">
               <li><a href="papers/2019/deraedt2019.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         
         
               <li><a class="popup-with-zoom-anim" href="#deraedt2019-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#deraedt2019-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="deraedt2019-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Neuro-Symbolic = Neural + Logical + Probabilistic</h2>
      <h4>L. De Raedt, R. Manhaeve, S. Duman&ccaron;i&cacute;, T. Demeester and A. Kimmig</h4>
      <br>in <pubconf>Proc. 14th Int. Workshop Neural-Symbolic Learn. and Reasoning (NeSy 2019 @ IJCAI 2019)</pubconf>, Macao, China, 12 Aug. <pubyear>2019</pubyear>.
      <br><br>
      <p>The overall goal of neuro-symbolic computation is to integrate high-level reasoning with low-level perception. We argue (1) that neuro-symbolic computation should integrate neural networks with the two most prominent methods for reasoning, that is, logic and probability, and (2) that neuro-symbolic integrated methods should have the pure neural, logical and probabilistic methods as special cases. We examine the state-of-the-art with regard to these claims and briefly position our own contribution DeepProbLog in this perspective.</p>
    </div>
    <div id="deraedt2019-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Neuro-Symbolic = Neural + Logical + Probabilistic</h2>
      <h4>L. De Raedt, R. Manhaeve, S. Duman&ccaron;i&cacute;, T. Demeester and A. Kimmig</h4>
      <br>in <pubconf>Proc. 14th Int. Workshop Neural-Symbolic Learn. and Reasoning (NeSy 2019 @ IJCAI 2019)</pubconf>, Macao, China, 12 Aug. <pubyear>2019</pubyear>.
      <br><br>
      <code>
@inproceedings{deraedt2019,<br>
  author = {De Raedt, Luc and Manhaeve, Robin and Duman&ccaron;i&cacute;, Sebastijan and Demeester, Thomas and Kimmig, Angelika},<br>
  title = {Neuro-Symbolic = Neural + Logical + Probabilistic},<br>
  booktitle = {Proc. 14th Int. Workshop Neural-Symbolic Learn. and Reasoning (NeSy 2019 @ IJCAI 2019)},<br>
  month = {12 Aug.},<br>
  year = {2019},<br>
  address = {Macao, China}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="hadifar2019repl4nlp" class="row add-bottom pubentry pub2019 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>A. Hadifar, L. Sterckx, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2019/hadifar2019repl4nlp.pdf" target="_blank">"A self-training approach for short text clustering"</a></pubtitle>, in <pubconf>Proc. 4th Workshop Represent. Learn. for NLP (RepL4NLP) at ACL 2019</pubconf>, Florence, Italy, 2 Aug. <pubyear>2019</pubyear>, pp. 194-199.
  <ul class="file-links">
               <li><a href="papers/2019/hadifar2019repl4nlp.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         
         <li><a href="https://www.aclweb.org/anthology/papers/W/W19/W19-4322/" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#hadifar2019repl4nlp-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#hadifar2019repl4nlp-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         <li><a href="https://github.com/hadifar/stc_clustering" target="_blank"><i class="fa fa-github"></i> Code</a></li>
  </ul>
     </div>
     <div id="hadifar2019repl4nlp-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>A self-training approach for short text clustering</h2>
      <h4>A. Hadifar, L. Sterckx, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Proc. 4th Workshop Represent. Learn. for NLP (RepL4NLP) at ACL 2019</pubconf>, Florence, Italy, 2 Aug. <pubyear>2019</pubyear>, pp. 194-199.
      <br><br>
      <p>Short text clustering is a challenging problem when adopting traditional bag-of-words or TF-IDF representations, since these lead to sparse vector representations of the short texts. Low-dimensional continuous representations or embeddings can counter that sparseness problem: their high representational power is exploited in deep clustering algorithms. While deep clustering has been studied extensively in computer vision, relatively little work has focused on NLP. The method we propose, learns discriminative features from both an autoencoder and a sentence embedding, then uses assignments from a clustering algorithm as supervision to update weights of the encoder network. Experiments on three short text datasets empirically validate the effectiveness of our method.</p>
    </div>
    <div id="hadifar2019repl4nlp-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>A self-training approach for short text clustering</h2>
      <h4>A. Hadifar, L. Sterckx, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Proc. 4th Workshop Represent. Learn. for NLP (RepL4NLP) at ACL 2019</pubconf>, Florence, Italy, 2 Aug. <pubyear>2019</pubyear>, pp. 194-199.
      <br><br>
      <code>
@inproceedings{hadifar2019repl4nlp,<br>
  author = {Hadifar, Amir and Sterckx, Lucas and Demeester, Thomas and Develder, Chris},<br>
  title = {A self-training approach for short text clustering},<br>
  booktitle = {Proc. 4th Workshop Represent. Learn. for NLP (RepL4NLP) at ACL 2019},<br>
  month = {2 Aug.},<br>
  year = {2019},<br>
  pages = {194--199},<br>
  address = {Florence, Italy},<br>
  url = {https://www.aclweb.org/anthology/papers/W/W19/W19-4322/}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="deboom2019" class="row add-bottom pubentry pub2019 pubir pubarticle">
     <div class="one column">
     <h3 class="article"><span>pubarticle</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>C. De Boom, T. Demeester and B. Dhoedt</pubauth>,
<pubtitle><a href="papers/2019/deboom2019.pdf" target="_blank">"Character-level recurrent neural networks in practice: comparing training and sampling schemes"</a></pubtitle>, <pubjournal>Neural Comput. &amp; Applic.</pubjournal>, Vol. 31, No. 8, Aug. <pubyear>2019</pubyear>, pp. 4001-4017.
  <ul class="file-links">
               <li><a href="papers/2019/deboom2019.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.1007/s00521-017-3322-z" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#deboom2019-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#deboom2019-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/1801.00632" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         <li><a href="https://github.com/cedricdeboom/character-level-rnn-datasets" target="_blank"><i class="fa fa-github"></i> Code</a></li>
  </ul>
     </div>
     <div id="deboom2019-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Character-level recurrent neural networks in practice: comparing training and sampling schemes</h2>
      <h4>C. De Boom, T. Demeester and B. Dhoedt</h4>
      <br><pubjournal>Neural Comput. &amp; Applic.</pubjournal>, Vol. 31, No. 8, Aug. <pubyear>2019</pubyear>, pp. 4001-4017.
      <br><br>
      <p>Recurrent neural networks are nowadays successfully used in an abundance of applications, going from text, speech and image processing to recommender systems. Backpropagation through time is the algorithm that is commonly used to train these networks on specific tasks. Many deep learning frameworks have their own implementation of training and sampling procedures for recurrent neural networks, while there are in fact multiple other possibilities to choose from and other parameters to tune. In the existing literature, this is very often overlooked or ignored. In this paper, we therefore give an overview of possible training and sampling schemes for character-level recurrent neural networks to solve the task of predicting the next token in a given sequence. We test these different schemes on a variety of datasets, neural network architectures and parameter settings, and formulate a number of take-home recommendations. The choice of training and sampling scheme turns out to be subject to a number of trade-offs, such as training stability, sampling time, model performance and implementation effort, but is largely independent of the data. Perhaps the most surprising result is that transferring hidden states for correctly initializing the model on subsequences often leads to unstable training behavior depending on the dataset.</p>
    </div>
    <div id="deboom2019-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Character-level recurrent neural networks in practice: comparing training and sampling schemes</h2>
      <h4>C. De Boom, T. Demeester and B. Dhoedt</h4>
      <br><pubjournal>Neural Comput. &amp; Applic.</pubjournal>, Vol. 31, No. 8, Aug. <pubyear>2019</pubyear>, pp. 4001-4017.
      <br><br>
      <code>
@article{deboom2019,<br>
  author = {De Boom, Cedric and Demeester, Thomas and Dhoedt, Bart},<br>
  title = {Character-level recurrent neural networks in practice: comparing training and sampling schemes},<br>
  journal = {Neural Comput. &amp; Applic.},<br>
  month = {Aug.},<br>
  year = {2019},<br>
  volume = {31},<br>
  number = {8},<br>
  pages = {4001--4017},<br>
  doi = {10.1007/s00521-017-3322-z}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="bitew2019clpsych" class="row add-bottom pubentry pub2019 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>S.K. Bitew, G. Bekoulis, J. Deleu, L. Sterckx, K. Zaporojets, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2019/bitew2019clpsych.pdf" target="_blank">"Predicting suicide risk from online postings in Reddit: The UGent-IDLab submission to the CLPysch 2019 Shared Task A"</a></pubtitle>, in <pubconf>Proc. 6th Ann. Workshop on Comput. Ling. Clin. Psychol. (CLPsych 2019) at NAACL-HLT 2019</pubconf>, Minneapolis, MN, USA, 6 Jun. <pubyear>2019</pubyear>, pp. 158-161.
  <ul class="file-links">
               <li><a href="papers/2019/bitew2019clpsych.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.18653/v1/W19-3019" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         <li><a href="https://www.aclweb.org/anthology/papers/W/W19/W19-3019/" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#bitew2019clpsych-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#bitew2019clpsych-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="bitew2019clpsych-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Predicting suicide risk from online postings in Reddit: The UGent-IDLab submission to the CLPysch 2019 Shared Task A</h2>
      <h4>S.K. Bitew, G. Bekoulis, J. Deleu, L. Sterckx, K. Zaporojets, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Proc. 6th Ann. Workshop on Comput. Ling. Clin. Psychol. (CLPsych 2019) at NAACL-HLT 2019</pubconf>, Minneapolis, MN, USA, 6 Jun. <pubyear>2019</pubyear>, pp. 158-161.
      <br><br>
      <p>This paper describes IDLab’s text classifica-tion systems submitted to Task A as part of the CLPsych 2019 shared task. The aim of this shared task was to develop automated sys-tems that predict the degree of suicide risk of people based on their posts on Reddit. Bag-of-words features, emotion features and post-level predictions are used to derive user-levelpredictions. Linear models and ensembles of these models are used to predict final scores. We find that predicting fine-grained risk levels is much more difficult than flagging potentially at-risk users. Furthermore, we do not find clear added value from building richer ensembles compared to simple baselines, given the available training data and the nature of the prediction task.</p>
    </div>
    <div id="bitew2019clpsych-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Predicting suicide risk from online postings in Reddit: The UGent-IDLab submission to the CLPysch 2019 Shared Task A</h2>
      <h4>S.K. Bitew, G. Bekoulis, J. Deleu, L. Sterckx, K. Zaporojets, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Proc. 6th Ann. Workshop on Comput. Ling. Clin. Psychol. (CLPsych 2019) at NAACL-HLT 2019</pubconf>, Minneapolis, MN, USA, 6 Jun. <pubyear>2019</pubyear>, pp. 158-161.
      <br><br>
      <code>
@inproceedings{bitew2019clpsych,<br>
  author = {Bitew, Semere Kiros and Giannis Bekoulis and Johannes Deleu and Lucas Sterckx and Klim Zaporojets and Thomas Demeester and Chris Develder},<br>
  title = {Predicting suicide risk from online postings in Reddit: The UGent-IDLab submission to the CLPysch 2019 Shared Task A},<br>
  booktitle = {Proc. 6th Ann. Workshop on Comput. Ling. Clin. Psychol. (CLPsych 2019) at NAACL-HLT 2019},<br>
  month = {6 Jun.},<br>
  year = {2019},<br>
  pages = {158--161},<br>
  address = {Minneapolis, MN, USA},<br>
  url = {https://www.aclweb.org/anthology/papers/W/W19/W19-3019/},<br>
  doi = {10.18653/v1/W19-3019}<br>
}
      </code>
    </div>
</div> <!-- end row -->


<div id="bekoulis2019naacl" class="row add-bottom pubentry pub2019 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>G. Bekoulis, J. Deleu, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2019/bekoulis2019naacl.pdf" target="_blank">"Sub-event detection from Twitter streams as a sequence labeling problem"</a></pubtitle>, in <pubconf>Proc. Ann. Conf. North American Chapter Assoc. Comp. Linguist. (NAACL-HLT 2019)</pubconf>, Minneapolis, MN, USA, 3-5 Jun. <pubyear>2019</pubyear>, pp. 745-750.
  <ul class="file-links">
               <li><a href="papers/2019/bekoulis2019naacl.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.18653/v1/N19-1081" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         <li><a href="https://www.aclweb.org/anthology/papers/N/N19/N19-1081/" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#bekoulis2019naacl-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#bekoulis2019naacl-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/1903.05396" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         <li><a href="https://github.com/bekou/subevent_sequence_labeling" target="_blank"><i class="fa fa-github"></i> Code</a></li>
  </ul>
     </div>
     <div id="bekoulis2019naacl-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Sub-event detection from Twitter streams as a sequence labeling problem</h2>
      <h4>G. Bekoulis, J. Deleu, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Proc. Ann. Conf. North American Chapter Assoc. Comp. Linguist. (NAACL-HLT 2019)</pubconf>, Minneapolis, MN, USA, 3-5 Jun. <pubyear>2019</pubyear>, pp. 745-750.
      <br><br>
      <p>This paper introduces improved methods for sub-event detection in social media streams ,by applying neural sequence models not only on the level of individual posts, but also directly on the stream level. Current approaches to identify sub-events within a given event (e.g., a goal during a soccer match), essentially do not exploit the sequential nature of social media streams. We address this shortcoming by framing the sub-event detection problem in social media streams as a sequence labeling task and adopt a neural sequence architecture that explicitly accounts for the chronological order of posts. Specifically, we (i) establish aneural baseline that outperforms a graph-based state-of-the-art method for binary sub-event detection (2.7% F1 improvement), as well as (ii) demonstrate superiority of a recurrent neural network model on the posts sequence level for labeled sub-events (2.4% F1 improvement over non-sequential models).</p>
    </div>
    <div id="bekoulis2019naacl-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Sub-event detection from Twitter streams as a sequence labeling problem</h2>
      <h4>G. Bekoulis, J. Deleu, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Proc. Ann. Conf. North American Chapter Assoc. Comp. Linguist. (NAACL-HLT 2019)</pubconf>, Minneapolis, MN, USA, 3-5 Jun. <pubyear>2019</pubyear>, pp. 745-750.
      <br><br>
      <code>
@inproceedings{Bekoulis2019NAACL,<br>
  author = {Bekoulis, Giannis and Deleu, Johannes and Demeester, Thomas and Develder, Chris},<br>
  title = {Sub-event detection from Twitter streams as a sequence labeling problem},<br>
  booktitle = {Proc. Ann. Conf. North American Chapter Assoc. Comp. Linguist. (NAACL-HLT 2019)},<br>
  month = {3--5 Jun.},<br>
  year = {2019},<br>
  pages = {745--750},<br>
  address = {Minneapolis, MN, USA},<br>
  url = {https://www.aclweb.org/anthology/papers/N/N19/N19-1081/},<br>
  doi = {10.18653/v1/N19-1081}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="bekoulis2018eswa2" class="row add-bottom pubentry pub2018 pubir pubarticle">
     <div class="one column">
     <h3 class="article"><span>pubarticle</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>G. Bekoulis, J. Deleu, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2018/bekoulis2018eswa2.pdf" target="_blank">"Joint entity recognition and relation extraction as a multi-head selection problem"</a></pubtitle>, <pubjournal>Expert Syst. Appl.</pubjournal>, Vol. 114, Dec. <pubyear>2018</pubyear>, pp. 34-45.
  <ul class="file-links">
               <li><a href="papers/2018/bekoulis2018eswa2.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.1016/j.eswa.2018.07.032" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#bekoulis2018eswa2-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#bekoulis2018eswa2-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/1804.07847" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         <li><a href="https://github.com/bekou/multihead_joint_entity_relation_extraction" target="_blank"><i class="fa fa-github"></i> Code</a></li>
  </ul>
     </div>
     <div id="bekoulis2018eswa2-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Joint entity recognition and relation extraction as a multi-head selection problem</h2>
      <h4>G. Bekoulis, J. Deleu, T. Demeester and C. Develder</h4>
      <br><pubjournal>Expert Syst. Appl.</pubjournal>, Vol. 114, Dec. <pubyear>2018</pubyear>, pp. 34-45.
      <br><br>
      <p>State-of-the-art models for joint entity recognition and relation extraction strongly rely on external natural language processing (NLP) tools such as POS (part-of-speech) taggers and dependency parsers. Thus, the performance of such joint models depends on the quality of the features obtained from these NLP tools. However, these features are not always accurate for various languages and contexts. In this paper, we propose a joint neural model which performs entity recognition and relation extraction simultaneously, without the need of any manually extracted features or the use of any external tool. Specifically, we model the entity recognition task using a CRF (Conditional Random Fields) layer and the relation extraction task as a multi-head selection problem (i.e., potentially identify multiple relations for each entity). We present an extensive experimental setup, to demonstrate the effectiveness of our method using datasets from various contexts (i.e., news, biomedical, real estate) and languages (i.e., English, Dutch). Our model outperforms the previous neural models that use automatically extracted features, while it performs within a reasonable margin of feature-based neural models, or even beats them.</p>
    </div>
    <div id="bekoulis2018eswa2-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Joint entity recognition and relation extraction as a multi-head selection problem</h2>
      <h4>G. Bekoulis, J. Deleu, T. Demeester and C. Develder</h4>
      <br><pubjournal>Expert Syst. Appl.</pubjournal>, Vol. 114, Dec. <pubyear>2018</pubyear>, pp. 34-45.
      <br><br>
      <code>
@article{bekoulis2018eswa2,<br>
  author = {Giannis Bekoulis and Johannes Deleu and Thomas Demeester and Chris Develder},<br>
  title = {Joint entity recognition and relation extraction as a multi-head selection problem},<br>
  journal = {Expert Syst. Appl.},<br>
  month = {Dec.},<br>
  year = {2018},<br>
  volume = {114},<br>
  pages = {34--45},<br>
  doi = {10.1016/j.eswa.2018.07.032}<br>
}
      </code>
    </div>
</div> <!-- end row -->


<div id="bekoulis2018emnlp" class="row add-bottom pubentry pub2018 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>G. Bekoulis, J. Deleu, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2018/bekoulis2018emnlp.pdf" target="_blank">"Adversarial training for multi-context joint entity and relation extraction"</a></pubtitle>, in <pubconf>Proc. Conf. Empirical Methods in Natural Lang. Processing (EMNLP 2018)</pubconf>, Brussels, Belgium, 31 Oct. - 4 Nov. <pubyear>2018</pubyear>, pp. 2830-36.
  <ul class="file-links">
               <li><a href="papers/2018/bekoulis2018emnlp.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.18653/v1/D18-1307" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         <li><a href="https://www.aclweb.org/anthology/papers/D/D18/D18-1307/" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#bekoulis2018emnlp-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#bekoulis2018emnlp-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/1808.06876" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         <li><a href="https://github.com/bekou/multihead_joint_entity_relation_extraction" target="_blank"><i class="fa fa-github"></i> Code</a></li>
  </ul>
     </div>
     <div id="bekoulis2018emnlp-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Adversarial training for multi-context joint entity and relation extraction</h2>
      <h4>G. Bekoulis, J. Deleu, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Proc. Conf. Empirical Methods in Natural Lang. Processing (EMNLP 2018)</pubconf>, Brussels, Belgium, 31 Oct. - 4 Nov. <pubyear>2018</pubyear>, pp. 2830-36.
      <br><br>
      <p>Adversarial training (AT) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data. We show how to use AT for the tasks of entity recognition and relation extraction. In particular, we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations, allows improving the state-of-the-art effectiveness on several datasets in different contexts (i.e., news, biomedical, and real estate data) and for different languages (English and Dutch).</p>
    </div>
    <div id="bekoulis2018emnlp-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Adversarial training for multi-context joint entity and relation extraction</h2>
      <h4>G. Bekoulis, J. Deleu, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Proc. Conf. Empirical Methods in Natural Lang. Processing (EMNLP 2018)</pubconf>, Brussels, Belgium, 31 Oct. - 4 Nov. <pubyear>2018</pubyear>, pp. 2830-36.
      <br><br>
      <code>
@inproceedings{bekoulis2018emnlp,<br>
  author = {Bekoulis, Giannis and Deleu, Johannes and Demeester, Thomas and Develder, Chris},<br>
  title = {Adversarial training for multi-context joint entity and relation extraction},<br>
  booktitle = {Proc. Conf. Empirical Methods in Natural Lang. Processing (EMNLP 2018)},<br>
  month = {31 Oct. -- 4 Nov.},<br>
  year = {2018},<br>
  pages = {2830--36},<br>
  address = {Brussels, Belgium},<br>
  url = {https://www.aclweb.org/anthology/papers/D/D18/D18-1307/},<br>
  doi = {10.18653/v1/D18-1307}<br>
}
      </code>
    </div>
</div> <!-- end row -->


<div id="godin2018emnlp" class="row add-bottom pubentry pub2018 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>F. Godin, K. Demuynck, J. Dambre, W. De Neve and T. Demeester</pubauth>,
<pubtitle><a href="papers/2018/godin2018emnlp.pdf" target="_blank">"Explaining character-aware neural networks for word-level prediction: Do they discover linguistic rules?"</a></pubtitle>, in <pubconf>Proc. Conf. Empirical Methods in Natural Lang. Processing (EMNLP 2018)</pubconf>, Brussels, Belgium, 31 Oct. - 4 Nov. <pubyear>2018</pubyear>, pp. 3275-3284.
  <ul class="file-links">
               <li><a href="papers/2018/godin2018emnlp.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.18653/v1/D18-1365" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         <li><a href="https://www.aclweb.org/anthology/D18-1365" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#godin2018emnlp-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#godin2018emnlp-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/1808.09551" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         
  </ul>
     </div>
     <div id="godin2018emnlp-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Explaining character-aware neural networks for word-level prediction: Do they discover linguistic rules?</h2>
      <h4>F. Godin, K. Demuynck, J. Dambre, W. De Neve and T. Demeester</h4>
      <br>in <pubconf>Proc. Conf. Empirical Methods in Natural Lang. Processing (EMNLP 2018)</pubconf>, Brussels, Belgium, 31 Oct. - 4 Nov. <pubyear>2018</pubyear>, pp. 3275-3284.
      <br><br>
      <p>Character-level features are currently used in different neural network-based natural language processing algorithms. However, little is known about the character-level patterns those models learn. Moreover, models are often compared only quantitatively while a qualitative analysis is missing. In this paper, we investigate which character-level patterns neural networks learn and if those patterns coincide with manually-defined word segmentations and annotations. To that end, we extend the contextual decomposition technique (Murdoch et al. 2018) to convolutional neural networks which allows us to compare convolutional neural networks and bidirectional long short-term memory networks. We evaluate and compare these models for the task of morphological tagging on three morphologically different languages and show that these models implicitly discover understandable linguistic rules.</p>
    </div>
    <div id="godin2018emnlp-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Explaining character-aware neural networks for word-level prediction: Do they discover linguistic rules?</h2>
      <h4>F. Godin, K. Demuynck, J. Dambre, W. De Neve and T. Demeester</h4>
      <br>in <pubconf>Proc. Conf. Empirical Methods in Natural Lang. Processing (EMNLP 2018)</pubconf>, Brussels, Belgium, 31 Oct. - 4 Nov. <pubyear>2018</pubyear>, pp. 3275-3284.
      <br><br>
      <code>
@inproceedings{godin2018emnlp,<br>
  author = {Godin, Frederic and Kris Demuynck and Joni Dambre and De Neve, Wesley and Thomas Demeester},<br>
  title = {Explaining character-aware neural networks for word-level prediction: Do they discover linguistic rules?},<br>
  booktitle = {Proc. Conf. Empirical Methods in Natural Lang. Processing (EMNLP 2018)},<br>
  month = {31 Oct. -- 4 Nov.},<br>
  year = {2018},<br>
  pages = {3275--3284},<br>
  address = {Brussels, Belgium},<br>
  url = {https://www.aclweb.org/anthology/D18-1365},<br>
  doi = {10.18653/v1/D18-1365}<br>
}
      </code>
    </div>
</div> <!-- end row -->



<div id="demeester2018conll" class="row add-bottom pubentry pub2018 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>T. Demeester, J. Deleu, F. Godin and C. Develder</pubauth>,
<pubtitle><a href="papers/2018/demeester2018conll.pdf" target="_blank">"Predefined sparseness in recurrent sequence models"</a></pubtitle>, in <pubconf>Proc. SIGNLL Conf. Comput. Lang. Learn. (CoNLL 2018)</pubconf>, Brussels, Belgium, 31 Oct. - 1 Nov. <pubyear>2018</pubyear>, pp. 324-333.
  <ul class="file-links">
               <li><a href="papers/2018/demeester2018conll.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.18653/v1/K18-1032" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         <li><a href="https://www.aclweb.org/anthology/papers/K/K18/K18-1032/" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#demeester2018conll-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#demeester2018conll-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/1808.08720" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         <li><a href="https://github.com/tdmeeste/SparseSeqModels" target="_blank"><i class="fa fa-github"></i> Code</a></li>
  </ul>
     </div>
     <div id="demeester2018conll-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Predefined sparseness in recurrent sequence models</h2>
      <h4>T. Demeester, J. Deleu, F. Godin and C. Develder</h4>
      <br>in <pubconf>Proc. SIGNLL Conf. Comput. Lang. Learn. (CoNLL 2018)</pubconf>, Brussels, Belgium, 31 Oct. - 1 Nov. <pubyear>2018</pubyear>, pp. 324-333.
      <br><br>
      <p>Inducing sparseness while training neural networks has been shown to yield models with a lower memory footprint but similar effectiveness to dense models. However, sparseness is typically induced starting from a dense model, and thus this advantage does not hold during training. We propose techniques to enforce sparseness upfront in recurrent sequence models for NLP applications, to also benefit training. First, in language modeling, we show how to increase hidden state sizes in recurrent layers without increasing the number of parameters, leading to more expressive models. Second, for sequence labeling, we show that word embeddings with predefined sparseness lead to similar performance as dense embeddings, at a fraction of the number of trainable parameters.</p>
    </div>
    <div id="demeester2018conll-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Predefined sparseness in recurrent sequence models</h2>
      <h4>T. Demeester, J. Deleu, F. Godin and C. Develder</h4>
      <br>in <pubconf>Proc. SIGNLL Conf. Comput. Lang. Learn. (CoNLL 2018)</pubconf>, Brussels, Belgium, 31 Oct. - 1 Nov. <pubyear>2018</pubyear>, pp. 324-333.
      <br><br>
      <code>
@inproceedings{demeester2018conll,<br>
  author = {Demeester, Thomas and Deleu, Johannes and Godin, Frederic and Develder, Chris},<br>
  title = {Predefined sparseness in recurrent sequence models},<br>
  booktitle = {Proc. SIGNLL Conf. Comput. Lang. Learn. (CoNLL 2018)},<br>
  month = {31 Oct. -- 1 Nov.},<br>
  year = {2018},<br>
  pages = {324--333},<br>
  address = {Brussels, Belgium},<br>
  url = {https://www.aclweb.org/anthology/papers/K/K18/K18-1032/},<br>
  doi = {10.18653/v1/K18-1032}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="bekoulis2018eswa" class="row add-bottom pubentry pub2018 pubir pubarticle">
     <div class="one column">
     <h3 class="article"><span>pubarticle</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>G. Bekoulis, J. Deleu, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2018/bekoulis2018eswa.pdf" target="_blank">"An attentive neural architecture for joint segmentation and parsing and its application to real estate ads"</a></pubtitle>, <pubjournal>Expert Syst. Appl.</pubjournal>, Vol. 102, 15 Jul. <pubyear>2018</pubyear>, pp. 100-112.
  <ul class="file-links">
               <li><a href="papers/2018/bekoulis2018eswa.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.1016/j.eswa.2018.02.031" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#bekoulis2018eswa-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#bekoulis2018eswa-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/1709.09590" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
  </ul>
     </div>
     <div id="bekoulis2018eswa-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>An attentive neural architecture for joint segmentation and parsing and its application to real estate ads</h2>
      <h4>G. Bekoulis, J. Deleu, T. Demeester and C. Develder</h4>
      <br><pubjournal>Expert Syst. Appl.</pubjournal>, Vol. 102, 15 Jul. <pubyear>2018</pubyear>, pp. 100-112.
      <br><br>
      <p>In processing human produced text using natural language processing (NLP) techniques, two fundamental subtasks that arise are (i)&nbsp;segmentation of the plain text into meaningful subunits (e.g., entities), and (ii)&nbsp;dependency parsing, to establish relations between subunits. Such structural interpretation of text provides essential building blocks for upstream expert system tasks: e.g., from interpreting textual real estate ads, one may want to provide an accurate price estimate and/or provide selection filters for end users looking for a particular property -- which all could rely on knowing the types and number of rooms, etc. In this paper we develop a relatively simple and effective neural joint model that performs both segmentation and dependency parsing together, instead of one after the other as in most state-of-the-art works. We will focus in particular on the real estate ad setting, aiming to convert an ad to a structured description, which we name property tree, comprising the tasks of (1)&nbsp;identifying important entities of a property (e.g., rooms) from classifieds and (2)&nbsp;structuring them into a tree format. In this work, we propose a new joint model that is able to tackle the two tasks simultaneously and construct the property tree by (i)&nbsp;avoiding the error propagation that would arise from the subtasks one after the other in a pipelined fashion, and (ii)&nbsp;exploiting the interactions between the subtasks. For this purpose, we perform an extensive comparative study of the pipeline methods and the new proposed joint model, reporting an improvement of over three percentage points in the overall edge F1 score of the property tree. Also, we propose attention methods, to encourage our model to focus on salient tokens during the construction of the property tree. Thus we experimentally demonstrate the usefulness of attentive neural architectures for the proposed joint model, showcasing a further improvement of two percentage points in edge F1 score for our application. While the results demonstrated are for the particular real estate setting, the model is generic in nature, and thus could be equally applied to other expert system scenarios requiring the general tasks of both (i)&nbsp;detecting entities (segmentation) and (ii)&nbsp;establishing relations among them (dependency parsing).</p>
    </div>
    <div id="bekoulis2018eswa-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>An attentive neural architecture for joint segmentation and parsing and its application to real estate ads</h2>
      <h4>G. Bekoulis, J. Deleu, T. Demeester and C. Develder</h4>
      <br><pubjournal>Expert Syst. Appl.</pubjournal>, Vol. 102, 15 Jul. <pubyear>2018</pubyear>, pp. 100-112.
      <br><br>
      <code>
@article{bekoulis2018eswa,<br>
  author = {Bekoulis, Giannis and Deleu, Johannes and Demeester, Thomas and Develder, Chris},<br>
  title = {An attentive neural architecture for joint segmentation and parsing and its application to real estate ads},<br>
  journal = {Expert Syst. Appl.},<br>
  month = {15 Jul.},<br>
  year = {2018},<br>
  volume = {102},<br>
  pages = {100--112},<br>
  doi = {10.1016/j.eswa.2018.02.031}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="weissenborn2018" class="row add-bottom pubentry pub2018 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>D. Weissenborn, P. Minervini, T. Dettmers, I. Augenstein, J. Welbl, T. Rocktaschel, M. Bosnjak, J. Mitchell, T. Demeester, P. Stenetorp and S. Riedel</pubauth>,
<pubtitle><a href="papers/2018/weissenborn2018.pdf" target="_blank">"Jack the Reader - A machine reading framework"</a></pubtitle>, in <pubconf>Proc. 56th Annual. Meeting Assoc. Comput. Ling. - Demos Track (ACL 2018)</pubconf>, Melbourne, Australia, 15-20 Jul. <pubyear>2018</pubyear>.
  <ul class="file-links">
               <li><a href="papers/2018/weissenborn2018.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         
         
               <li><a class="popup-with-zoom-anim" href="#weissenborn2018-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#weissenborn2018-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/1806.08727" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         <li><a href="https://github.com/uclnlp/jack" target="_blank"><i class="fa fa-github"></i> Code</a></li>
  </ul>
     </div>
     <div id="weissenborn2018-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Jack the Reader - A machine reading framework</h2>
      <h4>D. Weissenborn, P. Minervini, T. Dettmers, I. Augenstein, J. Welbl, T. Rocktaschel, M. Bosnjak, J. Mitchell, T. Demeester, P. Stenetorp and S. Riedel</h4>
      <br>in <pubconf>Proc. 56th Annual. Meeting Assoc. Comput. Ling. - Demos Track (ACL 2018)</pubconf>, Melbourne, Australia, 15-20 Jul. <pubyear>2018</pubyear>.
      <br><br>
      <p>Many Machine Reading and Natural Language Understanding tasks require reading supporting text in order to answer questions. For example, in Question Answering, the supporting text can be newswire or Wikipedia articles; in Natural Language Inference, premises can be seen as the supporting text and hypotheses as questions. Providing a set of useful primitives operating in a single framework of related tasks would allow for expressive modelling, and easier model comparison and replication. To that end, we present Jack the Reader (Jack), a framework for Machine Reading that allows for quick model prototyping by component reuse, evaluation of new models on existing datasets as well as integrating new datasets and applying them on a growing set of implemented baseline models. Jack is currently supporting (but not limited to) three tasks: Question Answering, Natural Language Inference, and Link Prediction. It is developed with the aim of increasing research efficiency and code reuse.</p>
    </div>
    <div id="weissenborn2018-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Jack the Reader - A machine reading framework</h2>
      <h4>D. Weissenborn, P. Minervini, T. Dettmers, I. Augenstein, J. Welbl, T. Rocktaschel, M. Bosnjak, J. Mitchell, T. Demeester, P. Stenetorp and S. Riedel</h4>
      <br>in <pubconf>Proc. 56th Annual. Meeting Assoc. Comput. Ling. - Demos Track (ACL 2018)</pubconf>, Melbourne, Australia, 15-20 Jul. <pubyear>2018</pubyear>.
      <br><br>
      <code>
@inproceedings{weissenborn2018,<br>
  author = {Dirk Weissenborn and Pasquale Minervini and Tim Dettmers and Isabelle Augenstein and Johannes Welbl and Tim Rocktaschel and Matko Bosnjak and Jeff Mitchell and Thomas Demeester and Pontus Stenetorp and Sebastian Riedel},<br>
  title = {Jack the Reader - A machine reading framework},<br>
  booktitle = {Proc. 56th Annual. Meeting Assoc. Comput. Ling. - Demos Track (ACL 2018)},<br>
  month = {15--20 Jul.},<br>
  year = {2018},<br>
  address = {Melbourne, Australia}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="sterckx2018arxiv" class="row add-bottom pubentry pub2018 pubir pubarticle">
     <div class="one column">
     <h3 class="article"><span>pubarticle</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>L. Sterckx, J. Deleu, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2018/sterckx2018arxiv.pdf" target="_blank">"Prior attention for style-aware sequence-to-sequence models"</a></pubtitle>, <pubjournal>Arxiv Preprint</pubjournal>, <pubyear>2018</pubyear>.
  <ul class="file-links">
               <li><a href="papers/2018/sterckx2018arxiv.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         
         
               <li><a class="popup-with-zoom-anim" href="#sterckx2018arxiv-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#sterckx2018arxiv-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/1806.09439" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         
  </ul>
     </div>
     <div id="sterckx2018arxiv-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Prior attention for style-aware sequence-to-sequence models</h2>
      <h4>L. Sterckx, J. Deleu, T. Demeester and C. Develder</h4>
      <br><pubjournal>Arxiv Preprint</pubjournal>, <pubyear>2018</pubyear>.
      <br><br>
      <p>We extend sequence-to-sequence models with the possibility to control the characteristics or style of the generated output, via attention that is generated a priori (before decoding) from a latent code vector. After training an initial attention-based sequence-to-sequence model, we use a variational auto-encoder conditioned on representations of input sequences and a latent code vector space to generate attention matrices. By sampling the code vector from specific regions of this latent space during decoding and imposing prior attention generated from it in the seq2seq model, output can be steered towards having certain attributes. This is demonstrated for the task of sentence simplification, where the latent code vector allows control over output length and lexical simplification, and enables fine-tuning to optimize for different evaluation metrics.</p>
    </div>
    <div id="sterckx2018arxiv-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Prior attention for style-aware sequence-to-sequence models</h2>
      <h4>L. Sterckx, J. Deleu, T. Demeester and C. Develder</h4>
      <br><pubjournal>Arxiv Preprint</pubjournal>, <pubyear>2018</pubyear>.
      <br><br>
      <code>
@article{sterckx2018arxiv,<br>
  author = {Sterckx, Lucas and Deleu, Johannes and Demeester, Thomas and Develder, Chris},<br>
  title = {Prior attention for style-aware sequence-to-sequence models},<br>
  journal = {Arxiv Preprint},<br>
  year = {2018}<br>
}
      </code>
    </div>
</div> <!-- end row -->


<div id="zaporojets2018clpsych" class="row add-bottom pubentry pub2018 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>K. Zaporojets, L. Sterckx, J. Deleu, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2018/zaporojets2018clpsych.pdf" target="_blank">"Predicting psychological health from childhood essays: The UGent-IDLab CLPsych 2018 shared task system"</a></pubtitle>, in <pubconf>Proc. 5th Ann. Workshop on Comput. Ling. Clin. Psychol. (CLPsych 2018) at NAACL-HLT 2018</pubconf>, New Orleans, LA, USA, 5 Jun. <pubyear>2018</pubyear>, pp. 119-125.
  <ul class="file-links">
               <li><a href="papers/2018/zaporojets2018clpsych.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.18653/v1/W18-0613" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         <li><a href="https://www.aclweb.org/anthology/papers/W/W18/W18-0613/" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#zaporojets2018clpsych-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#zaporojets2018clpsych-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="zaporojets2018clpsych-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Predicting psychological health from childhood essays: The UGent-IDLab CLPsych 2018 shared task system</h2>
      <h4>K. Zaporojets, L. Sterckx, J. Deleu, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Proc. 5th Ann. Workshop on Comput. Ling. Clin. Psychol. (CLPsych 2018) at NAACL-HLT 2018</pubconf>, New Orleans, LA, USA, 5 Jun. <pubyear>2018</pubyear>, pp. 119-125.
      <br><br>
      <p>This paper describes the IDLab system submitted to Task A of the CLPsych 2018 shared task. The goal of this task is predicting psychological health of children based on language used in hand-written essays and socio-demographic control variables. Our entry uses word- and character-based features as well as lexicon-based features and features derived from the essays such as the quality of the language. We apply linear models, gradient boosting as well as neural-network based regressors (feed-forward, CNNs and RNNs) to predict scores. We then make ensembles of our best performing models using a weighted average. </p>
    </div>
    <div id="zaporojets2018clpsych-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Predicting psychological health from childhood essays: The UGent-IDLab CLPsych 2018 shared task system</h2>
      <h4>K. Zaporojets, L. Sterckx, J. Deleu, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Proc. 5th Ann. Workshop on Comput. Ling. Clin. Psychol. (CLPsych 2018) at NAACL-HLT 2018</pubconf>, New Orleans, LA, USA, 5 Jun. <pubyear>2018</pubyear>, pp. 119-125.
      <br><br>
      <code>
@inproceedings{zaporojets2018clpsych,<br>
  author = {Zaporojets, Klim and Lucas Sterckx and Johannes Deleu and Thomas Demeester and Chris Develder},<br>
  title = {Predicting psychological health from childhood essays: The UGent-IDLab CLPsych 2018 shared task system},<br>
  booktitle = {Proc. 5th Ann. Workshop on Comput. Ling. Clin. Psychol. (CLPsych 2018) at NAACL-HLT 2018},<br>
  month = {5 Jun.},<br>
  year = {2018},<br>
  pages = {119--125},<br>
  address = {New Orleans, LA, USA},<br>
  url = {https://www.aclweb.org/anthology/papers/W/W18/W18-0613/},<br>
  doi = {10.18653/v1/W18-0613}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="sterckx2017lrev" class="row add-bottom pubentry pub2018 pubir pubarticle">
     <div class="one column">
     <h3 class="article"><span>pubarticle</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>L. Sterckx, T. Demeester, J. Deleu and C. Develder</pubauth>,
<pubtitle><a href="papers/2018/sterckx2017lrev.pdf" target="_blank">"Creation and evaluation of large keyphrase extraction collections with multiple opinions"</a></pubtitle>, <pubjournal>Lang. Resour. Eval.</pubjournal>, Vol. 52, No. 2, Jul. <pubyear>2018</pubyear>, pp. 503-532.
  <ul class="file-links">
               <li><a href="papers/2018/sterckx2017lrev.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.1007/s10579-017-9395-6" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#sterckx2017lrev-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#sterckx2017lrev-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="sterckx2017lrev-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Creation and evaluation of large keyphrase extraction collections with multiple opinions</h2>
      <h4>L. Sterckx, T. Demeester, J. Deleu and C. Develder</h4>
      <br><pubjournal>Lang. Resour. Eval.</pubjournal>, Vol. 52, No. 2, Jul. <pubyear>2018</pubyear>, pp. 503-532.
      <br><br>
      <p>While several Automatic Keyphrase Extraction (AKE) techniques have been developed and analyzed, there is little consensus on the definition of the task and a lack of overview of the effectiveness of different techniques. Proper evaluation of keyphrase extraction requires large test collections with multiple opinions, currently not available for research. In this paper, we (i) present a set of test collections derived from various sources with multiple annotations (which we also refer to as opinions in the remained of the paper) for each document, (ii) systematically evaluate keyphrase extraction using several supervised and unsupervised AKE techniques, (iii) and experimentally analyze the effects of disagreement on AKE evaluation. Our newly created set of test collections spans different types of topical content from general news and magazines, and is annotated with multiple annotations per article by a large user panel. Our user study shows that for a given document there seems to be a large disagreement on the preferred keyphrases, suggesting the need for multiple opinions per document. A first systematic evaluation of ranking and classification of keyphrases using both unsupervised and supervised AKE techniques on the test collections shows a superior effectiveness of supervised models, even for a low annotation effort and with basic positional and frequency features, and highlights the importance of a suitable keyphrase candidate generation approach. We also study the influence of multiple opinions, training data and document length on evaluation of keyphrase extraction. Our new test collection for keyphrase extraction is one of the largest of its kind and will be made available to stimulate future work to improve reliable evaluation of new keyphrase extractors.</p>
    </div>
    <div id="sterckx2017lrev-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Creation and evaluation of large keyphrase extraction collections with multiple opinions</h2>
      <h4>L. Sterckx, T. Demeester, J. Deleu and C. Develder</h4>
      <br><pubjournal>Lang. Resour. Eval.</pubjournal>, Vol. 52, No. 2, Jul. <pubyear>2018</pubyear>, pp. 503-532.
      <br><br>
      <code>
@article{Sterckx2017LREV,<br>
  author = {Sterckx, Lucas and Demeester, Thomas and Deleu, Johannes and Develder, Chris},<br>
  title = {Creation and evaluation of large keyphrase extraction collections with multiple opinions},<br>
  journal = {Lang. Resour. Eval.},<br>
  month = {Jul.},<br>
  year = {2018},<br>
  volume = {52},<br>
  number = {2},<br>
  pages = {503--532},<br>
  doi = {10.1007/s10579-017-9395-6}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="deboom2018" class="row add-bottom pubentry pub2018 pubir pubarticle">
     <div class="one column">
     <h3 class="article"><span>pubarticle</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>C. De Boom, R. Agrawal, S. Hansen, E. Kumar, R. Yon, C.-W. Chen, T. Demeester and B. Dhoedt</pubauth>,
<pubtitle><a href="papers/2018/deboom2018.pdf" target="_blank">"Large-scale user modeling with recurrent neural networks for music discovery on multiple time scales"</a></pubtitle>, <pubjournal>Multimed. Tools Applic.</pubjournal>, Vol. 77, Jun. <pubyear>2018</pubyear>, pp. 15385-15407.
  <ul class="file-links">
               <li><a href="papers/2018/deboom2018.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.1007/s11042-017-5121-z" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#deboom2018-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#deboom2018-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/1708.06520" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         
  </ul>
     </div>
     <div id="deboom2018-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Large-scale user modeling with recurrent neural networks for music discovery on multiple time scales</h2>
      <h4>C. De Boom, R. Agrawal, S. Hansen, E. Kumar, R. Yon, C.-W. Chen, T. Demeester and B. Dhoedt</h4>
      <br><pubjournal>Multimed. Tools Applic.</pubjournal>, Vol. 77, Jun. <pubyear>2018</pubyear>, pp. 15385-15407.
      <br><br>
      <p>The amount of content on online music streaming platforms is immense, and most users only access a tiny fraction of this content. Recommender systems are the application of choice to open up the collection to these users. Collaborative filtering has the disadvantage that it relies on explicit ratings, which are often unavailable, and generally disregards the temporal nature of music consumption. On the other hand, item co-occurrence algorithms, such as the recently introduced word2vec-based recommenders, are typically left without an effective user representation. In this paper, we present a new approach to model users through recurrent neural networks by sequentially processing consumed items, represented by any type of embeddings and other context features. This way we obtain semantically rich user representations, which capture a user’s musical taste over time. Our experimental analysis on large-scale user data shows that our model can be used to predict future songs a user will likely listen to, both in the short and long term.</p>
    </div>
    <div id="deboom2018-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Large-scale user modeling with recurrent neural networks for music discovery on multiple time scales</h2>
      <h4>C. De Boom, R. Agrawal, S. Hansen, E. Kumar, R. Yon, C.-W. Chen, T. Demeester and B. Dhoedt</h4>
      <br><pubjournal>Multimed. Tools Applic.</pubjournal>, Vol. 77, Jun. <pubyear>2018</pubyear>, pp. 15385-15407.
      <br><br>
      <code>
@article{deboom2018,<br>
  author = {De Boom, Cedric and Agrawal, Rohan and Hansen, Samantha and Kumar, Esh and Yon, Romain and Chen, Ching-Wei and Demeester, Thomas and Dhoedt, Bart},<br>
  title = {Large-scale user modeling with recurrent neural networks for music discovery on multiple time scales},<br>
  journal = {Multimed. Tools Applic.},<br>
  month = {Jun.},<br>
  year = {2018},<br>
  volume = {77},<br>
  pages = {15385--15407},<br>
  doi = {10.1007/s11042-017-5121-z}<br>
}
      </code>
    </div>
</div> <!-- end row -->


<div id="vancanneyt2018" class="row add-bottom pubentry pub2018 pubir pubarticle">
     <div class="one column">
     <h3 class="article"><span>pubarticle</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>S. Van Canneyt, P. Leroux, B. Dhoedt and T. Demeester</pubauth>,
<pubtitle><a href="papers/2018/vancanneyt2018.pdf" target="_blank">"Modeling and predicting the popularity of online news based on temporal and content-related features"</a></pubtitle>, <pubjournal>Multimed. Tools Appl.</pubjournal>, Vol. 77, No. 1, Jan. <pubyear>2018</pubyear>, pp. 1409-1436.
  <ul class="file-links">
               <li><a href="papers/2018/vancanneyt2018.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.1007/s11042-017-4348-z" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#vancanneyt2018-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#vancanneyt2018-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="vancanneyt2018-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Modeling and predicting the popularity of online news based on temporal and content-related features</h2>
      <h4>S. Van Canneyt, P. Leroux, B. Dhoedt and T. Demeester</h4>
      <br><pubjournal>Multimed. Tools Appl.</pubjournal>, Vol. 77, No. 1, Jan. <pubyear>2018</pubyear>, pp. 1409-1436.
      <br><br>
      <p>As the market of globally available online news is large and still growing, there is a strong competition between online publishers in order to reach the largest possible audience. Therefore an intelligent online publishing strategy is of the highest importance to publishers. A prerequisite for being able to optimize any online strategy, is to have trustworthy predictions of how popular new online content may become. This paper presents a novel methodology to model and predict the popularity of online news. We first introduce a new strategy and mathematical model to capture view patterns of online news. After a thorough analysis of such view patterns, we show that well-chosen base functions lead to suitable models, and show how the influence of day versus night on the total view patterns can be taken into account to further increase the accuracy, without leading to more complex models. Second, we turn to the prediction of future popularity, given recently published content. By means of a new real-world dataset, we show that the combination of features related to content, meta-data, and the temporal behavior leads to significantly improved predictions, compared to existing approaches which only consider features based on the historical popularity of the considered articles. Whereas traditionally linear regression is used for the application under study, we show that the more expressive gradient tree boosting method proves beneficial for predicting news popularity.</p>
    </div>
    <div id="vancanneyt2018-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Modeling and predicting the popularity of online news based on temporal and content-related features</h2>
      <h4>S. Van Canneyt, P. Leroux, B. Dhoedt and T. Demeester</h4>
      <br><pubjournal>Multimed. Tools Appl.</pubjournal>, Vol. 77, No. 1, Jan. <pubyear>2018</pubyear>, pp. 1409-1436.
      <br><br>
      <code>
@article{vancanneyt2018,<br>
  author = {Van Canneyt, Steven and Leroux, Philippe and Dhoedt, Bart and Demeester, Thomas},<br>
  title = {Modeling and predicting the popularity of online news based on temporal and content-related features},<br>
  journal = {Multimed. Tools Appl.},<br>
  month = {Jan.},<br>
  year = {2018},<br>
  volume = {77},<br>
  number = {1},<br>
  pages = {1409--1436},<br>
  doi = {10.1007/s11042-017-4348-z}<br>
}
      </code>
    </div>
</div> <!-- end row -->


<div id="deygers2018" class="row add-bottom pubentry pub2018 pubir pubarticle">
     <div class="one column">
     <h3 class="article"><span>pubarticle</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>B. Deygers, K. Van Gorp and T. Demeester</pubauth>,
<pubtitle><a href="papers/2018/deygers2018.pdf" target="_blank">"The B2 level and the dream of a common standard"</a></pubtitle>, <pubjournal>Lang. Assess. Quarterly</pubjournal>, Vol. 15, No. 1, Jan. <pubyear>2018</pubyear>, pp. 44-58.
  <ul class="file-links">
               <li><a href="papers/2018/deygers2018.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.1080/15434303.2017.1421955" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#deygers2018-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#deygers2018-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="deygers2018-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>The B2 level and the dream of a common standard</h2>
      <h4>B. Deygers, K. Van Gorp and T. Demeester</h4>
      <br><pubjournal>Lang. Assess. Quarterly</pubjournal>, Vol. 15, No. 1, Jan. <pubyear>2018</pubyear>, pp. 44-58.
      <br><br>
      <p>In Flanders, Belgium, university admission of undergraduate international L2 students requires a certificate of an accredited test of Dutch. The two main university entrance tests used for certification share highly comparable oral components and CEFR-based oral rating criteria. This article discusses to what extent ratings on the oral components of these tests can be compared. The data used are the ratings of the oral performances of the same 82 candidates on both oral test components, which were administered within the same week. The correlation on the overall scores is high, but lower on the oral test component. Further analyses, including linear regression and multifaceted Rasch analysis, indicate that the B2 level was interpreted differently in the two tests. The results show that using the same language proficiency scales as the basis for rating scale criteria may lead to superficial correspondences or a perceived equivalence but does not necessarily lead to greater comparability of shared criteria. The findings of this study are especially useful for contexts in which different tests use similar criteria that are based on the same descriptors, and comparability is only assumed.</p>
    </div>
    <div id="deygers2018-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>The B2 level and the dream of a common standard</h2>
      <h4>B. Deygers, K. Van Gorp and T. Demeester</h4>
      <br><pubjournal>Lang. Assess. Quarterly</pubjournal>, Vol. 15, No. 1, Jan. <pubyear>2018</pubyear>, pp. 44-58.
      <br><br>
      <code>
@article{deygers2018,<br>
  author = {Deygers, Bart and Van Gorp, Koen and Demeester, Thomas},<br>
  title = {The B2 level and the dream of a common standard},<br>
  journal = {Lang. Assess. Quarterly},<br>
  month = {Jan.},<br>
  year = {2018},<br>
  volume = {15},<br>
  number = {1},<br>
  pages = {44--58},<br>
  doi = {10.1080/15434303.2017.1421955}<br>
}
      </code>
    </div>
</div> <!-- end row -->
  
<div id="minervini2017" class="row add-bottom pubentry pub2017 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>P. Minervini, T. Demeester, T. Rockt&auml;schel and S. Riedel</pubauth>,
<pubtitle><a href="papers/2017/minervini2017.pdf" target="_blank">"Adversarial sets for regularising neural link predictors"</a></pubtitle>, in <pubconf>Proc. 33rd Conf. Uncertainty in Artificial Intelligence (UAI 2017)</pubconf>, Sydney, Australia, Aug. 11-15 <pubyear>2017</pubyear>.
  <ul class="file-links">
               <li><a href="papers/2017/minervini2017.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         
         
               <li><a class="popup-with-zoom-anim" href="#minervini2017-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#minervini2017-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/1707.07596" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         <li><a href="https://github.com/uclnlp/inferbeddings" target="_blank"><i class="fa fa-github"></i> Code</a></li>
  </ul>
     </div>
     <div id="minervini2017-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Adversarial sets for regularising neural link predictors</h2>
      <h4>P. Minervini, T. Demeester, T. Rockt&auml;schel and S. Riedel</h4>
      <br>in <pubconf>Proc. 33rd Conf. Uncertainty in Artificial Intelligence (UAI 2017)</pubconf>, Sydney, Australia, Aug. 11-15 <pubyear>2017</pubyear>.
      <br><br>
      <p>In adversarial training, a set of models learn together by pursuing competing goals, usually defined on single data instances. However, in relational learning and other non-i.i.d domains, goals can also be defined over sets of instances. For example, a link predictor for the is-a relation needs to be consistent with the transitivity property: if is-a(x_1, x_2) and is-a(x_2, x_3) hold, is-a(x_1, x_3) needs to hold as well. Here we use such assumptions for deriving an inconsistency loss, measuring the degree to which the model violates the assumptions on an adversarially-generated set of examples. The training objective is defined as a minimax problem, where an adversary finds the most offending adversarial examples by maximising the inconsistency loss, and the model is trained by jointly minimising a supervised loss and the inconsistency loss on the adversarial examples. This yields the first method that can use function-free Horn clauses (as in Datalog) to regularise any neural link predictor, with complexity independent of the domain size. We show that for several link prediction models, the optimisation problem faced by the adversary has efficient closed-form solutions. Experiments on link prediction benchmarks indicate that given suitable prior knowledge, our method can significantly improve neural link predictors on all relevant metrics.</p>
    </div>
    <div id="minervini2017-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Adversarial sets for regularising neural link predictors</h2>
      <h4>P. Minervini, T. Demeester, T. Rockt&auml;schel and S. Riedel</h4>
      <br>in <pubconf>Proc. 33rd Conf. Uncertainty in Artificial Intelligence (UAI 2017)</pubconf>, Sydney, Australia, Aug. 11-15 <pubyear>2017</pubyear>.
      <br><br>
      <code>
@inproceedings{Minervini2017,<br>
  author = {Minervini, Pasquale and Demeester, Thomas and Rockt&auml;schel, Tim and Riedel, Sebastian},<br>
  title = {Adversarial sets for regularising neural link predictors},<br>
  booktitle = {Proc. 33rd Conf. Uncertainty in Artificial Intelligence (UAI 2017)},<br>
  month = {Aug. 11--15},<br>
  year = {2017},<br>
  address = {Sydney, Australia}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="sterckx2017emnlp" class="row add-bottom pubentry pub2017 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>L. Sterckx, J. Naradowsky, B. Byrne, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2017/sterckx2017emnlp.pdf" target="_blank">"Break it down for me: A study in automated lyric annotation"</a></pubtitle>, in <pubconf>Proc. Conf. Empirical Methods in Natural Lang. Processing (EMNLP 2017)</pubconf>, Copenhagen, Denmark, 7-11 Sep. <pubyear>2017</pubyear>, pp. 2064-70.
  <ul class="file-links">
               <li><a href="papers/2017/sterckx2017emnlp.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.18653/v1/D17-1220" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         <li><a href="https://www.aclweb.org/anthology/papers/D/D17/D17-1220/" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#sterckx2017emnlp-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#sterckx2017emnlp-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="sterckx2017emnlp-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Break it down for me: A study in automated lyric annotation</h2>
      <h4>L. Sterckx, J. Naradowsky, B. Byrne, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Proc. Conf. Empirical Methods in Natural Lang. Processing (EMNLP 2017)</pubconf>, Copenhagen, Denmark, 7-11 Sep. <pubyear>2017</pubyear>, pp. 2064-70.
      <br><br>
      <p>Comprehending lyrics, as found in songs and poems, can pose a challenge to human and machine readers alike. This motivates the need for systems that can understand the ambiguity and jargon found in such creative texts, and provide commentary to aid readers in reaching the correct interpretation.<br>We introduce the task of automated lyric annotation (ALA). Like text simplification, a goal of ALA is to rephrase the original text in a more easily understandable manner. However, in ALA the system must often include additional information to clarify niche terminology and abstract concepts. To stimulate research on this task, we release a large collection of crowdsourced annotations for song lyrics. We analyze the performance of translation and retrieval models on this task, measuring performance with both automated and human evaluation. We find that each model captures a unique type of information important to the task.</p>
    </div>
    <div id="sterckx2017emnlp-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Break it down for me: A study in automated lyric annotation</h2>
      <h4>L. Sterckx, J. Naradowsky, B. Byrne, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Proc. Conf. Empirical Methods in Natural Lang. Processing (EMNLP 2017)</pubconf>, Copenhagen, Denmark, 7-11 Sep. <pubyear>2017</pubyear>, pp. 2064-70.
      <br><br>
      <code>
@inproceedings{Sterckx2017EMNLP,<br>
  author = {Sterckx, Lucas and and Jason Naradowsky and Bill Byrne and Thomas Demeester and Develder, Chris},<br>
  title = {Break it down for me: A study in automated lyric annotation},<br>
  booktitle = {Proc. Conf. Empirical Methods in Natural Lang. Processing (EMNLP 2017)},<br>
  month = {7--11 Sep.},<br>
  year = {2017},<br>
  pages = {2064--70},<br>
  address = {Copenhagen, Denmark},<br>
  url = {https://www.aclweb.org/anthology/papers/D/D17/D17-1220/},<br>
  doi = {10.18653/v1/D17-1220}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="bekoulis2017eacl" class="row add-bottom pubentry pub2017 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>G. Bekoulis, J. Deleu, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2017/bekoulis2017eacl.pdf" target="_blank">"Reconstructing the house from the ad: Structured prediction on real estate classifieds"</a></pubtitle>, in <pubconf>Proc. 15th Conf. Eur. Chapter Assoc. Comput. Ling. (EACL 2017), Vol. 2</pubconf>, Valencia, Spain, 3-7 Apr. <pubyear>2017</pubyear>, pp. 274-279.
  <ul class="file-links">
               <li><a href="papers/2017/bekoulis2017eacl.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         
         <li><a href="https://www.aclweb.org/anthology/papers/E/E17/E17-2044/" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#bekoulis2017eacl-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#bekoulis2017eacl-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
                 
  </ul>
     </div>
     <div id="bekoulis2017eacl-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Reconstructing the house from the ad: Structured prediction on real estate classifieds</h2>
      <h4>G. Bekoulis, J. Deleu, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Proc. 15th Conf. Eur. Chapter Assoc. Comput. Ling. (EACL 2017), Vol. 2</pubconf>, Valencia, Spain, 3-7 Apr. <pubyear>2017</pubyear>, pp. 274-279.
      <br><br>
      <p>In this paper, we address the (to the best of our knowledge) new problem of extracting a structured description of real estate properties from their natural language descriptions in classifieds. We survey and present several models to (a) identify important entities of a property (e.g., rooms) from classifieds and (b) structure them into a tree format, with the entities as nodes and edges representing a part-of relation. Experiments show that a graph-based system deriving the tree from an initially fully connected entity graph, outperforms a transition-based system starting from only the entity nodes, since it better reconstructs the tree.</p>
    </div>
    <div id="bekoulis2017eacl-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Reconstructing the house from the ad: Structured prediction on real estate classifieds</h2>
      <h4>G. Bekoulis, J. Deleu, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Proc. 15th Conf. Eur. Chapter Assoc. Comput. Ling. (EACL 2017), Vol. 2</pubconf>, Valencia, Spain, 3-7 Apr. <pubyear>2017</pubyear>, pp. 274-279.
      <br><br>
      <code>
@inproceedings{Bekoulis2017EACL,<br>
  author = {Bekoulis, Giannis and Deleu, Johannes and Demeester, Thomas and Develder, Chris},<br>
  title = {Reconstructing the house from the ad: Structured prediction on real estate classifieds},<br>
  booktitle = {Proc. 15th Conf. Eur. Chapter Assoc. Comput. Ling. (EACL 2017), Vol. 2},<br>
  month = {3--7 Apr.},<br>
  year = {2017},<br>
  pages = {274--279},<br>
  address = {Valencia, Spain},<br>
  url = {https://www.aclweb.org/anthology/papers/E/E17/E17-2044/}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="sterckx2016emnlp" class="row add-bottom pubentry pub2016 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>L. Sterckx, C. Caragea, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2016/sterckx2016emnlp.pdf" target="_blank">"Supervised keyphrase extraction as positive unlabeled learning"</a></pubtitle>, in <pubconf>Proc. Conf. Empirical Methods in Natural Lang. Proc. (EMNLP 2016)</pubconf>, Austin, TX, USA, 1-5 Nov. <pubyear>2016</pubyear>, pp. 1924-29.
  <ul class="file-links">
               <li><a href="papers/2016/sterckx2016emnlp.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.18653/v1/D16-1198" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         <li><a href="https://www.aclweb.org/anthology/papers/D/D16/D16-1198/" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#sterckx2016emnlp-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#sterckx2016emnlp-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="sterckx2016emnlp-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Supervised keyphrase extraction as positive unlabeled learning</h2>
      <h4>L. Sterckx, C. Caragea, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Proc. Conf. Empirical Methods in Natural Lang. Proc. (EMNLP 2016)</pubconf>, Austin, TX, USA, 1-5 Nov. <pubyear>2016</pubyear>, pp. 1924-29.
      <br><br>
      <p>The problem of noisy and unbalanced train- ing data for supervised keyphrase extraction results from the subjectivity of keyphrase assignment, which we quantify by crowdsourcing keyphrases for news and fashion magazine articles with many annotators per document. We show that annotators exhibit substantial disagreement, meaning that single annotator data could lead to very different training sets for supervised keyphrase extractors. Thus, annotations from single authors or readers lead to noisy training data and poor extraction performance of the resulting supervised extractor. We provide a simple but effective solution to still work with such data by reweighting the importance of unlabeled candidate phrases in a two stage Positive Unlabeled Learning setting. We show that performance of trained keyphrase extractors approximates a classifier trained on articles labeled by multiple annotators, leading to higher average F1scores and better rankings of keyphrases. We apply this strategy to a variety of test collections from different backgrounds and show improvements over strong baseline models.</p>
    </div>
    <div id="sterckx2016emnlp-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Supervised keyphrase extraction as positive unlabeled learning</h2>
      <h4>L. Sterckx, C. Caragea, T. Demeester and C. Develder</h4>
      <br>in <pubconf>Proc. Conf. Empirical Methods in Natural Lang. Proc. (EMNLP 2016)</pubconf>, Austin, TX, USA, 1-5 Nov. <pubyear>2016</pubyear>, pp. 1924-29.
      <br><br>
      <code>
@inproceedings{sterckx2016emnlp,<br>
  author = {Sterckx, Lucas and Caragea, Cornelia and Demeester, Thomas and Develder, Chris},<br>
  title = {Supervised keyphrase extraction as positive unlabeled learning},<br>
  booktitle = {Proc. Conf. Empirical Methods in Natural Lang. Proc. (EMNLP 2016)},<br>
  month = {1--5 Nov.},<br>
  year = {2016},<br>
  pages = {1924--29},<br>
  address = {Austin, TX, USA},<br>
  url = {https://www.aclweb.org/anthology/papers/D/D16/D16-1198/},<br>
  doi = {10.18653/v1/D16-1198}<br>
}
      </code>
    </div>
</div> <!-- end row -->


<div id="demeester2016emnlp" class="row add-bottom pubentry pub2016 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>T. Demeester, T. Rockt&auml;schel and S. Riedel</pubauth>,
<pubtitle><a href="papers/2016/demeester2016emnlp.pdf" target="_blank">"Lifted rule injection for relation embeddings"</a></pubtitle>, in <pubconf>Proc. Conf. Empirical Methods in Natural Lang. Proc. (EMNLP 2016)</pubconf>, Austin, TX, USA, 1-5 Nov. <pubyear>2016</pubyear>, pp. 1389-1399.
  <ul class="file-links">
               <li><a href="papers/2016/demeester2016emnlp.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.18653/v1/D16-1146" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         <li><a href="https://www.aclweb.org/anthology/D16-1146" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#demeester2016emnlp-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#demeester2016emnlp-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/1606.08359" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         
  </ul>
     </div>
     <div id="demeester2016emnlp-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Lifted rule injection for relation embeddings</h2>
      <h4>T. Demeester, T. Rockt&auml;schel and S. Riedel</h4>
      <br>in <pubconf>Proc. Conf. Empirical Methods in Natural Lang. Proc. (EMNLP 2016)</pubconf>, Austin, TX, USA, 1-5 Nov. <pubyear>2016</pubyear>, pp. 1389-1399.
      <br><br>
      <p>Methods based on representation learning currently hold the state-of-the-art in many natural language processing and knowledge base inference tasks. Yet, a major challenge is how to efficiently incorporate commonsense knowledge into such models. A recent approach regularizes relation and entity representations by propositionalization of first-order logic rules. However, propositionalization does not scale beyond domains with only few entities and rules. In this paper we present a highly efficient method for incorporating implication rules into distributed representations for automated knowledge base construction. We map entity-tuple embeddings into an approximately Boolean space and encourage a partial ordering over relation embeddings based on implication rules mined from WordNet. Surprisingly, we find that the strong restriction of the entity-tuple embedding space does not hurt the expressiveness of the model and even acts as a regularizer that improves generalization. By incorporating few commonsense rules, we achieve an increase of 2 percentage points mean average precision over a matrix factorization baseline, while observing a negligible increase in runtime.</p>
    </div>
    <div id="demeester2016emnlp-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Lifted rule injection for relation embeddings</h2>
      <h4>T. Demeester, T. Rockt&auml;schel and S. Riedel</h4>
      <br>in <pubconf>Proc. Conf. Empirical Methods in Natural Lang. Proc. (EMNLP 2016)</pubconf>, Austin, TX, USA, 1-5 Nov. <pubyear>2016</pubyear>, pp. 1389-1399.
      <br><br>
      <code>
@inproceedings{demeester2016emnlp,<br>
  author = {Demeester, Thomas and Rockt&auml;schel, Tim and Riedel, Sebastian},<br>
  title = {Lifted rule injection for relation embeddings},<br>
  booktitle = {Proc. Conf. Empirical Methods in Natural Lang. Proc. (EMNLP 2016)},<br>
  month = {1--5 Nov.},<br>
  year = {2016},<br>
  pages = {1389--1399},<br>
  address = {Austin, TX, USA},<br>
  url = {https://www.aclweb.org/anthology/D16-1146},<br>
  doi = {10.18653/v1/D16-1146}<br>
}
      </code>
    </div>
</div> <!-- end row -->


<div id="sterckx2016kbs" class="row add-bottom pubentry pub2016 pubir pubarticle">
     <div class="one column">
     <h3 class="article"><span>pubarticle</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>L. Sterckx, T. Demeester, J. Deleu and C. Develder</pubauth>,
<pubtitle><a href="papers/2016/sterckx2016kbs.pdf" target="_blank">"Knowledge base population using semantic label propagation"</a></pubtitle>, <pubjournal>Knowledge-Based Syst.</pubjournal>, Vol. 108, Sep. <pubyear>2016</pubyear>, pp. 79-91.
  <ul class="file-links">
               <li><a href="papers/2016/sterckx2016kbs.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.1016/j.knosys.2016.05.015" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#sterckx2016kbs-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#sterckx2016kbs-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
  </ul>
     </div>
     <div id="sterckx2016kbs-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Knowledge base population using semantic label propagation</h2>
      <h4>L. Sterckx, T. Demeester, J. Deleu and C. Develder</h4>
      <br><pubjournal>Knowledge-Based Syst.</pubjournal>, Vol. 108, Sep. <pubyear>2016</pubyear>, pp. 79-91.
      <br><br>
      <p>Training relation extractors for the purpose of automated knowledge base population requires the availability of sufficient training data. The amount of manual labeling can be significantly reduced by applying distant supervision, which generates training data by aligning large text corpora with existing knowledge bases. This typically results in a highly noisy training set, where many training sentences do not express the intended relation. In this paper, we propose to combine distant supervision with minimal human supervision by annotating features (in particular shortest dependency paths) rather than complete relation instances. Such feature labeling eliminates noise from the initial training set, resulting in a significant increase of precision at the expense of recall. We further improve on this approach by introducing the Semantic Label Propagation (SLP) method, which uses the similarity between low-dimensional representations of candidate training instances to again extend the (filtered) training set in order to increase recall while maintaining high precision. Our strategy is evaluated on an established test collection designed for knowledge base population (KBP) from the TAC KBP English slot filling task. The experimental results show that SLP leads to substantial performance gains when compared to existing approaches while requiring an almost negligible human annotation effort.</p>
    </div>
    <div id="sterckx2016kbs-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Knowledge base population using semantic label propagation</h2>
      <h4>L. Sterckx, T. Demeester, J. Deleu and C. Develder</h4>
      <br><pubjournal>Knowledge-Based Syst.</pubjournal>, Vol. 108, Sep. <pubyear>2016</pubyear>, pp. 79-91.
      <br><br>
      <code>
@article{Sterckx2016KBS,<br>
  author = {Lucas Sterckx and Thomas Demeester and Johannes Deleu and Chris Develder},<br>
  title = {Knowledge base population using semantic label propagation},<br>
  journal = {Knowledge-Based Syst.},<br>
  month = {Sep.},<br>
  year = {2016},<br>
  volume = {108},<br>
  pages = {79--91},<br>
  doi = {10.1016/j.knosys.2016.05.015}<br>
}
      </code>
    </div>
</div> <!-- end row -->


<div id="deboom2016prl" class="row add-bottom pubentry pub2016 pubir pubarticle">
     <div class="one column">
     <h3 class="article"><span>pubarticle</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>C. De Boom, S. Van Canneyt, T. Demeester and B. Dhoedt</pubauth>,
<pubtitle><a href="papers/2016/deboom2016prl.pdf" target="_blank">"Representation learning for very short texts using weighted word embedding aggregation"</a></pubtitle>, <pubjournal>Pattern Recogn. Lett.</pubjournal>, Vol. 80, Sep. <pubyear>2016</pubyear>, pp. 150-156.
  <ul class="file-links">
               <li><a href="papers/2016/deboom2016prl.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.1016/j.patrec.2016.06.012" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#deboom2016prl-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#deboom2016prl-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/1607.00570" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         
  </ul>
     </div>
     <div id="deboom2016prl-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Representation learning for very short texts using weighted word embedding aggregation</h2>
      <h4>C. De Boom, S. Van Canneyt, T. Demeester and B. Dhoedt</h4>
      <br><pubjournal>Pattern Recogn. Lett.</pubjournal>, Vol. 80, Sep. <pubyear>2016</pubyear>, pp. 150-156.
      <br><br>
      <p>We create text representations by weighing word embeddings using idf information.A novel median-based loss is designed to mitigate the negative effect of outliers.A dataset of semantically related textual pairs from Wikipedia and Twitter is made.Our method outperforms all word embedding baselines in a semantic similarity task.Our method is out-of-the-box and thus requires no retraining in different contexts. Short text messages such as tweets are very noisy and sparse in their use of vocabulary. Traditional textual representations, such as tf-idf, have difficulty grasping the semantic meaning of such texts, which is important in applications such as event detection, opinion mining, news recommendation, etc. We constructed a method based on semantic word embeddings and frequency information to arrive at low-dimensional representations for short texts designed to capture semantic similarity. For this purpose we designed a weight-based model and a learning procedure based on a novel median-based loss function. This paper discusses the details of our model and the optimization methods, together with the experimental results on both Wikipedia and Twitter data. We find that our method outperforms the baseline approaches in the experiments, and that it generalizes well on different word embeddings without retraining. Our method is therefore capable of retaining most of the semantic information in the text, and is applicable out-of-the-box.</p>
    </div>
    <div id="deboom2016prl-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Representation learning for very short texts using weighted word embedding aggregation</h2>
      <h4>C. De Boom, S. Van Canneyt, T. Demeester and B. Dhoedt</h4>
      <br><pubjournal>Pattern Recogn. Lett.</pubjournal>, Vol. 80, Sep. <pubyear>2016</pubyear>, pp. 150-156.
      <br><br>
      <code>
@article{deboom2016prl,<br>
  author = {De Boom, Cedric and Van Canneyt, Steven and Demeester, Thomas and Dhoedt, Bart},<br>
  title = {Representation learning for very short texts using weighted word embedding aggregation},<br>
  journal = {Pattern Recogn. Lett.},<br>
  month = {Sep.},<br>
  year = {2016},<br>
  volume = {80},<br>
  pages = {150--156},<br>
  doi = {10.1016/j.patrec.2016.06.012}<br>
}
      </code>
    </div>
</div> <!-- end row -->


<div id="deboom2016deml" class="row add-bottom pubentry pub2016 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>C. De Boom, S. Leroux, S. Bohez, P. Simoens, T. Demeester and B. Dhoedt</pubauth>,
<pubtitle><a href="papers/2016/deboom2016deml.pdf" target="_blank">"Efficiency evaluation of character-level RNN training schedules"</a></pubtitle>, in <pubconf>Proc. ICML 2016 Workshop Data Efficient Machine Learn. (DEML 2016)</pubconf>, 24 Jun. <pubyear>2016</pubyear>.
  <ul class="file-links">
               <li><a href="papers/2016/deboom2016deml.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         
         
               <li><a class="popup-with-zoom-anim" href="#deboom2016deml-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#deboom2016deml-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/1605.02486" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         
  </ul>
     </div>
     <div id="deboom2016deml-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Efficiency evaluation of character-level RNN training schedules</h2>
      <h4>C. De Boom, S. Leroux, S. Bohez, P. Simoens, T. Demeester and B. Dhoedt</h4>
      <br>in <pubconf>Proc. ICML 2016 Workshop Data Efficient Machine Learn. (DEML 2016)</pubconf>, 24 Jun. <pubyear>2016</pubyear>.
      <br><br>
      <p>We present four training and prediction schedules from the same character-level recurrent neural network. The efficiency of these schedules is tested in terms of model effectiveness as a function of training time and amount of training data seen. We show that the choice of training and prediction schedule potentially has a considerable impact on the prediction effectiveness for a given training budget.</p>
    </div>
    <div id="deboom2016deml-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Efficiency evaluation of character-level RNN training schedules</h2>
      <h4>C. De Boom, S. Leroux, S. Bohez, P. Simoens, T. Demeester and B. Dhoedt</h4>
      <br>in <pubconf>Proc. ICML 2016 Workshop Data Efficient Machine Learn. (DEML 2016)</pubconf>, 24 Jun. <pubyear>2016</pubyear>.
      <br><br>
      <code>
@inproceedings{deboom2016deml,<br>
  author = {De Boom, Cedric and Leroux, Sam and Bohez, Steven and Simoens, Pieter and Demeester, Thomas and Dhoedt, Bart},<br>
  title = {Efficiency evaluation of character-level RNN training schedules},<br>
  booktitle = {Proc. ICML 2016 Workshop Data Efficient Machine Learn. (DEML 2016)},<br>
  month = {24 Jun.},<br>
  year = {2016}<br>
}
      </code>
    </div>
</div> <!-- end row -->


<div id="demeester2016akbc" class="row add-bottom pubentry pub2016 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>T. Demeester, T. Rockt&auml;schel and S. Riedel</pubauth>,
<pubtitle><a href="papers/2016/demeester2016akbc.pdf" target="_blank">"Regularizing relation representations by first-order implications"</a></pubtitle>, in <pubconf>Proc. 5th Workshop Autom. Knowl. Base Constr. (AKBC 2016)</pubconf>, San Diego, CA, USA, 17 Jun. <pubyear>2016</pubyear>, pp. 75-80.
  <ul class="file-links">
               <li><a href="papers/2016/demeester2016akbc.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.18653/v1/W16-1314" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         <li><a href="https://www.aclweb.org/anthology/W16-1314" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#demeester2016akbc-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#demeester2016akbc-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="demeester2016akbc-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Regularizing relation representations by first-order implications</h2>
      <h4>T. Demeester, T. Rockt&auml;schel and S. Riedel</h4>
      <br>in <pubconf>Proc. 5th Workshop Autom. Knowl. Base Constr. (AKBC 2016)</pubconf>, San Diego, CA, USA, 17 Jun. <pubyear>2016</pubyear>, pp. 75-80.
      <br><br>
      <p>Methods for automated knowledge base construction often rely on trained fixed-length vector representations of relations and entities to predict facts. Recent work showed that such representations can be regularized to inject first-order logic formulae. This enables to incorporate domain-knowledge for improved prediction of facts, especially for uncommon relations. However, current approaches rely on propositionalization of formulae and thus do not scale to large sets of formulae or knowledge bases with many facts. Here we propose a method that imposes first-order constraints directly on relation representations, avoiding costly grounding of formulae. We show that our approach works well for implications between pairs of relations on artificial datasets.</p>
    </div>
    <div id="demeester2016akbc-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Regularizing relation representations by first-order implications</h2>
      <h4>T. Demeester, T. Rockt&auml;schel and S. Riedel</h4>
      <br>in <pubconf>Proc. 5th Workshop Autom. Knowl. Base Constr. (AKBC 2016)</pubconf>, San Diego, CA, USA, 17 Jun. <pubyear>2016</pubyear>, pp. 75-80.
      <br><br>
      <code>
@inproceedings{demeester2016akbc,<br>
  author = {Demeester, Thomas and Rockt&auml;schel, Tim and Riedel, Sebastian},<br>
  title = {Regularizing relation representations by first-order implications},<br>
  booktitle = {Proc. 5th Workshop Autom. Knowl. Base Constr. (AKBC 2016)},<br>
  month = {17 Jun.},<br>
  year = {2016},<br>
  pages = {75--80},<br>
  address = {San Diego, CA, USA},<br>
  url = {https://www.aclweb.org/anthology/W16-1314},<br>
  doi = {10.18653/v1/W16-1314}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="vandersmissen2016" class="row add-bottom pubentry pub2016 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>B. Vandersmissen, L. Sterckx, T. Demeester, A. Jalalvand, W. De Neve and R. Van de Walle</pubauth>,
<pubtitle><a href="papers/2016/vandersmissen2016.pdf" target="_blank">"An automated end-to-end pipeline for fine-grained video annotation using deep neural networks"</a></pubtitle>, in <pubconf>Proc. ACM Int. Conf. Multimedia Retr. (ICMR 2016)</pubconf>, New York, NY, USA, 6-9 Jun. <pubyear>2016</pubyear>.
  <ul class="file-links">
               <li><a href="papers/2016/vandersmissen2016.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.1145/2911996.2912028" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#vandersmissen2016-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#vandersmissen2016-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="vandersmissen2016-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>An automated end-to-end pipeline for fine-grained video annotation using deep neural networks</h2>
      <h4>B. Vandersmissen, L. Sterckx, T. Demeester, A. Jalalvand, W. De Neve and R. Van de Walle</h4>
      <br>in <pubconf>Proc. ACM Int. Conf. Multimedia Retr. (ICMR 2016)</pubconf>, New York, NY, USA, 6-9 Jun. <pubyear>2016</pubyear>.
      <br><br>
      <p>The searchability of video content is often limited to the descriptions authors and/or annotators care to provide. The level of description can range from absolutely nothing to fine-grained annotations at the level of frames. Based on these annotations, certain parts of the video content are more searchable than others.<br>Within the context of the STEAMER project, we developed an innovative end-to-end system that attempts to tackle the problem of unsupervised retrieval of news video content, leveraging multiple information streams and deep neural networks. In particular, we extracted keyphrases and named entities from transcripts, subsequently refining these keyphrases and named entities based on their visual appearance in the news video content. Moreover, to allow for fine-grained frame-level annotations, we temporally located high-confidence keyphrases in the news video content. To that end, we had to tackle challenges such as the automatic construction of training sets and the automatic assessment of keyphrase imageability.<br>In this paper, we discuss the main components of our end-to-end system, capable of transforming textual and visual information into fine-grained video annotations.</p>
    </div>
    <div id="vandersmissen2016-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>An automated end-to-end pipeline for fine-grained video annotation using deep neural networks</h2>
      <h4>B. Vandersmissen, L. Sterckx, T. Demeester, A. Jalalvand, W. De Neve and R. Van de Walle</h4>
      <br>in <pubconf>Proc. ACM Int. Conf. Multimedia Retr. (ICMR 2016)</pubconf>, New York, NY, USA, 6-9 Jun. <pubyear>2016</pubyear>.
      <br><br>
      <code>
@inproceedings{vandersmissen2016,<br>
  author = {Vandersmissen, Baptist and Sterckx, Lucas and Demeester, Thomas and Jalalvand, Azarakhsh and De Neve, Wesley and Van de Walle, Rik},<br>
  title = {An automated end-to-end pipeline for fine-grained video annotation using deep neural networks},<br>
  booktitle = {Proc. ACM Int. Conf. Multimedia Retr. (ICMR 2016)},<br>
  month = {6--9 Jun.},<br>
  year = {2016},<br>
  address = {New York, NY, USA},<br>
  doi = {10.1145/2911996.2912028}<br>
}
      </code>
    </div>
</div> <!-- end row -->


<div id="demeester2015ir" class="row add-bottom pubentry pub2016 pubir pubarticle">
     <div class="one column">
     <h3 class="article"><span>pubarticle</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>T. Demeester, R. Aly, D. Hiemstra, D. Nguyen and C. Develder</pubauth>,
<pubtitle><a href="papers/2016/demeester2015ir.pdf" target="_blank">"Predicting relevance based on assessor disagreement: Analysis and practical applications for search evaluation"</a></pubtitle>, <pubjournal>Inf. Retr.</pubjournal>, Vol. 19, No. 3, Jun. <pubyear>2016</pubyear>, pp. 284-312.
  <ul class="file-links">
               <li><a href="papers/2016/demeester2015ir.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.1007/s10791-015-9275-x" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#demeester2015ir-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#demeester2015ir-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/1511.07237" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         
  </ul>
     </div>
     <div id="demeester2015ir-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Predicting relevance based on assessor disagreement: Analysis and practical applications for search evaluation</h2>
      <h4>T. Demeester, R. Aly, D. Hiemstra, D. Nguyen and C. Develder</h4>
      <br><pubjournal>Inf. Retr.</pubjournal>, Vol. 19, No. 3, Jun. <pubyear>2016</pubyear>, pp. 284-312.
      <br><br>
      <p>Evaluation of search engines relies on assessments of search results for selected test queries, from which we would ideally like to draw conclusions in terms of relevance of the results for general (e.g., future, unknown) users. In practice however, most evaluation scenarios only allow us to conclusively determine the relevance towards the particular assessor that provided the judgments. A factor that cannot be ignored when extending conclusions made from assessors towards users, is the possible disagreement on relevance, assuming that a single gold truth label does not exist. This paper presents and analyzes the predicted relevance model (PRM), which allows predicting a particular result’s relevance for a random user, based on an observed assessment and knowledge on the average disagreement between assessors. With the PRM, existing evaluation metrics designed to measure binary assessor relevance, can be transformed into more robust and effectively graded measures that evaluate relevance towards a random user. It also leads to a principled way of quantifying multiple graded or categorical relevance levels for use as gains in established graded relevance measures, such as normalized discounted cumulative gain, which nowadays often use heuristic and data-independent gain values. Given a set of test topics with graded relevance judgments, the PRM allows evaluating systems on different scenarios, such as their capability of retrieving top results, or how well they are able to filter out non-relevant ones. Its use in actual evaluation scenarios is illustrated on several information retrieval test collections.</p>
    </div>
    <div id="demeester2015ir-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Predicting relevance based on assessor disagreement: Analysis and practical applications for search evaluation</h2>
      <h4>T. Demeester, R. Aly, D. Hiemstra, D. Nguyen and C. Develder</h4>
      <br><pubjournal>Inf. Retr.</pubjournal>, Vol. 19, No. 3, Jun. <pubyear>2016</pubyear>, pp. 284-312.
      <br><br>
      <code>
@article{Demeester2015IR,<br>
  author = {Demeester, Thomas and Aly, Robin and Hiemstra, Djoerd and Nguyen, Dong and Develder, Chris},<br>
  title = {Predicting relevance based on assessor disagreement: Analysis and practical applications for search evaluation},<br>
  journal = {Inf. Retr.},<br>
  month = {Jun.},<br>
  year = {2016},<br>
  volume = {19},<br>
  number = {3},<br>
  pages = {284--312},<br>
  doi = {10.1007/s10791-015-9275-x}<br>
}
      </code>
    </div>
</div> <!-- end row -->


<div id="sterckx2015tac" class="row add-bottom pubentry pub2015 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>L. Sterckx, T. Demeester, J. Deleu and C. Develder</pubauth>,
<pubtitle><a href="papers/2015/sterckx2015tac.pdf" target="_blank">"Ghent University-IBCN participation in the TAC KBP 2015 cold start slot filling task"</a></pubtitle>, in <pubconf>Proc. 8th Text Analysis Conf. (TAC 2015)</pubconf>, Gaithersburg, MD, USA, 16-17 Nov. <pubyear>2015</pubyear>.
  <ul class="file-links">
               <li><a href="papers/2015/sterckx2015tac.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         
         <li><a href="https://tac.nist.gov/publications/2015/participant.papers/TAC2015.UGENT_IBCN.proceedings.pdf"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#sterckx2015tac-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#sterckx2015tac-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
  </ul>
     </div>
     <div id="sterckx2015tac-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Ghent University-IBCN participation in the TAC KBP 2015 cold start slot filling task</h2>
      <h4>L. Sterckx, T. Demeester, J. Deleu and C. Develder</h4>
      <br>in <pubconf>Proc. 8th Text Analysis Conf. (TAC 2015)</pubconf>, Gaithersburg, MD, USA, 16-17 Nov. <pubyear>2015</pubyear>.
      <br><br>
      <p>This paper presents the system of the UGENT IBCN team for the TAC KBP 2015 cold start (slot filling variant) task. This was the team’s second participation. The slot filling system uses distant supervision to generate training data combined with  feature labeling and semi-supervision, and two different types of classifiers. We show that the noise reduction step significantly improves precision, and propose an application of word embeddings for slot filling.</p>
    </div>
    <div id="sterckx2015tac-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Ghent University-IBCN participation in the TAC KBP 2015 cold start slot filling task</h2>
      <h4>L. Sterckx, T. Demeester, J. Deleu and C. Develder</h4>
      <br>in <pubconf>Proc. 8th Text Analysis Conf. (TAC 2015)</pubconf>, Gaithersburg, MD, USA, 16-17 Nov. <pubyear>2015</pubyear>.
      <br><br>
      <code>
@inproceedings{sterckx2015tac,<br>
  author = {Lucas Sterckx and Thomas Demeester and Johannes Deleu and Chris Develder},<br>
  title = {Ghent University-IBCN participation in the TAC KBP 2015 cold start slot filling task},<br>
  booktitle = {Proc. 8th Text Analysis Conf. (TAC 2015)},<br>
  month = {16--17 Nov.},<br>
  year = {2015},<br>
  address = {Gaithersburg, MD, USA},<br>
  url = {https://tac.nist.gov/publications/2015/participant.papers/TAC2015.UGENT_IBCN.proceedings.pdf}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="deboom2015icdmw" class="row add-bottom pubentry pub2015 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>C. De Boom, S. Van Canneyt, S. Bohez, T. Demeester and B. Dhoedt</pubauth>,
<pubtitle><a href="papers/2015/deboom2015icdmw.pdf" target="_blank">"Learning semantic similarity for very short texts"</a></pubtitle>, in <pubconf>Proc. IEEE Int. Conf. Data Min. Workshop (ICDMW 2015)</pubconf>, Atlantic City, NJ, USA, 15-17 Nov. <pubyear>2015</pubyear>.
  <ul class="file-links">
               <li><a href="papers/2015/deboom2015icdmw.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.1109/ICDMW.2015.86" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#deboom2015icdmw-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#deboom2015icdmw-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         <li><a href="https://arxiv.org/abs/1512.00765" target="_blank"><i class="ai ai-arxiv"></i> arXiv</a></li>
         
  </ul>
     </div>
     <div id="deboom2015icdmw-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Learning semantic similarity for very short texts</h2>
      <h4>C. De Boom, S. Van Canneyt, S. Bohez, T. Demeester and B. Dhoedt</h4>
      <br>in <pubconf>Proc. IEEE Int. Conf. Data Min. Workshop (ICDMW 2015)</pubconf>, Atlantic City, NJ, USA, 15-17 Nov. <pubyear>2015</pubyear>.
      <br><br>
      <p>Levering data on social media, such as Twitter and Facebook, requires information retrieval algorithms to become able to relate very short text fragments to each other. Traditional text similarity methods such as tf-idf cosine-similarity, based on word overlap, mostly fail to produce good results in this case, since word overlap is little or non-existent. Recently, distributed word representations, or word embeddings, have been shown to successfully allow words to match on the semantic level. In order to pair short text fragments -- as a concatenation of separate words - an adequate distributed sentence representation is needed, in existing literature often obtained by naively combining the individual word representations. We therefore investigated several text representations as a combination of word embeddings in the context of semantic pair matching. This paper investigates the effectiveness of several such naive techniques, as well as traditional tf-idf similarity, for fragments of different lengths. Our main contribution is a first step towards a hybrid method that combines the strength of dense distributed representations - as opposed to sparse term matching - with the strength of tf-idf based methods to automatically reduce the impact of less informative terms. Our new approach outperforms the existing techniques in a toy experimental set-up, leading to the conclusion that the combination of word embeddings and tf-idf information might lead to a better model for semantic content within very short text fragments.</p>
    </div>
    <div id="deboom2015icdmw-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Learning semantic similarity for very short texts</h2>
      <h4>C. De Boom, S. Van Canneyt, S. Bohez, T. Demeester and B. Dhoedt</h4>
      <br>in <pubconf>Proc. IEEE Int. Conf. Data Min. Workshop (ICDMW 2015)</pubconf>, Atlantic City, NJ, USA, 15-17 Nov. <pubyear>2015</pubyear>.
      <br><br>
      <code>
@inproceedings{deboom2015icdmw,<br>
  author = {De Boom, Cedric and Van Canneyt, Steven and Bohez, Steven and Demeester, Thomas and Dhoedt, Bart},<br>
  title = {Learning semantic similarity for very short texts},<br>
  booktitle = {Proc. IEEE Int. Conf. Data Min. Workshop (ICDMW 2015)},<br>
  month = {15--17 Nov.},<br>
  year = {2015},<br>
  address = {Atlantic City, NJ, USA},<br>
  doi = {10.1109/ICDMW.2015.86}<br>
}
      </code>
    </div>
</div> <!-- end row -->


<div id="barrio2015cikm" class="row add-bottom pubentry pub2015 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>P. Barrio, L. Gravano and C. Develder</pubauth>,
<pubtitle><a href="papers/2015/barrio2015cikm.pdf" target="_blank">"Ranking deep web text collections for scalable information extraction"</a></pubtitle>, in <pubconf>Proc. 24th ACM Int. Conf. Inf. Knowl. Management (CIKM 2015)</pubconf>, Melbourne, Australia, 19-23 Oct. <pubyear>2015</pubyear>, pp. 153-162.
  <ul class="file-links">
               <li><a href="papers/2015/barrio2015cikm.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.1145/2806416.2806581" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#barrio2015cikm-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#barrio2015cikm-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
  </ul>
     </div>
     <div id="barrio2015cikm-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Ranking deep web text collections for scalable information extraction</h2>
      <h4>P. Barrio, L. Gravano and C. Develder</h4>
      <br>in <pubconf>Proc. 24th ACM Int. Conf. Inf. Knowl. Management (CIKM 2015)</pubconf>, Melbourne, Australia, 19-23 Oct. <pubyear>2015</pubyear>, pp. 153-162.
      <br><br>
      <p>Information extraction (IE) systems discover structured in- formation from natural language text, to enable much richer querying and data mining than possible directly over the unstructured text. Unfortunately, IE is generally a com- putationally expensive process, and hence improving its ef- ficiency, so that it scales over large volumes of text, is of critical importance. State-of-the-art approaches for scaling the IE process focus on one text collection at a time. These approaches prioritize the extraction effort by learning key- word queries to identify the “useful” documents for the IE task at hand, namely, those that lead to the extraction of structured “tuples.” These approaches, however, do not at- tempt to predict which text collections are useful for the IE task—and hence merit further processing—and which ones will not contribute any useful output—and hence should be ignored altogether, for efficiency. In this paper, we focus on an especially valuable family of text sources, the so-called deep web collections, whose (remote) contents are only ac- cessible via querying. Specifically, we introduce and study techniques for ranking deep web collections for an IE task, to prioritize the extraction effort by focusing on collections with substantial numbers of useful documents for the task. We study both (adaptations of) state-of-the-art resource selec- tion strategies for distributed information retrieval, as well as IE-specific approaches. Our large-scale experimental eval- uation over realistic deep web collections, and for several different IE tasks, shows the merits and limitations of the alternative families of approaches, and provides a roadmap for addressing this critically important building block for efficient, scalable information extraction.</p>
    </div>
    <div id="barrio2015cikm-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Ranking deep web text collections for scalable information extraction</h2>
      <h4>P. Barrio, L. Gravano and C. Develder</h4>
      <br>in <pubconf>Proc. 24th ACM Int. Conf. Inf. Knowl. Management (CIKM 2015)</pubconf>, Melbourne, Australia, 19-23 Oct. <pubyear>2015</pubyear>, pp. 153-162.
      <br><br>
      <code>
@inproceedings{Barrio2015CIKM,<br>
  author = {Barrio, Pablo and Gravano, Luis and Develder, Chris},<br>
  title = {Ranking deep web text collections for scalable information extraction},<br>
  booktitle = {Proc. 24th ACM Int. Conf. Inf. Knowl. Management (CIKM 2015)},<br>
  month = {19--23 Oct.},<br>
  year = {2015},<br>
  pages = {153--162},<br>
  address = {Melbourne, Australia},<br>
  doi = {10.1145/2806416.2806581}<br>
}
      </code>
    </div>
</div> <!-- end row -->


<div id="sterckx2015wwwa" class="row add-bottom pubentry pub2015 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>L. Sterckx, T. Demeester, J. Deleu and C. Develder</pubauth>,
<pubtitle><a href="papers/2015/sterckx2015wwwa.pdf" target="_blank">"When topic models disagree: Keyphrase extraction with multiple topic models"</a></pubtitle>, in <pubconf>Proc. 24th Int. World Wide Web Conf. (WWW 2015)</pubconf>, Florence, Italy, 18-22 May <pubyear>2015</pubyear>, pp. 123-124.
  <ul class="file-links">
               <li><a href="papers/2015/sterckx2015wwwa.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="http://dx.doi.org/10.1145/2740908.2742731" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#sterckx2015wwwa-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#sterckx2015wwwa-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
  </ul>
     </div>
     <div id="sterckx2015wwwa-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>When topic models disagree: Keyphrase extraction with multiple topic models</h2>
      <h4>L. Sterckx, T. Demeester, J. Deleu and C. Develder</h4>
      <br>in <pubconf>Proc. 24th Int. World Wide Web Conf. (WWW 2015)</pubconf>, Florence, Italy, 18-22 May <pubyear>2015</pubyear>, pp. 123-124.
      <br><br>
      <p>We explore how the unsupervised extraction of topic-related keywords benefits from combining multiple topic models. We show that averaging multiple topic models, inferred from different corpora, leads to more accurate keyphrases than when using a single topic model and other state-of-the-art techniques. The experiments confirm the intuitive idea that a prerequisite for the significant benefit of combining multiple models is that the models should be sufficiently different, i.e., they should provide distinct contexts in terms of topical word importance.</p>
    </div>
    <div id="sterckx2015wwwa-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>When topic models disagree: Keyphrase extraction with multiple topic models</h2>
      <h4>L. Sterckx, T. Demeester, J. Deleu and C. Develder</h4>
      <br>in <pubconf>Proc. 24th Int. World Wide Web Conf. (WWW 2015)</pubconf>, Florence, Italy, 18-22 May <pubyear>2015</pubyear>, pp. 123-124.
      <br><br>
      <code>
@inproceedings{Sterckx2015WWWa,<br>
  author = {Lucas Sterckx and Thomas Demeester and Johannes Deleu and Chris Develder},<br>
  title = {When topic models disagree: Keyphrase extraction with multiple topic models},<br>
  booktitle = {Proc. 24th Int. World Wide Web Conf. (WWW 2015)},<br>
  month = {18--22 May},<br>
  year = {2015},<br>
  pages = {123--124},<br>
  address = {Florence, Italy},<br>
  doi = {10.1145/2740908.2742731}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="sterckx2015wwwb" class="row add-bottom pubentry pub2015 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>L. Sterckx, T. Demeester, J. Deleu and C. Develder</pubauth>,
<pubtitle><a href="papers/2015/sterckx2015wwwb.pdf" target="_blank">"Topical word importance for fast keyphrase extraction"</a></pubtitle>, in <pubconf>Proc. 24th Int. World Wide Web Conf. (WWW 2015)</pubconf>, Florence, Italy, 18-22 May <pubyear>2015</pubyear>, pp. 121-122.
  <ul class="file-links">
               <li><a href="papers/2015/sterckx2015wwwb.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="http://dx.doi.org/10.1145/2740908.2742730" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#sterckx2015wwwb-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#sterckx2015wwwb-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
  </ul>
     </div>
     <div id="sterckx2015wwwb-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Topical word importance for fast keyphrase extraction</h2>
      <h4>L. Sterckx, T. Demeester, J. Deleu and C. Develder</h4>
      <br>in <pubconf>Proc. 24th Int. World Wide Web Conf. (WWW 2015)</pubconf>, Florence, Italy, 18-22 May <pubyear>2015</pubyear>, pp. 121-122.
      <br><br>
      <p>We propose an improvement on a state-of-the-art keyphrase extraction algorithm, Topical PageRank (TPR), incorporating topical information from topic models. While the original algorithm requires a random walk for each topic in the topic model being used, ours is independent of the topic model, computing but a single PageRank for each text regardless of the amount of topics in the model. This increases the speed drastically and enables it for use on large collections of text using vast topic models, while not altering performance of the original algorithm.</p>
    </div>
    <div id="sterckx2015wwwb-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Topical word importance for fast keyphrase extraction</h2>
      <h4>L. Sterckx, T. Demeester, J. Deleu and C. Develder</h4>
      <br>in <pubconf>Proc. 24th Int. World Wide Web Conf. (WWW 2015)</pubconf>, Florence, Italy, 18-22 May <pubyear>2015</pubyear>, pp. 121-122.
      <br><br>
      <code>
@inproceedings{Sterckx2015WWWb,<br>
  author = {Lucas Sterckx and Thomas Demeester and Johannes Deleu and Chris Develder},<br>
  title = {Topical word importance for fast keyphrase extraction},<br>
  booktitle = {Proc. 24th Int. World Wide Web Conf. (WWW 2015)},<br>
  month = {18--22 May},<br>
  year = {2015},<br>
  pages = {121--122},<br>
  address = {Florence, Italy},<br>
  doi = {10.1145/2740908.2742730}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="demeester2015fedweb" class="row add-bottom pubentry pub2015 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>T. Demeester, D. Trieschnigg, K. Zhou, D. Nguyen and D. Hiemstra</pubauth>,
<pubtitle><a href="papers/2015/demeester2015fedweb.pdf" target="_blank">"FedWeb greatest hits: Presenting the new test collection for federated web search"</a></pubtitle>, in <pubconf>Proc. 24th Int. World Wide Web Conf. (WWW 2015)</pubconf>, Florence, Italy, 18-22 May <pubyear>2015</pubyear>.
  <ul class="file-links">
               <li><a href="papers/2015/demeester2015fedweb.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.1145/2740908.2742755" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#demeester2015fedweb-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#demeester2015fedweb-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="demeester2015fedweb-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>FedWeb greatest hits: Presenting the new test collection for federated web search</h2>
      <h4>T. Demeester, D. Trieschnigg, K. Zhou, D. Nguyen and D. Hiemstra</h4>
      <br>in <pubconf>Proc. 24th Int. World Wide Web Conf. (WWW 2015)</pubconf>, Florence, Italy, 18-22 May <pubyear>2015</pubyear>.
      <br><br>
      <p>This paper presents ‘FedWeb Greatest Hits’, a large new test collection for research in web information retrieval. As a combination and extension of the datasets used in the TREC Federated Web Search Track, this collection opens up new research possibilities on federated web search challenges, as well as on various other problems.</p>
    </div>
    <div id="demeester2015fedweb-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>FedWeb greatest hits: Presenting the new test collection for federated web search</h2>
      <h4>T. Demeester, D. Trieschnigg, K. Zhou, D. Nguyen and D. Hiemstra</h4>
      <br>in <pubconf>Proc. 24th Int. World Wide Web Conf. (WWW 2015)</pubconf>, Florence, Italy, 18-22 May <pubyear>2015</pubyear>.
      <br><br>
      <code>
@inproceedings{demeester2015fedweb,<br>
  author = {Demeester, Thomas and Trieschnigg, Dolf and Zhou, Ke and Nguyen, Dong and Hiemstra, Djoerd},<br>
  title = {FedWeb greatest hits: Presenting the new test collection for federated web search},<br>
  booktitle = {Proc. 24th Int. World Wide Web Conf. (WWW 2015)},<br>
  month = {18--22 May},<br>
  year = {2015},<br>
  address = {Florence, Italy},<br>
  doi = {10.1145/2740908.2742755}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="sterckx2014akbc" class="row add-bottom pubentry pub2014 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>L. Sterckx, T. Demeester, J. Deleu and C. Develder</pubauth>,
<pubtitle><a href="papers/2014/sterckx2014akbc.pdf" target="_blank">"Using semantic clustering and active learning for noise reduction in distant supervision"</a></pubtitle>, in <pubconf>Proc. 4th Workshop on Automated Knowledge Base Construction (AKBC 2014) at NIPS 2014</pubconf>, Montreal, Canada, 13 Dec. <pubyear>2014</pubyear>.
    <ul class="file-links">
               <li><a href="papers/2014/sterckx2014akbc.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           
           
               <li><a class="popup-with-zoom-anim" href="#sterckx2014akbc-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#sterckx2014akbc-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
    </ul>
     </div>
     <div id="sterckx2014akbc-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Using semantic clustering and active learning for noise reduction in distant supervision</h2>
        <h4>L. Sterckx, T. Demeester, J. Deleu and C. Develder</h4>
        <br>in <pubconf>Proc. 4th Workshop on Automated Knowledge Base Construction (AKBC 2014) at NIPS 2014</pubconf>, Montreal, Canada, 13 Dec. <pubyear>2014</pubyear>.
        <br><br>
        <p>The use of external databases to generate training data, also known as Distant Supervision, has become an effective way to train supervised relation extractors but this approach inherently suffers from noise. In this paper we propose a method for noise reduction in distantly supervised training data, using a discriminative classifier and semantic similarity between the contexts of the training examples. We describe an active learning strategy which exploits hierarchical clustering of the candidate training samples. To further improve the effectiveness of this approach, we study the use of several methods for dimensionality reduction of the training samples. We find that semantic clustering of training data combined with cluster-based active learning allows filtering the training data, hence facilitating the creation of a clean training set for relation extraction, at a reduced manual labeling cost.</p>
    </div>
    <div id="sterckx2014akbc-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Using semantic clustering and active learning for noise reduction in distant supervision</h2>
        <h4>L. Sterckx, T. Demeester, J. Deleu and C. Develder</h4>
        <br>in <pubconf>Proc. 4th Workshop on Automated Knowledge Base Construction (AKBC 2014) at NIPS 2014</pubconf>, Montreal, Canada, 13 Dec. <pubyear>2014</pubyear>.
        <br><br>
        <code>
@inproceedings{Sterckx2014AKBC,<br>
  author = {Sterckx, Lucas and Demeester, Thomas and Deleu, Johannes and Develder, Chris},<br>
  title = {Using semantic clustering and active learning for noise reduction in distant supervision},<br>
  booktitle = {Proc. 4th Workshop on Automated Knowledge Base Construction (AKBC 2014) at NIPS 2014},<br>
  month = {13 Dec.},<br>
  year = {2014},<br>
  address = {Montreal, Canada}<br>
}
        </code>
      </div>
</div> <!-- end row -->


<div id="feys2014fire" class="row add-bottom pubentry pub2014 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>M. Feys, T. Demeester, B. Fortuna, J. Deleu and C. Develder</pubauth>,
<pubtitle><a href="papers/2014/feys2014fire.pdf" target="_blank">"On the robustness of event detection evaluation: A case study"</a></pubtitle>, in <pubconf>Proc. Forum for Inf. Retr. Evaluation (FIRE 2014)</pubconf>, Bangalore, India, 5-7 Dec. <pubyear>2014</pubyear>.
    <ul class="file-links">
               <li><a href="papers/2014/feys2014fire.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           
           
               <li><a class="popup-with-zoom-anim" href="#feys2014fire-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#feys2014fire-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
    </ul>
     </div>
     <div id="feys2014fire-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>On the robustness of event detection evaluation: A case study</h2>
        <h4>M. Feys, T. Demeester, B. Fortuna, J. Deleu and C. Develder</h4>
        <br>in <pubconf>Proc. Forum for Inf. Retr. Evaluation (FIRE 2014)</pubconf>, Bangalore, India, 5-7 Dec. <pubyear>2014</pubyear>.
        <br><br>
        <p>Research on evaluation of IR systems has led to the insight that a robust evaluation strategy requires tests on a large number of events/queries. However, especially for event detection, the number of manually labeled events may be limited. In this paper we investigate how to optimize the evaluation strategy in those cases to maximize robustness. We also introduce two new vector space models for event detection that aim to incorporate bursty information of terms and compare these with existing models. Our experiments show that by using graded relevance levels we can reduce the impact of subjectivity and ambiguity of event detection evaluation. We also show that although user disagreement is significant, it has no real impact on the ranking of the results.</p>
    </div>
    <div id="feys2014fire-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>On the robustness of event detection evaluation: A case study</h2>
        <h4>M. Feys, T. Demeester, B. Fortuna, J. Deleu and C. Develder</h4>
        <br>in <pubconf>Proc. Forum for Inf. Retr. Evaluation (FIRE 2014)</pubconf>, Bangalore, India, 5-7 Dec. <pubyear>2014</pubyear>.
        <br><br>
        <code>
@inproceedings{Feys2014FIRE,<br>
  author = {Feys, Matthias and Demeester, Thomas and Fortuna, Blaz and Deleu, Johannes and Develder, Chris},<br>
  title = {On the robustness of event detection evaluation: A case study},<br>
  booktitle = {Proc. Forum for Inf. Retr. Evaluation (FIRE 2014)},<br>
  month = {5--7 Dec.},<br>
  year = {2014},<br>
  address = {Bangalore, India}<br>
}
        </code>
      </div>
</div> <!-- end row -->

<div id="mertens2014fire" class="row add-bottom pubentry pub2014 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>L. Mertens, T. Demeester, J. Deleu, M. Feys and C. Develder</pubauth>,
<pubtitle><a href="papers/2014/mertens2014fire.pdf" target="_blank">"Entity linking: Test collections revisited"</a></pubtitle>, in <pubconf>Proc. Forum for Inf. Retr. Evaluation (FIRE 2014)</pubconf>, Bangalore, India, 5-7 Dec. <pubyear>2014</pubyear>.
    <ul class="file-links">
               <li><a href="papers/2014/mertens2014fire.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           
           
               <li><a class="popup-with-zoom-anim" href="#mertens2014fire-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#mertens2014fire-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
    </ul>
     </div>
     <div id="mertens2014fire-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Entity linking: Test collections revisited</h2>
        <h4>L. Mertens, T. Demeester, J. Deleu, M. Feys and C. Develder</h4>
        <br>in <pubconf>Proc. Forum for Inf. Retr. Evaluation (FIRE 2014)</pubconf>, Bangalore, India, 5-7 Dec. <pubyear>2014</pubyear>.
        <br><br>
        <p>This paper analyzes two important conditions that are usually taken for granted in the evaluation of information retrieval systems: the test queries should be representative for the intended application scenario, and a sufficient amount of queries are needed to robustly assess system performance, as well as discern performance differ- ences between systems. Both issues have important consequences, as studied in this paper for the specific case of Entity Linking systems. We investigate two methods for automatic query generation, and show them to have a vast impact on evaluated system perfor- mance. We further demonstrate the effect a query set’s size has on its ability to faithfully distinguish systems, and propose a method for assessing the possible impact on system performance adding a specific number of queries to the set might have.</p>
    </div>
    <div id="mertens2014fire-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Entity linking: Test collections revisited</h2>
        <h4>L. Mertens, T. Demeester, J. Deleu, M. Feys and C. Develder</h4>
        <br>in <pubconf>Proc. Forum for Inf. Retr. Evaluation (FIRE 2014)</pubconf>, Bangalore, India, 5-7 Dec. <pubyear>2014</pubyear>.
        <br><br>
        <code>
@inproceedings{Mertens2014FIRE,<br>
  author = {Mertens, Laurent and Demeester, Thomas and Deleu, Johannes and Feys, Matthias and Develder, Chris},<br>
  title = {Entity linking: Test collections revisited},<br>
  booktitle = {Proc. Forum for Inf. Retr. Evaluation (FIRE 2014)},<br>
  month = {5--7 Dec.},<br>
  year = {2014},<br>
  address = {Bangalore, India}<br>
}
        </code>
      </div>
</div> <!-- end row -->

<div id="feys2014tac" class="row add-bottom pubentry pub2014 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>M. Feys, L. Sterckx, L. Mertens, J. Deleu, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2014/feys2014tac.pdf" target="_blank">"Ghent University-IBCN participation in TAC-KBP 2014 slot filling and cold start tasks"</a></pubtitle>, in <pubconf>Proc. 7th Text Analysis Conf. (TAC 2014)</pubconf>, Gaithersburg, MD, USA, 17-18 Nov. <pubyear>2014</pubyear>.
    <ul class="file-links">
               <li><a href="papers/2014/feys2014tac.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           
           
               <li><a class="popup-with-zoom-anim" href="#feys2014tac-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#feys2014tac-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
    </ul>
     </div>
     <div id="feys2014tac-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Ghent University-IBCN participation in TAC-KBP 2014 slot filling and cold start tasks</h2>
        <h4>M. Feys, L. Sterckx, L. Mertens, J. Deleu, T. Demeester and C. Develder</h4>
        <br>in <pubconf>Proc. 7th Text Analysis Conf. (TAC 2014)</pubconf>, Gaithersburg, MD, USA, 17-18 Nov. <pubyear>2014</pubyear>.
        <br><br>
        <p>This paper presents the system of the UGENT IBCN team for the TAC KBP 2014 slot filling and cold start (slot filling variant) tasks. This was the team’s first participation in both tasks. The slot filling system uses distant supervision to generate training data combined with a noise reduction step, and two different types of classifiers. We show that the noise reduction step significantly improves precision, and propose an application of word embeddings for slot filling.</p>
    </div>
    <div id="feys2014tac-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Ghent University-IBCN participation in TAC-KBP 2014 slot filling and cold start tasks</h2>
        <h4>M. Feys, L. Sterckx, L. Mertens, J. Deleu, T. Demeester and C. Develder</h4>
        <br>in <pubconf>Proc. 7th Text Analysis Conf. (TAC 2014)</pubconf>, Gaithersburg, MD, USA, 17-18 Nov. <pubyear>2014</pubyear>.
        <br><br>
        <code>
@inproceedings{Feys2014TAC,<br>
  author = {Feys, Matthias and Sterckx, Lucas and Mertens, Laurent and Deleu, Johannes and Demeester, Thomas and Develder, Chris},<br>
  title = {Ghent University-IBCN participation in TAC-KBP 2014 slot filling and cold start tasks},<br>
  booktitle = {Proc. 7th Text Analysis Conf. (TAC 2014)},<br>
  month = {17--18 Nov.},<br>
  year = {2014},<br>
  address = {Gaithersburg, MD, USA}<br>
}
        </code>
      </div>
</div> <!-- end row -->


<div id="zhou2014" class="row add-bottom pubentry pub2014 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>K. Zhou, T. Demeester, D. Nguyen, D. Hiemstra and D. Trieschnigg</pubauth>,
<pubtitle><a href="papers/2014/zhou2014.pdf" target="_blank">"Aligning vertical collection relevance with user intent"</a></pubtitle>, in <pubconf>Proc. 23rd ACM Int. Conf. Inf. Knowl. Management (CIKM 2014)</pubconf>, Shanghai, China, 3-7 Nov. <pubyear>2014</pubyear>, pp. 1915-1918.
  <ul class="file-links">
               <li><a href="papers/2014/zhou2014.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.1145/2661829.2661941" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#zhou2014-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#zhou2014-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="zhou2014-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Aligning vertical collection relevance with user intent</h2>
      <h4>K. Zhou, T. Demeester, D. Nguyen, D. Hiemstra and D. Trieschnigg</h4>
      <br>in <pubconf>Proc. 23rd ACM Int. Conf. Inf. Knowl. Management (CIKM 2014)</pubconf>, Shanghai, China, 3-7 Nov. <pubyear>2014</pubyear>, pp. 1915-1918.
      <br><br>
      <p>Selecting and aggregating different types of content from multiple vertical search engines is becoming popular in web search. The user vertical intent, the verticals the user expects to be relevant for a particular information need, might not correspond to the vertical collection relevance, the verticals containing the most relevant content. In this work we propose different approaches to define the set of relevant verticals based on document judgments. We correlate the collection-based relevant verticals obtained from these approaches to the real user vertical intent, and show that they can be aligned relatively well. The set of relevant verticals defined by those approaches could therefore serve as an approximate but reliable ground-truth for evaluating vertical selection, avoiding the need for collecting explicit user vertical intent, and vice versa.</p>
    </div>
    <div id="zhou2014-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Aligning vertical collection relevance with user intent</h2>
      <h4>K. Zhou, T. Demeester, D. Nguyen, D. Hiemstra and D. Trieschnigg</h4>
      <br>in <pubconf>Proc. 23rd ACM Int. Conf. Inf. Knowl. Management (CIKM 2014)</pubconf>, Shanghai, China, 3-7 Nov. <pubyear>2014</pubyear>, pp. 1915-1918.
      <br><br>
      <code>
@inproceedings{zhou2014,<br>
  author = {Zhou, Ke and Demeester, Thomas and Nguyen, Dong and Hiemstra, Djoerd and Trieschnigg, Dolf},<br>
  title = {Aligning vertical collection relevance with user intent},<br>
  booktitle = {Proc. 23rd ACM Int. Conf. Inf. Knowl. Management (CIKM 2014)},<br>
  month = {3--7 Nov.},<br>
  year = {2014},<br>
  pages = {1915--1918},<br>
  address = {Shanghai, China},<br>
  doi = {10.1145/2661829.2661941}<br>
}
      </code>
    </div>
</div> <!-- end row -->


<div id="demeester2014trec" class="row add-bottom pubentry pub2014 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>T. Demeester, D. Drieschnigg, K. Zhou and D. Hiemstra</pubauth>,
<pubtitle><a href="papers/2014/demeester2014trec.pdf" target="_blank">"Overview of the TREC 2014 federated web search track"</a></pubtitle>, in <pubconf>Proc. 23rd Text Retr. Conf. (TREC 2014)</pubconf>, Gaithersburg, MD, USA, 19-21 Nov. <pubyear>2014</pubyear>.
  <ul class="file-links">
               <li><a href="papers/2014/demeester2014trec.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         
         <li><a href="https://trec.nist.gov/pubs/trec23/papers/overview-federated.pdf" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#demeester2014trec-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#demeester2014trec-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="demeester2014trec-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Overview of the TREC 2014 federated web search track</h2>
      <h4>T. Demeester, D. Drieschnigg, K. Zhou and D. Hiemstra</h4>
      <br>in <pubconf>Proc. 23rd Text Retr. Conf. (TREC 2014)</pubconf>, Gaithersburg, MD, USA, 19-21 Nov. <pubyear>2014</pubyear>.
      <br><br>
      <p>The TREC Federated Web Search track facilitates research on federated web search, by providing a large realistic data collection sampled from a multitude of online search engines. The FedWeb 2013 Resource Selection and Results Merging tasks are again included in FedWeb 2014, and we additionally introduced the task of vertical selection. Other new aspects are the required link between the Resource Selection and Results Merging tasks, and the importance of diversity in the merged results. After an overview of the new data collection and relevance judgments, the individual participants’ results for the tasks are introduced, analyzed, and compared.</p>
    </div>
    <div id="demeester2014trec-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Overview of the TREC 2014 federated web search track</h2>
      <h4>T. Demeester, D. Drieschnigg, K. Zhou and D. Hiemstra</h4>
      <br>in <pubconf>Proc. 23rd Text Retr. Conf. (TREC 2014)</pubconf>, Gaithersburg, MD, USA, 19-21 Nov. <pubyear>2014</pubyear>.
      <br><br>
      <code>
@inproceedings{demeester2014trec,<br>
  author = {Demeester, Thomas and Drieschnigg, Dong and Zhou, Ke and Hiemstra, Djoerd},<br>
  title = {Overview of the TREC 2014 federated web search track},<br>
  booktitle = {Proc. 23rd Text Retr. Conf. (TREC 2014)},<br>
  month = {19--21 Nov.},<br>
  year = {2014},<br>
  address = {Gaithersburg, MD, USA},<br>
  url = {https://trec.nist.gov/pubs/trec23/papers/overview-federated.pdf}<br>
}
      </code>
    </div>
</div> <!-- end row -->


<div id="fortuna2014" class="row add-bottom pubentry pub2014 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>B. Fortuna, T. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2014/fortuna2014.pdf" target="_blank">"Towards large-scale event detection and extraction from news"</a></pubtitle>, in <pubconf>Proc. Large-scale Online Learn. and Decision Making Workshop (LSOLDM 2014)</pubconf>, Windsor, UK, 10-12 Sep. <pubyear>2014</pubyear>.
    <ul class="file-links">
               <li><a href="papers/2014/fortuna2014.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           
           
               <li><a class="popup-with-zoom-anim" href="#fortuna2014-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#fortuna2014-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
    </ul>
     </div>
     <div id="fortuna2014-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Towards large-scale event detection and extraction from news</h2>
        <h4>B. Fortuna, T. Demeester and C. Develder</h4>
        <br>in <pubconf>Proc. Large-scale Online Learn. and Decision Making Workshop (LSOLDM 2014)</pubconf>, Windsor, UK, 10-12 Sep. <pubyear>2014</pubyear>.
        <br><br>
        <p>Understanding and reasoning about textual data is one of the important topics in artificial intelligence and is being addressed by various research communities, ranging from knowledge representation, over natural language processing to text mining. Each community provides a different set of often overlapping intuitions, tools and methodologies for working with text.<br>Event processing from news and social media can be seen as a subtopic of text understanding [1, 2, 3, 4]. It comprises different tasks, including New and Retrospective Event Discovery, Event Type Classification and Event Template Extraction.<br>Research on event processing requires access to annotated data covering different tasks in the event processing pipeline. Over the last decades, several datasets have been created covering event discovery and event extraction, e.g., [5, 6, 7]. These datasets are rather limited in scope. For example, they contain articles from only few selected sources, or they contain a limited number of annotated events with a high selection bias (e.g., towards larger or well defined events like natural disasters or terrorism). Using such limited datasets to evaluate solutions for the event processing tasks may lead to favoring approaches that do not work well on real-world datasets. The main reasons for these limitations are (1) limited access to data resources and (2) the required and expensive manual annotations.<br>The main contributions we are working towards are (1) a systematic methodology for efficiently creating a large golden standard of manually annotated events over a large corpus of news articles with a realistic distribution over the covered topics and events, and (2) a resulting annotated corpus a resulting annotated corpus of 10,000 English general news articles embedded in 31 million news articles.</p>
    </div>
    <div id="fortuna2014-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Towards large-scale event detection and extraction from news</h2>
        <h4>B. Fortuna, T. Demeester and C. Develder</h4>
        <br>in <pubconf>Proc. Large-scale Online Learn. and Decision Making Workshop (LSOLDM 2014)</pubconf>, Windsor, UK, 10-12 Sep. <pubyear>2014</pubyear>.
        <br><br>
        <code>
@inproceedings{Fortuna2014,<br>
  author = {Fortuna, Blaz and Demeester, Thomas and Develder, Chris},<br>
  title = {Towards large-scale event detection and extraction from news},<br>
  booktitle = {Proc. Large-scale Online Learn. and Decision Making Workshop (LSOLDM 2014)},<br>
  month = {10--12 Sep.},<br>
  year = {2014},<br>
  address = {Windsor, UK}<br>
}
        </code>
      </div>
</div> <!-- end row -->

<div id="sterckx2014ecir" class="row add-bottom pubentry pub2014 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>L. Sterckx, T. Demeester, J. Deleu, L. Mertens and C. Develder</pubauth>,
<pubtitle><a href="papers/2014/sterckx2014ecir.pdf" target="_blank">"Assessing quality of unsupervised topics in song lyrics"</a></pubtitle>, in <pubconf>Proc. 36th Eur. Conf. Inf. Retr. (ECIR 2014)</pubconf>, Amsterdam, The Netherlands, 13-16 Apr. <pubyear>2014</pubyear>.
    <ul class="file-links">
               <li><a href="papers/2014/sterckx2014ecir.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           <li><a href="http://dx.doi.org/10.1007/978-3-319-06028-6_55" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
           
               <li><a class="popup-with-zoom-anim" href="#sterckx2014ecir-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#sterckx2014ecir-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
    </ul>
     </div>
     <div id="sterckx2014ecir-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Assessing quality of unsupervised topics in song lyrics</h2>
        <h4>L. Sterckx, T. Demeester, J. Deleu, L. Mertens and C. Develder</h4>
        <br>in <pubconf>Proc. 36th Eur. Conf. Inf. Retr. (ECIR 2014)</pubconf>, Amsterdam, The Netherlands, 13-16 Apr. <pubyear>2014</pubyear>.
        <br><br>
        <p>How useful are topic models based on song lyrics for applications in music information retrieval? Unsupervised topic models on text corpora are often difficult to interpret. Based on a large collection of lyrics, we investigate how well automatically generated topics are related to manual topic annotations. We propose to use the kurtosis metric to align unsupervised topics with a reference model of supervised topics. This metric is well-suited for topic assessments, as it turns out to be more strongly correlated with manual topic quality scores than existing measures for semantic coherence. We also show how it can be used for a detailed graphical topic quality assessment.</p>
    </div>
    <div id="sterckx2014ecir-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Assessing quality of unsupervised topics in song lyrics</h2>
        <h4>L. Sterckx, T. Demeester, J. Deleu, L. Mertens and C. Develder</h4>
        <br>in <pubconf>Proc. 36th Eur. Conf. Inf. Retr. (ECIR 2014)</pubconf>, Amsterdam, The Netherlands, 13-16 Apr. <pubyear>2014</pubyear>.
        <br><br>
        <code>
@inproceedings{Sterckx2014ECIR,<br>
  author = {Sterckx, Lucas and Demeester, Thomas and Deleu, Johannes and Mertens, Laurent and Develder, Chris},<br>
  title = {Assessing quality of unsupervised topics in song lyrics},<br>
  booktitle = {Proc. 36th Eur. Conf. Inf. Retr. (ECIR 2014)},<br>
  month = {13--16 Apr.},<br>
  year = {2014},<br>
  address = {Amsterdam, The Netherlands},<br>
  doi = {10.1007/978-3-319-06028-6_55}<br>
}
        </code>
      </div>
</div> <!-- end row -->

<div id="vancanneyt2014snow" class="row add-bottom pubentry pub2014 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>S. Van Canneyt, M. Feys, S. Schockaert, T. Demeester, C. Develder and B. Dhoedt</pubauth>,
<pubtitle><a href="papers/2014/vancanneyt2014snow.pdf" target="_blank">"Detecting newsworthy topics in Twitter"</a></pubtitle>, in <pubconf>Proc. 2nd Workshop on Social News on the Web at WWW 2014 (SNOW 2014)</pubconf>, Seoul, Korea, 8 Apr. <pubyear>2014</pubyear>.
    <ul class="file-links">
               <li><a href="papers/2014/vancanneyt2014snow.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           
           
               <li><a class="popup-with-zoom-anim" href="#vancanneyt2014snow-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#vancanneyt2014snow-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
    </ul>
     </div>
     <div id="vancanneyt2014snow-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Detecting newsworthy topics in Twitter</h2>
        <h4>S. Van Canneyt, M. Feys, S. Schockaert, T. Demeester, C. Develder and B. Dhoedt</h4>
        <br>in <pubconf>Proc. 2nd Workshop on Social News on the Web at WWW 2014 (SNOW 2014)</pubconf>, Seoul, Korea, 8 Apr. <pubyear>2014</pubyear>.
        <br><br>
        <p>The task of the SNOW 2014 Data Challenge is to mine Twit- ter streams to provide journalists a set of headlines and complementary information that summarize the most newswor- thy topics for a number of given time intervals. We propose a 4-step approach to solve this. First, a classifier is trained to determine whether a Twitter user is likely to post tweets about newsworthy stories. Second, tweets posted by these users during the time interval of interest are clustered into topics. For this clustering, the cosine similarity between a boosted tf-idf representation of the tweets is used. Third, we use a classifier to estimate the confidence that the obtained topics are newsworthy. Finally, for each obtained newswor- thy topic, a descriptive headline is generated together with relevant keywords, tweets and pictures. Experimental re- sults show the effectiveness of the proposed methodology.</p>
    </div>
    <div id="vancanneyt2014snow-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Detecting newsworthy topics in Twitter</h2>
        <h4>S. Van Canneyt, M. Feys, S. Schockaert, T. Demeester, C. Develder and B. Dhoedt</h4>
        <br>in <pubconf>Proc. 2nd Workshop on Social News on the Web at WWW 2014 (SNOW 2014)</pubconf>, Seoul, Korea, 8 Apr. <pubyear>2014</pubyear>.
        <br><br>
        <code>
@inproceedings{vancanneyt2014snow,<br>
  author = {Van Canneyt, Steven and Feys, Matthias and Schockaert, Steven and Demeester, Thomas and Develder, Chris and Dhoedt, Bart},<br>
  title = {Detecting newsworthy topics in Twitter},<br>
  booktitle = {Proc. 2nd Workshop on Social News on the Web at WWW 2014 (SNOW 2014)},<br>
  month = {8 Apr.},<br>
  year = {2014},<br>
  address = {Seoul, Korea}<br>
}
        </code>
      </div>
</div> <!-- end row -->


<div id="demeester2014ir" class="row add-bottom pubentry pub2014 pubir pubarticle">
     <div class="one column">
     <h3 class="article"><span>pubarticle</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>R. Aly, T. Demeester and S. Robertson</pubauth>,
<pubtitle><a href="papers/2014/demeester2014ir.pdf" target="_blank">"Probabilistic models in IR and their relationships"</a></pubtitle>, <pubjournal>Inf. Retr.</pubjournal>, Vol. 17, No. 2, Apr. <pubyear>2014</pubyear>, pp. 177-201.
  <ul class="file-links">
               <li><a href="papers/2014/demeester2014ir.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.1007/s10791-013-9226-3" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#demeester2014ir-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#demeester2014ir-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="demeester2014ir-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Probabilistic models in IR and their relationships</h2>
      <h4>R. Aly, T. Demeester and S. Robertson</h4>
      <br><pubjournal>Inf. Retr.</pubjournal>, Vol. 17, No. 2, Apr. <pubyear>2014</pubyear>, pp. 177-201.
      <br><br>
      <p>A solid research path towards new information retrieval models is to further develop the theory behind existing models. A profound understanding of these models is therefore essential. In this paper, we revisit probability ranking principle (PRP)-based models, probability of relevance (PR) models, and language models, finding conceptual differences in their definition and interrelationships. The probabilistic model of the PRP has not been explicitly defined previously, but doing so leads to the formulation of two actual principles with different objectives. First, the belief probability ranking principle (BPRP), which considers uncertain relevance between known documents and the current query, and second, the popularity probability ranking principle (PPRP), which considers the probability of relevance of documents among multiple queries with the same features. Our analysis shows how some of the discussed PR models implement the BPRP or the PPRP while others do not. However, for some models the parameter estimation is challenging. Finally, language models are often presented as related to PR models. However, we find that language models differ from PR models in every aspect of a probabilistic model and the effectiveness of language models cannot be explained by the PRP.</p>
    </div>
    <div id="demeester2014ir-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Probabilistic models in IR and their relationships</h2>
      <h4>R. Aly, T. Demeester and S. Robertson</h4>
      <br><pubjournal>Inf. Retr.</pubjournal>, Vol. 17, No. 2, Apr. <pubyear>2014</pubyear>, pp. 177-201.
      <br><br>
      <code>
@article{demeester2014ir,<br>
  author = {Aly, Robin and Demeester, Thomas and Robertson, Stephen},<br>
  title = {Probabilistic models in IR and their relationships},<br>
  journal = {Inf. Retr.},<br>
  month = {Apr.},<br>
  year = {2014},<br>
  volume = {17},<br>
  number = {2},<br>
  pages = {177--201},<br>
  doi = {10.1007/s10791-013-9226-3}<br>
}
      </code>
    </div>
</div> <!-- end row -->


<div id="demeester2014wsdm" class="row add-bottom pubentry pub2014 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>T. Demeester, R. Aly, D. Hiemstra, D. Nguyen, D. Trieschnigg and C. Develder</pubauth>,
<pubtitle><a href="papers/2014/demeester2014wsdm.pdf" target="_blank">"Exploiting user disagreement for web search evaluation: An experimental approach"</a></pubtitle>, in <pubconf>Proc. 7th ACM Int. Conf. Web Search and Data Min. (WSDM 2014)</pubconf>, New York, NY, USA, 24-28 Feb. <pubyear>2014</pubyear>.
    <ul class="file-links">
               <li><a href="papers/2014/demeester2014wsdm.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           <li><a href="http://dx.doi.org/10.1145/2556195.2556268" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
           
               <li><a class="popup-with-zoom-anim" href="#demeester2014wsdm-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#demeester2014wsdm-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
    </ul>
     </div>
     <div id="demeester2014wsdm-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Exploiting user disagreement for web search evaluation: An experimental approach</h2>
        <h4>T. Demeester, R. Aly, D. Hiemstra, D. Nguyen, D. Trieschnigg and C. Develder</h4>
        <br>in <pubconf>Proc. 7th ACM Int. Conf. Web Search and Data Min. (WSDM 2014)</pubconf>, New York, NY, USA, 24-28 Feb. <pubyear>2014</pubyear>.
        <br><br>
        <p>In order to express a more nuanced notion of relevance as compared to binary judgments, graded relevance levels can be used for the evaluation of search results. Especially in Web search, users strongly prefer top results over less relevant results, and yet they often disagree on which are the top results for a given information need. This paper proposes a method to capture this user disagreement and integrate it into the evaluation procedure.<br>First, we present experiments that investigate the user disagreement. After that, a probabilistic model is proposed that results in a weighting of the relevance levels with a probabilistic interpretation. This is followed by a validity analysis, and an explanation of how to integrate the model with well-established evaluation metrics. Finally, we discuss a specific application of the model, in the estimation of suitable combined page and snippet relevance weights from Web search assessments.</p>
    </div>
    <div id="demeester2014wsdm-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Exploiting user disagreement for web search evaluation: An experimental approach</h2>
        <h4>T. Demeester, R. Aly, D. Hiemstra, D. Nguyen, D. Trieschnigg and C. Develder</h4>
        <br>in <pubconf>Proc. 7th ACM Int. Conf. Web Search and Data Min. (WSDM 2014)</pubconf>, New York, NY, USA, 24-28 Feb. <pubyear>2014</pubyear>.
        <br><br>
        <code>
@inproceedings{Demeester2014WSDM,<br>
  author = {Demeester, Thomas and Robin Aly and Djoerd Hiemstra and Dong Nguyen and Dolf Trieschnigg and Chris Develder},<br>
  title = {Exploiting user disagreement for web search evaluation: An experimental approach},<br>
  booktitle = {Proc. 7th ACM Int. Conf. Web Search and Data Min. (WSDM 2014)},<br>
  month = {24--28 Feb.},<br>
  year = {2014},<br>
  address = {New York, NY, USA},<br>
  note = {Acceptance rate: 17% (64/376)},<br>
  doi = {10.1145/2556195.2556268}<br>
}
        </code>
      </div>
</div> <!-- end row -->

<div id="mertens13_tac" class="row add-bottom pubentry pub2013 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>L. Mertens, T. Demeester, J. Deleu and C. Develder</pubauth>,
<pubtitle><a href="papers/2013/mertens13_tac.pdf" target="_blank">"UGent participation in the TAC 2013 entity-linking task"</a></pubtitle>, in <pubconf>Proc. Text Analysis Conference (TAC 2013)</pubconf>, Gaithersburg, MD, USA, 18-19 Nov. <pubyear>2013</pubyear>, pp. 1-12.
  <ul class="file-links">
               <li><a href="papers/2013/mertens13_tac.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         
         <li><a href="https://tac.nist.gov//publications/2013/participant.papers/UGENT_IBCN.TAC2013.proceedings.pdf"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#mertens13_tac-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#mertens13_tac-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="mertens13_tac-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>UGent participation in the TAC 2013 entity-linking task</h2>
      <h4>L. Mertens, T. Demeester, J. Deleu and C. Develder</h4>
      <br>in <pubconf>Proc. Text Analysis Conference (TAC 2013)</pubconf>, Gaithersburg, MD, USA, 18-19 Nov. <pubyear>2013</pubyear>, pp. 1-12.
      <br><br>
      <p>This article describes the system used by the UGent-IBCN team for participating in the Text Analysis Conference (TAC) 2013 English Entity-Linking task. We kept the overall rule-based workflow of our last year’s submission, but significantly altered individual<br>components. Most importantly, these changes include improved document pre-processing, new ways of candidate selection, and completely redesigned scoring and NIL-detection mechanisms. Finally, we provide detailed data of our system’s performance.</p>
    </div>
    <div id="mertens13_tac-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>UGent participation in the TAC 2013 entity-linking task</h2>
      <h4>L. Mertens, T. Demeester, J. Deleu and C. Develder</h4>
      <br>in <pubconf>Proc. Text Analysis Conference (TAC 2013)</pubconf>, Gaithersburg, MD, USA, 18-19 Nov. <pubyear>2013</pubyear>, pp. 1-12.
      <br><br>
      <code>
@inproceedings{Mertens13_TAC,<br>
  author = {Mertens, Laurent and Demeester, Thomas and Deleu, Johannes and Develder, Chris},<br>
  title = {UGent participation in the TAC 2013 entity-linking task},<br>
  booktitle = {Proc. Text Analysis Conference (TAC 2013)},<br>
  month = {18--19 Nov.},<br>
  year = {2013},<br>
  pages = {1--12},<br>
  address = {Gaithersburg, MD, USA},<br>
  url = {https://tac.nist.gov//publications/2013/participant.papers/UGENT_IBCN.TAC2013.proceedings.pdf}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="demeester2013trec" class="row add-bottom pubentry pub2013 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>T. Demeester, D. Drieschnigg, K. Zhou and D. Hiemstra</pubauth>,
<pubtitle><a href="papers/2013/demeester2013trec.pdf" target="_blank">"Overview of the TREC 2013 federated web search track"</a></pubtitle>, in <pubconf>Proc. 22nd Text Retr. Conf. (TREC 2013)</pubconf>, Gaithersburg, MD, USA, 19-22 Nov. <pubyear>2013</pubyear>.
  <ul class="file-links">
               <li><a href="papers/2013/demeester2013trec.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         
         <li><a href="https://trec.nist.gov/pubs/trec22/papers/FEDERATED.OVERVIEW.pdf" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#demeester2013tac-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#demeester2013trec-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="demeester2013trec-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Overview of the TREC 2013 federated web search track</h2>
      <h4>T. Demeester, D. Drieschnigg, K. Zhou and D. Hiemstra</h4>
      <br>in <pubconf>Proc. 22nd Text Retr. Conf. (TREC 2013)</pubconf>, Gaithersburg, MD, USA, 19-22 Nov. <pubyear>2013</pubyear>.
      <br><br>
      <p>The TREC Federated Web Search track is intended to promote research related to federated search in a realistic web setting, and hereto provides a large data collection gathered from a series of online search engines. This overview paper discusses the results of the first edition of the track, FedWeb<br>2013. The focus was on basic challenges in federated search: (1) resource selection, and (2) results merging. After an overview of the provided data collection and the relevance judgments for the test topics, the participants’ individual approaches and results on both tasks are discussed. Promising research directions and an outlook on the 2014 edition of the track are provided as well.</p>
    </div>
    <div id="demeester2013trec-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Overview of the TREC 2013 federated web search track</h2>
      <h4>T. Demeester, D. Drieschnigg, K. Zhou and D. Hiemstra</h4>
      <br>in <pubconf>Proc. 22nd Text Retr. Conf. (TREC 2013)</pubconf>, Gaithersburg, MD, USA, 19-22 Nov. <pubyear>2013</pubyear>.
      <br><br>
      <code>
@inproceedings{demeester2013trec,<br>
  author = {Demeester, Thomas and Drieschnigg, Dong and Zhou, Ke and Hiemstra, Djoerd},<br>
  title = {Overview of the TREC 2013 federated web search track},<br>
  booktitle = {Proc. 22nd Text Retr. Conf. (TREC 2013)},<br>
  month = {19--22 Nov.},<br>
  year = {2013},<br>
  address = {Gaithersburg, MD, USA},<br>
  url = {https://trec.nist.gov/pubs/trec22/papers/FEDERATED.OVERVIEW.pdf}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="aly2013" class="row add-bottom pubentry pub2013 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>R. Aly, D. Hiemstra, D. Trieschnigg and T. Demeester</pubauth>,
<pubtitle><a href="papers/2013/aly2013.pdf" target="_blank">"Mirex and Taily at TREC 2013"</a></pubtitle>, in <pubconf>Proc. 22nd Text Retr. Conf. (TREC 2013)</pubconf>, Gaithersburg, MD, USA, 19-23 Nov. <pubyear>2013</pubyear>.
  <ul class="file-links">
               <li><a href="papers/2013/aly2013.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         
         <li><a href="https://trec.nist.gov/pubs/trec22/papers/lowlands-web-federated.pdf" target="_blank"><i class="fa fa-globe"></i> WWW</a></li>
               <li><a class="popup-with-zoom-anim" href="#aly2013-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#aly2013-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="aly2013-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Mirex and Taily at TREC 2013</h2>
      <h4>R. Aly, D. Hiemstra, D. Trieschnigg and T. Demeester</h4>
      <br>in <pubconf>Proc. 22nd Text Retr. Conf. (TREC 2013)</pubconf>, Gaithersburg, MD, USA, 19-23 Nov. <pubyear>2013</pubyear>.
      <br><br>
      <p>We describe the participation of the Lowlands at the Web Track and the FedWeb track of TREC 2013. For the Web Track we used the Mirex Map-Reduce library with out-of-thebox approaches and for the FedWeb Track we adapted our shard selection method Taily for resource selection. Here, our results were above median and close to the maximum performance achieved.</p>
    </div>
    <div id="aly2013-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Mirex and Taily at TREC 2013</h2>
      <h4>R. Aly, D. Hiemstra, D. Trieschnigg and T. Demeester</h4>
      <br>in <pubconf>Proc. 22nd Text Retr. Conf. (TREC 2013)</pubconf>, Gaithersburg, MD, USA, 19-23 Nov. <pubyear>2013</pubyear>.
      <br><br>
      <code>
@inproceedings{aly2013,<br>
  author = {Aly, Robin and Hiemstra, Djoerd and Trieschnigg, Dolf and Demeester, Thomas},<br>
  title = {Mirex and Taily at TREC 2013},<br>
  booktitle = {Proc. 22nd Text Retr. Conf. (TREC 2013)},<br>
  month = {19--23 Nov.},<br>
  year = {2013},<br>
  address = {Gaithersburg, MD, USA},<br>
  url = {https://trec.nist.gov/pubs/trec22/papers/lowlands-web-federated.pdf}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="aly2013sigir" class="row add-bottom pubentry pub2013 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>R. Aly, D. Hiemstra and T. Demeester</pubauth>,
<pubtitle><a href="papers/2013/aly2013sigir.pdf" target="_blank">"Taily: Shard selection using the tail of score distributions"</a></pubtitle>, in <pubconf>Proc. 36th Int. ACM SIGIR Conf. Research</pubconf>, Dublin, Ireland, 28 Jul.-1 Aug. <pubyear>2013</pubyear>, pp. 673-682.
  <ul class="file-links">
               <li><a href="papers/2013/aly2013sigir.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.1145/2484028.2484033" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#aly2013sigir-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#aly2013sigir-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="aly2013sigir-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Taily: Shard selection using the tail of score distributions</h2>
      <h4>R. Aly, D. Hiemstra and T. Demeester</h4>
      <br>in <pubconf>Proc. 36th Int. ACM SIGIR Conf. Research</pubconf>, Dublin, Ireland, 28 Jul.-1 Aug. <pubyear>2013</pubyear>, pp. 673-682.
      <br><br>
      <p>Search engines can improve their efficiency by selecting only few promising shards for each query. State-of-the-art shard selection algorithms first query a central index of sampled documents, and their effectiveness is similar to searching all shards. However, the search in the central index also hurts efficiency. Additionally, we show that the effectiveness of these approaches varies substantially with the sampled documents. This paper proposes Taily, a novel shard selection algorithm that models a query's score distribution in each shard as a Gamma distribution and selects shards with highly scored documents in the tail of the distribution. Taily estimates the parameters of score distributions based on the mean and variance of the score function's features in the collections and shards. Because Taily operates on term statistics instead of document samples, it is efficient and has deterministic effectiveness. Experiments on large web collections (Gov2, CluewebA and CluewebB) show that Taily achieves similar effectiveness to sample-based approaches, and improves upon their efficiency by roughly 20% in terms of used resources and response time.</p>
    </div>
    <div id="aly2013sigir-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Taily: Shard selection using the tail of score distributions</h2>
      <h4>R. Aly, D. Hiemstra and T. Demeester</h4>
      <br>in <pubconf>Proc. 36th Int. ACM SIGIR Conf. Research</pubconf>, Dublin, Ireland, 28 Jul.-1 Aug. <pubyear>2013</pubyear>, pp. 673-682.
      <br><br>
      <code>
@inproceedings{aly2013sigir,<br>
  author = {Aly, Robin and Hiemstra, Djoerd and Demeester, Thomas},<br>
  title = {Taily: Shard selection using the tail of score distributions},<br>
  booktitle = {Proc. 36th Int. ACM SIGIR Conf. Research},<br>
  month = {28 Jul.--1 Aug.},<br>
  year = {2013},<br>
  pages = {673--682},<br>
  address = {Dublin, Ireland},<br>
  doi = {10.1145/2484028.2484033}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="demeester2013ecir" class="row add-bottom pubentry pub2013 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>T. Demeester, D. Nguyen, D. Trieschnigg, C. Develder and D. Hiemstra</pubauth>,
<pubtitle><a href="papers/2013/demeester2013ecir.pdf" target="_blank">"Snippet-based relevance predictions for federated web search"</a></pubtitle>, in <pubconf>Proc. 35th Eur. Conf. Inf. Retr. (ECIR 2013)</pubconf>, Moscow, Russia, 24-27 Mar. <pubyear>2013</pubyear>, pp. 697-700.
    <ul class="file-links">
               <li><a href="papers/2013/demeester2013ecir.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           <li><a href="http://dx.doi.org/10.1007/978-3-642-36973-5_63" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
           
               <li><a class="popup-with-zoom-anim" href="#demeester2013ecir-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#demeester2013ecir-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
    </ul>
     </div>
     <div id="demeester2013ecir-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Snippet-based relevance predictions for federated web search</h2>
        <h4>T. Demeester, D. Nguyen, D. Trieschnigg, C. Develder and D. Hiemstra</h4>
        <br>in <pubconf>Proc. 35th Eur. Conf. Inf. Retr. (ECIR 2013)</pubconf>, Moscow, Russia, 24-27 Mar. <pubyear>2013</pubyear>, pp. 697-700.
        <br><br>
        <p>How well can the relevance of a page be predicted, purely based on snippets? This would be highly useful in a Federated Web Search setting where caching large amounts of result snippets is more feasible than caching entire pages. The experiments reported in this pa- per make use of result snippets and pages from a diverse set of actual Web search engines. A linear classifier is trained to predict the snippet- based user estimate of page relevance, but also, to predict the actual page relevance, again based on snippets alone. The presented results confirm the validity of the proposed approach and provide promising insights into future result merging strategies for a Federated Web Search setting.</p>
    </div>
    <div id="demeester2013ecir-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Snippet-based relevance predictions for federated web search</h2>
        <h4>T. Demeester, D. Nguyen, D. Trieschnigg, C. Develder and D. Hiemstra</h4>
        <br>in <pubconf>Proc. 35th Eur. Conf. Inf. Retr. (ECIR 2013)</pubconf>, Moscow, Russia, 24-27 Mar. <pubyear>2013</pubyear>, pp. 697-700.
        <br><br>
        <code>
@inproceedings{Demeester2013ECIR,<br>
  author = {Demeester, Thomas and Nguyen, Dong and Trieschnigg, Dolf and Develder, Chris and Hiemstra, Djoerd},<br>
  title = {Snippet-based relevance predictions for federated web search},<br>
  booktitle = {Proc. 35th Eur. Conf. Inf. Retr. (ECIR 2013)},<br>
  month = {24--27 Mar.},<br>
  year = {2013},<br>
  pages = {697--700},<br>
  address = {Moscow, Russia},<br>
  doi = {10.1007/978-3-642-36973-5_63}<br>
}
        </code>
      </div>
</div> <!-- end row -->

<div id="demeester2012airs" class="row add-bottom pubentry pub2012 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>T. Demeester, D. Nguyen, D. Trieschnigg, C. Develder and D. Hiemstra</pubauth>,
<pubtitle><a href="papers/2012/demeester2012airs.pdf" target="_blank">"What snippets say about pages in federated web search"</a></pubtitle>, in <pubconf>Proc. 8th Asia Inf. Retr. Soc. Conf. (AIRS 2012)</pubconf>, Tianjin, China, 17-19 Dec. <pubyear>2012</pubyear>.
    <ul class="file-links">
               <li><a href="papers/2012/demeester2012airs.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           <li><a href="http://dx.doi.org/10.1007/978-3-642-35341-3_21" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
           
               <li><a class="popup-with-zoom-anim" href="#demeester2012airs-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#demeester2012airs-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
    </ul>
     </div>
     <div id="demeester2012airs-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>What snippets say about pages in federated web search</h2>
        <h4>T. Demeester, D. Nguyen, D. Trieschnigg, C. Develder and D. Hiemstra</h4>
        <br>in <pubconf>Proc. 8th Asia Inf. Retr. Soc. Conf. (AIRS 2012)</pubconf>, Tianjin, China, 17-19 Dec. <pubyear>2012</pubyear>.
        <br><br>
        <p>What is the likelihood that a Web page is considered relevant to a query, given the relevance assessment of the corresponding snippet? Using a new federated IR test collection that contains search results from over a hundred search engines on the internet, we are able to investigate such research questions from a global perspective. Our test collection covers the main Web search engines like Google, Yahoo!, and Bing, as well as a number of smaller search engines dedicated to multimedia, shopping, etc., and as such reﬂects a realistic Web environment. <br>Using a large set of relevance assessments, we are able to investigate the connection between snippet quality and page relevance. The dataset is strongly inhomogeneous, and although the assessors’ consistency is shown to be satisfying, care is required when comparing resources. To this end, a number of probabilistic quantities, based on snippet and page relevance, are introduced and evaluated.</p>
    </div>
    <div id="demeester2012airs-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>What snippets say about pages in federated web search</h2>
        <h4>T. Demeester, D. Nguyen, D. Trieschnigg, C. Develder and D. Hiemstra</h4>
        <br>in <pubconf>Proc. 8th Asia Inf. Retr. Soc. Conf. (AIRS 2012)</pubconf>, Tianjin, China, 17-19 Dec. <pubyear>2012</pubyear>.
        <br><br>
        <code>
@inproceedings{Demeester2012AIRS,<br>
  author = {Demeester, Thomas and Dong Nguyen and Dolf Trieschnigg and Chris Develder and Djoerd Hiemstra},<br>
  title = {What snippets say about pages in federated web search},<br>
  booktitle = {Proc. 8th Asia Inf. Retr. Soc. Conf. (AIRS 2012)},<br>
  month = {17--19 Dec.},<br>
  year = {2012},<br>
  address = {Tianjin, China},<br>
  doi = {10.1007/978-3-642-35341-3_21}<br>
}
        </code>
      </div>
</div> <!-- end row -->

<div id="mertens2012tac" class="row add-bottom pubentry pub2012 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>L. Mertens, T. Demeester, J. Deleu, P. Demeester and C. Develder</pubauth>,
<pubtitle><a href="papers/2012/mertens2012tac.pdf" target="_blank">"UGent participation in the TAC 2012 entity-linking task"</a></pubtitle>, in <pubconf>Proc. 5th Text Analysis Conf. (TAC 2012)</pubconf>, Gaithersburg, MD, USA, 14-15 Nov. <pubyear>2012</pubyear>.
    <ul class="file-links">
               <li><a href="papers/2012/mertens2012tac.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           
           
               <li><a class="popup-with-zoom-anim" href="#mertens2012tac-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#mertens2012tac-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
    </ul>
     </div>
     <div id="mertens2012tac-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>UGent participation in the TAC 2012 entity-linking task</h2>
        <h4>L. Mertens, T. Demeester, J. Deleu, P. Demeester and C. Develder</h4>
        <br>in <pubconf>Proc. 5th Text Analysis Conf. (TAC 2012)</pubconf>, Gaithersburg, MD, USA, 14-15 Nov. <pubyear>2012</pubyear>.
        <br><br>
        <p>This article describes in detail the system used by the UGent-IBCN team for participating in the Text Analysis Conference (TAC) 2012 Mono-Lingual Entity-Linking task. The pre- sented system is essentially rule-based, following a generic framework that is highly optimised for each label (i.e. with different rules for persons, organisations, and locations). The main contribution of this work is in identifying a number of label-specific issues and presenting simple heuristic solutions that yet allow building an efficient and effective system. These treated issues include resolving abbreviated organisation names, resolving popular nicknames, or taking into account American vs British spelling.</p>
    </div>
    <div id="mertens2012tac-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>UGent participation in the TAC 2012 entity-linking task</h2>
        <h4>L. Mertens, T. Demeester, J. Deleu, P. Demeester and C. Develder</h4>
        <br>in <pubconf>Proc. 5th Text Analysis Conf. (TAC 2012)</pubconf>, Gaithersburg, MD, USA, 14-15 Nov. <pubyear>2012</pubyear>.
        <br><br>
        <code>
@inproceedings{Mertens2012TAC,<br>
  author = {Mertens, Laurent and Demeester, Thomas and Deleu, Johannes and Demeester, Piet and Develder, Chris},<br>
  title = {UGent participation in the TAC 2012 entity-linking task},<br>
  booktitle = {Proc. 5th Text Analysis Conf. (TAC 2012)},<br>
  month = {14-15 Nov.},<br>
  year = {2012},<br>
  address = {Gaithersburg, MD, USA}<br>
}
        </code>
      </div>
</div> <!-- end row -->

<div id="vanduc2012trec" class="row add-bottom pubentry pub2012 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>T.H. Van Duc, T. Demeester, J. Deleu and C. Develder</pubauth>,
<pubtitle><a href="papers/2012/vanduc2012trec.pdf" target="_blank">"UGent participation in the Microblog Track 2012"</a></pubtitle>, in <pubconf>Proc. Text Retr. Conf. (TREC 2012)</pubconf>, Gaithersburg, MD, 6-9 Nov. <pubyear>2012</pubyear>.
    <ul class="file-links">
               <li><a href="papers/2012/vanduc2012trec.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           
           
               <li><a class="popup-with-zoom-anim" href="#vanduc2012trec-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#vanduc2012trec-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
    </ul>
     </div>
     <div id="vanduc2012trec-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>UGent participation in the Microblog Track 2012</h2>
        <h4>T.H. Van Duc, T. Demeester, J. Deleu and C. Develder</h4>
        <br>in <pubconf>Proc. Text Retr. Conf. (TREC 2012)</pubconf>, Gaithersburg, MD, 6-9 Nov. <pubyear>2012</pubyear>.
        <br><br>
        <p>In this paper, we describe the search system, developed at Ghent University for the TREC 2012 Microblog Track in order to rank Twitter messages or ‘tweets’ from a fixed corpus in response to a number of search requests. Our system ranks the tweets based on a Logistic Regression classifier trained with data from the Microblog Track 2011. The features used for training the classifier include local tweets features, but also, query expansion and tweet expansion features, based on external Web data, which appear to significantly improve results.</p>
    </div>
    <div id="vanduc2012trec-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>UGent participation in the Microblog Track 2012</h2>
        <h4>T.H. Van Duc, T. Demeester, J. Deleu and C. Develder</h4>
        <br>in <pubconf>Proc. Text Retr. Conf. (TREC 2012)</pubconf>, Gaithersburg, MD, 6-9 Nov. <pubyear>2012</pubyear>.
        <br><br>
        <code>
@inproceedings{VanDuc2012TREC,<br>
  author = {Van Duc, Thong Hoang and Demeester, Thomas and Deleu, Johannes and Develder, Chris},<br>
  title = {UGent participation in the Microblog Track 2012},<br>
  booktitle = {Proc. Text Retr. Conf. (TREC 2012)},<br>
  month = {6--9 Nov.},<br>
  year = {2012},<br>
  address = {Gaithersburg, MD}<br>
}
        </code>
      </div>
</div> <!-- end row -->

<div id="nguyen2012cikm" class="row add-bottom pubentry pub2012 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>D. Nguyen, T. Demeester, D. Trieschnigg and D. Hiemstra</pubauth>,
<pubtitle><a href="papers/2012/nguyen2012cikm.pdf" target="_blank">"Federated search in the wild: The combined power of over a hundred search engines"</a></pubtitle>, in <pubconf>Proc. 21st ACM Int. Conf. Inf. Knowl. Management (CIKM 2012)</pubconf>, Maui, HI, USA, 29 Oct. - 2 Nov. <pubyear>2012</pubyear>, pp. 1874-1878.
  <ul class="file-links">
               <li><a href="papers/2012/nguyen2012cikm.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.1145/2396761.2398535" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#nguyen2012cikm-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#nguyen2012cikm-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="nguyen2012cikm-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Federated search in the wild: The combined power of over a hundred search engines</h2>
      <h4>D. Nguyen, T. Demeester, D. Trieschnigg and D. Hiemstra</h4>
      <br>in <pubconf>Proc. 21st ACM Int. Conf. Inf. Knowl. Management (CIKM 2012)</pubconf>, Maui, HI, USA, 29 Oct. - 2 Nov. <pubyear>2012</pubyear>, pp. 1874-1878.
      <br><br>
      <p>Federated search has the potential of improving web search: the user becomes less dependent on a single search provider and parts of the deep web become available through a unified interface, leading to a wider variety in the retrieved search results. However, a publicly available dataset for federated search reflecting an actual web environment has been absent. As a result, it has been difficult to assess whether proposed systems are suitable for the web setting. We introduce a new test collection containing the results from more than a hundred actual search engines, ranging from large general web search engines such as Google and Bing to small domain-specific engines. We discuss the design and analyze the effect of several sampling methods. For a set of test queries, we collected relevance judgements for the top 10 results of each search engine. The dataset is publicly available and is useful for researchers interested in resource selection for web search collections, result merging and size estimation of uncooperative resources.</p>
    </div>
    <div id="nguyen2012cikm-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Federated search in the wild: The combined power of over a hundred search engines</h2>
      <h4>D. Nguyen, T. Demeester, D. Trieschnigg and D. Hiemstra</h4>
      <br>in <pubconf>Proc. 21st ACM Int. Conf. Inf. Knowl. Management (CIKM 2012)</pubconf>, Maui, HI, USA, 29 Oct. - 2 Nov. <pubyear>2012</pubyear>, pp. 1874-1878.
      <br><br>
      <code>
@inproceedings{nguyen2012cikm,<br>
  author = {Nguyen, Dong and Demeester, Thomas and Trieschnigg, Dolf and Hiemstra, Djoerd},<br>
  title = {Federated search in the wild: The combined power of over a hundred search engines},<br>
  booktitle = {Proc. 21st ACM Int. Conf. Inf. Knowl. Management (CIKM 2012)},<br>
  month = {29 Oct. - 2 Nov.},<br>
  year = {2012},<br>
  pages = {1874--1878},<br>
  address = {Maui, HI, USA},<br>
  doi = {10.1145/2396761.2398535}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="deleu2012dir" class="row add-bottom pubentry pub2012 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>J. Deleu, A. De Moor, T. Demeester, B. Vermeulen and P. Demeester</pubauth>,
<pubtitle><a href="papers/2012/deleu2012dir.pdf" target="_blank">"Named entity recognition on flemish audio-visual and news-paper archives"</a></pubtitle>, in <pubconf>Proc. 12th Dutch-Belgian Inf. Retr. Workshop (DIR 2012)</pubconf>, Ghent, Belgium, 23-24 Feb. <pubyear>2012</pubyear>, pp. 38-41.
  <ul class="file-links">
               <li><a href="papers/2012/deleu2012dir.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         
         
               <li><a class="popup-with-zoom-anim" href="#deleu2012dir-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#deleu2012dir-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="deleu2012dir-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Named entity recognition on flemish audio-visual and news-paper archives</h2>
      <h4>J. Deleu, A. De Moor, T. Demeester, B. Vermeulen and P. Demeester</h4>
      <br>in <pubconf>Proc. 12th Dutch-Belgian Inf. Retr. Workshop (DIR 2012)</pubconf>, Ghent, Belgium, 23-24 Feb. <pubyear>2012</pubyear>, pp. 38-41.
      <br><br>
      <p>This paper describes a number of specific issues that we needed to deal with, in order to compose an accurate Named Entity Recognition tool on multimedia archives in Dutch. The considered data consists of archivation metadata from video collections, and large newspaper collections. For the video collections, the main challenge is to cope with a lack of capitalization in the metadata. To this end, specific capitalization features are calculated from Wikipedia. For the newspaper collections, the main concern is to create a system that maintains its performance over the course of many years. For that goal, special clustering features allow dealing with words that have not been encountered in training data. Results for the different components of the tool are reported on the target data, as well as on publicly available test data.</p>
    </div>
    <div id="deleu2012dir-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Named entity recognition on flemish audio-visual and news-paper archives</h2>
      <h4>J. Deleu, A. De Moor, T. Demeester, B. Vermeulen and P. Demeester</h4>
      <br>in <pubconf>Proc. 12th Dutch-Belgian Inf. Retr. Workshop (DIR 2012)</pubconf>, Ghent, Belgium, 23-24 Feb. <pubyear>2012</pubyear>, pp. 38-41.
      <br><br>
      <code>
@inproceedings{deleu2012dir,<br>
  author = {Deleu, Johannes and De Moor, An and Demeester, Thomas and Vermeulen, Brecht and Demeester, Piet},<br>
  title = {Named entity recognition on flemish audio-visual and news-paper archives},<br>
  booktitle = {Proc. 12th Dutch-Belgian Inf. Retr. Workshop (DIR 2012)},<br>
  month = {23--24 Feb.},<br>
  year = {2012},<br>
  pages = {38--41},<br>
  address = {Ghent, Belgium}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="mertens2012" class="row add-bottom pubentry pub2012 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>L. Mertens, T. Demeester, J. Deleu, C. Develder and P. Demeester</pubauth>,
<pubtitle><a href="papers/2012/mertens2012.pdf" target="_blank">"Context-based person identification for news collections"</a></pubtitle>, in <pubconf>Proc. 12th Dutch-Belgian Inf. Retr. Workshop (DIR 2012)</pubconf>, Ghent, Belgium, 23-24 Feb. <pubyear>2012</pubyear>, pp. 26-29.
    <ul class="file-links">
               <li><a href="papers/2012/mertens2012.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           
           
               <li><a class="popup-with-zoom-anim" href="#mertens2012-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#mertens2012-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
    </ul>
     </div>
     <div id="mertens2012-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Context-based person identification for news collections</h2>
        <h4>L. Mertens, T. Demeester, J. Deleu, C. Develder and P. Demeester</h4>
        <br>in <pubconf>Proc. 12th Dutch-Belgian Inf. Retr. Workshop (DIR 2012)</pubconf>, Ghent, Belgium, 23-24 Feb. <pubyear>2012</pubyear>, pp. 26-29.
        <br><br>
        <p>In modern automated information extraction systems, Named Entity Disambiguation (NED) techniques are becoming increasingly important. The ambiguity of person names leads to a decrease in the output quality of search engines. This paper presents a two-stage rule-based NED model, based on a local and global context of the mentioned persons. A number of experiments with different scoring functions are reported, as well as a specific evaluation method to estimate the efficiency of the model on a real-life data collection in an unsupervised way.</p>
    </div>
    <div id="mertens2012-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Context-based person identification for news collections</h2>
        <h4>L. Mertens, T. Demeester, J. Deleu, C. Develder and P. Demeester</h4>
        <br>in <pubconf>Proc. 12th Dutch-Belgian Inf. Retr. Workshop (DIR 2012)</pubconf>, Ghent, Belgium, 23-24 Feb. <pubyear>2012</pubyear>, pp. 26-29.
        <br><br>
        <code>
@inproceedings{Mertens2012,<br>
  author = {Mertens, Laurent and Demeester, Thomas and Deleu, Johannes and Develder, Chris and Demeester, Piet},<br>
  title = {Context-based person identification for news collections},<br>
  booktitle = {Proc. 12th Dutch-Belgian Inf. Retr. Workshop (DIR 2012)},<br>
  month = {23--24 Feb.},<br>
  year = {2012},<br>
  pages = {26--29},<br>
  address = {Ghent, Belgium}<br>
}
        </code>
      </div>
</div> <!-- end row -->

<div id="vandamme2012dir" class="row add-bottom pubentry pub2012 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>S. Vandamme, T. Wauters, T. Demeester and F. De Turck</pubauth>,
<pubtitle><a href="papers/2012/vandamme2012dir.pdf" target="_blank">"Implementation and evaluation of query filtering in a role ontology-enhanced search engine"</a></pubtitle>, in <pubconf>Proc. 12th Dutch-Belgian Inf. Retr. Workshop (DIR 2012)</pubconf>, Ghent, Belgium, 23-24 Feb. <pubyear>2012</pubyear>, pp. 34-37.
  <ul class="file-links">
               <li><a href="papers/2012/vandamme2012dir.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         
         
               <li><a class="popup-with-zoom-anim" href="#vandamme2012dir-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#vandamme2012dir-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="vandamme2012dir-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Implementation and evaluation of query filtering in a role ontology-enhanced search engine</h2>
      <h4>S. Vandamme, T. Wauters, T. Demeester and F. De Turck</h4>
      <br>in <pubconf>Proc. 12th Dutch-Belgian Inf. Retr. Workshop (DIR 2012)</pubconf>, Ghent, Belgium, 23-24 Feb. <pubyear>2012</pubyear>, pp. 34-37.
      <br><br>
      <p>We designed a role ontology-enhanced multimedia search enginewhere the user can search and subsequently filter news items withqueries and filter options describing the roles of the people whoappear in the items, specifically politicians. The system makes useof a separate knowledge base with domain information on politics.We demonstrate that when a user fails to recollect the name of a politician, role-based queries combined with filter options tailoredto the query and the result set, lead the user fast to both the namehe failed to recollect and the intended results in the multimediadatabase.</p>
    </div>
    <div id="vandamme2012dir-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Implementation and evaluation of query filtering in a role ontology-enhanced search engine</h2>
      <h4>S. Vandamme, T. Wauters, T. Demeester and F. De Turck</h4>
      <br>in <pubconf>Proc. 12th Dutch-Belgian Inf. Retr. Workshop (DIR 2012)</pubconf>, Ghent, Belgium, 23-24 Feb. <pubyear>2012</pubyear>, pp. 34-37.
      <br><br>
      <code>
@inproceedings{vandamme2012dir,<br>
  author = {Vandamme, Stijn and Wauters, Tim and Demeester, Thomas and De Turck, Filip},<br>
  title = {Implementation and evaluation of query filtering in a role ontology-enhanced search engine},<br>
  booktitle = {Proc. 12th Dutch-Belgian Inf. Retr. Workshop (DIR 2012)},<br>
  month = {23--24 Feb.},<br>
  year = {2012},<br>
  pages = {34--37},<br>
  address = {Ghent, Belgium}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="vandenbossche2012dir" class="row add-bottom pubentry pub2012 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>B. Van Den Bossche, B. Vermeulen, J. Deleu, T. Demeester and P. Demeester</pubauth>,
<pubtitle><a href="papers/2012/vandenbossche2012dir.pdf" target="_blank">"MediaHaven: Multimedia asset management with integrated NER and categorization"</a></pubtitle>, in <pubconf>Proc. 12th Dutch-Belgian Inf. Retr. Workshop (DIR 2012)</pubconf>, Ghent, Belgium, 23-24 Feb. <pubyear>2012</pubyear>, pp. 85-86.
  <ul class="file-links">
               <li><a href="papers/2012/vandenbossche2012dir.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         
         
               <li><a class="popup-with-zoom-anim" href="#vandenbossche2012dir-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#vandenbossche2012dir-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="vandenbossche2012dir-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>MediaHaven: Multimedia asset management with integrated NER and categorization</h2>
      <h4>B. Van Den Bossche, B. Vermeulen, J. Deleu, T. Demeester and P. Demeester</h4>
      <br>in <pubconf>Proc. 12th Dutch-Belgian Inf. Retr. Workshop (DIR 2012)</pubconf>, Ghent, Belgium, 23-24 Feb. <pubyear>2012</pubyear>, pp. 85-86.
      <br><br>
      <p>In order to allow for flexible search and asset management on the textual metadata of multimedia archives, the extraction of information and especially named entities is an essential step. Practically, they are of great help for applications like facetted search, input assistance, search suggestions, linking assets, etc. This paper describes MediaHaven, a Media Asset Management (MAM) system, commercialized by Zeticon, a spin-off of Ghent University-IBBT. MediaHaven incorporates an advanced NER and categorisation system to improve the user experience</p>
    </div>
    <div id="vandenbossche2012dir-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>MediaHaven: Multimedia asset management with integrated NER and categorization</h2>
      <h4>B. Van Den Bossche, B. Vermeulen, J. Deleu, T. Demeester and P. Demeester</h4>
      <br>in <pubconf>Proc. 12th Dutch-Belgian Inf. Retr. Workshop (DIR 2012)</pubconf>, Ghent, Belgium, 23-24 Feb. <pubyear>2012</pubyear>, pp. 85-86.
      <br><br>
      <code>
@inproceedings{vandenbossche2012dir,<br>
  author = {Van Den Bossche, Bruno and Vermeulen, Brecht and Deleu, Johannes and Demeester, Thomas and Demeester, Piet},<br>
  title = {MediaHaven: Multimedia asset management with integrated NER and categorization},<br>
  booktitle = {Proc. 12th Dutch-Belgian Inf. Retr. Workshop (DIR 2012)},<br>
  month = {23--24 Feb.},<br>
  year = {2012},<br>
  pages = {85--86},<br>
  address = {Ghent, Belgium}<br>
}
      </code>
    </div>
</div> <!-- end row -->

<div id="vanduc2012dir" class="row add-bottom pubentry pub2012 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>T.H. Van Duc, T. Demeester, C. Develder and H. Shin</pubauth>,
<pubtitle><a href="papers/2012/vanduc2012dir.pdf" target="_blank">"Effectiveness of learning to rank for finding user similarity in social media"</a></pubtitle>, in <pubconf>Proc. 12th Dutch-Belgian Inf. Retr. Workshop (DIR 2012)</pubconf>, Ghent, Belgium, 23-24 Feb. <pubyear>2012</pubyear>, pp. 30-33.
    <ul class="file-links">
               <li><a href="papers/2012/vanduc2012dir.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
           
           
               <li><a class="popup-with-zoom-anim" href="#vanduc2012dir-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
           <li><a class="popup-with-zoom-anim" href="#vanduc2012dir-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
    </ul>
     </div>
     <div id="vanduc2012dir-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Effectiveness of learning to rank for finding user similarity in social media</h2>
        <h4>T.H. Van Duc, T. Demeester, C. Develder and H. Shin</h4>
        <br>in <pubconf>Proc. 12th Dutch-Belgian Inf. Retr. Workshop (DIR 2012)</pubconf>, Ghent, Belgium, 23-24 Feb. <pubyear>2012</pubyear>, pp. 30-33.
        <br><br>
        <p>This paper focuses on an automatic and accurate approach for finding similar users in social networks. Many types of social networks could benefit from such techniques, but the focus in this paper is on online photo services. The similarity between users needs to be considered on two different levels, i.e., the semantic similarity (or correspondence in tagging behavior), and the similarity in terms of social relations. In recent work, heuristic formulas were introduced for the tag commonness (TC) and the link strength (LS), with an adaptive combination scheme to describe how relevant each of these similarity aspects are for particular users, in order to define the user similarity. This paper presents an experiment, where a Learning-to-Rank approach is used to find suitable combinations of TC and LS related parameter values, hence taking into account the proficiency of users to tag their photos, and their noticeability in the online community, in order to obtain an overall user similarity. The user experiments show that the results with this learning-to-rank approach are significantly better than with a former, heuristic, approach.</p>
    </div>
    <div id="vanduc2012dir-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
        <h2>Effectiveness of learning to rank for finding user similarity in social media</h2>
        <h4>T.H. Van Duc, T. Demeester, C. Develder and H. Shin</h4>
        <br>in <pubconf>Proc. 12th Dutch-Belgian Inf. Retr. Workshop (DIR 2012)</pubconf>, Ghent, Belgium, 23-24 Feb. <pubyear>2012</pubyear>, pp. 30-33.
        <br><br>
        <code>
@inproceedings{VanDuc2012DIR,<br>
  author = {Van Duc, Thong Hoang and Demeester, Thomas and Develder, Chris and Shin, Hyoseop},<br>
  title = {Effectiveness of learning to rank for finding user similarity in social media},<br>
  booktitle = {Proc. 12th Dutch-Belgian Inf. Retr. Workshop (DIR 2012)},<br>
  month = {23--24 Feb.},<br>
  year = {2012},<br>
  pages = {30--33},<br>
  address = {Ghent, Belgium}<br>
}
        </code>
      </div>
</div> <!-- end row -->

<div id="aly2011" class="row add-bottom pubentry pub2011 pubir pubinproceedings">
     <div class="one column">
     <h3 class="inproceedings"><span>pubinproceedings</span>
     </div>
     <div class="eleven columns ir">
     <pubauth>R. Aly and T. Demeester</pubauth>,
<pubtitle><a href="papers/2011/aly2011.pdf" target="_blank">"Towards a better understanding of the relationship between probabilistic models in IR"</a></pubtitle>, in <pubconf>Proc. 3rd Int. Conf. Theory Inf. Retr. (ICTIR 2011)</pubconf>, Bertorino, Italy, 12-14 Sep. <pubyear>2011</pubyear>, pp. 164-175.
  <ul class="file-links">
               <li><a href="papers/2011/aly2011.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a></li>
         <li><a href="https://doi.org/10.1007/978-3-642-23318-0_16" target="_blank"><i class="fa fa-external-link"></i> DOI</a></li>
         
               <li><a class="popup-with-zoom-anim" href="#aly2011-abstract"><i class="fa fa-eye"></i> Abstract</a></li>
         <li><a class="popup-with-zoom-anim" href="#aly2011-bibtex"><i class="fa fa-bookmark-o"></i> BibTeX</a></li>
         
         
  </ul>
     </div>
     <div id="aly2011-abstract" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Towards a better understanding of the relationship between probabilistic models in IR</h2>
      <h4>R. Aly and T. Demeester</h4>
      <br>in <pubconf>Proc. 3rd Int. Conf. Theory Inf. Retr. (ICTIR 2011)</pubconf>, Bertorino, Italy, 12-14 Sep. <pubyear>2011</pubyear>, pp. 164-175.
      <br><br>
      <p>Probability of relevance (PR) models are generally assumed to implement the Probability Ranking Principle (PRP) of IR, and recent publications claim that PR models and language models are similar. However, a careful analysis reveals two gaps in the chain of reasoning behind this statement. First, the PRP considers the relevance of particular documents, whereas PR models consider the relevance of any query-document pair. Second, unlike PR models, language models consider draws of terms and documents. We bridge the first gap by showing how the probability measure of PR models can be used to define the probabilistic model of the PRP. Furthermore, we argue that given the differences between PR models and language models, the second gap cannot be bridged at the probabilistic model level. We instead define a new PR model based on logistic regression, which has a similar score function to the one of the query likelihood model. The performance of both models is strongly correlated, hence providing a bridge for the second gap at the functional and ranking level. Understanding language models in relation with logistic regression models opens ample new research directions which we propose as future work.</p>
    </div>
    <div id="aly2011-bibtex" class="pubabstract zoom-anim-dialog mfp-hide">
      <h2>Towards a better understanding of the relationship between probabilistic models in IR</h2>
      <h4>R. Aly and T. Demeester</h4>
      <br>in <pubconf>Proc. 3rd Int. Conf. Theory Inf. Retr. (ICTIR 2011)</pubconf>, Bertorino, Italy, 12-14 Sep. <pubyear>2011</pubyear>, pp. 164-175.
      <br><br>
      <code>
@inproceedings{aly2011,<br>
  author = {Aly, Robin and Demeester, T.},<br>
  title = {Towards a better understanding of the relationship between probabilistic models in IR},<br>
  booktitle = {Proc. 3rd Int. Conf. Theory Inf. Retr. (ICTIR 2011)},<br>
  month = {12--14 Sep.},<br>
  year = {2011},<br>
  pages = {164--175},<br>
  address = {Bertorino, Italy},<br>
  doi = {10.1007/978-3-642-23318-0_16}<br>
}
      </code>
    </div>
</div> <!-- end row -->
